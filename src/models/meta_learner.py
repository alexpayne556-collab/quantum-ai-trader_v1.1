"""
Hierarchical Meta-Learner for Trading Forecaster
=================================================

Implementation of Perplexity Q1 recommendation:
- Stacking ensemble architecture
- Level 1: Specialized models (Pattern, Research, Dark Pool)
- Level 2: XGBoost meta-learner (max_depth=2, constrained)

Expected: +5-8% Sharpe improvement over simple weighted averaging
Computational: <10ms inference time

Author: Quantum AI Trader
Date: December 8, 2025
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, log_loss
from typing import Dict, Tuple, Optional
import logging
from datetime import datetime
import joblib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HierarchicalMetaLearner:
    """
    Hierarchical Stacking Ensemble for trading signal fusion.
    
    Architecture:
    -------------
    Level 1 (Specialized Models):
    - Pattern Model: LogisticRegression (linear, interpretable)
    - Research Model: XGBoost (non-linear, handles 60 features)
    - Dark Pool Model: XGBoost (institutional flow patterns)
    
    Level 2 (Meta-Learner):
    - XGBoost Classifier (max_depth=2 to prevent overfitting)
    - Inputs: Level 1 probabilities + regime indicators
    
    Usage:
    ------
    >>> learner = HierarchicalMetaLearner()
    >>> learner.train_ensemble(X_patterns, X_research, X_dark_pool, y_train, regime_ids)
    >>> prob = learner.predict(X_patterns_new, X_research_new, X_dark_pool_new, regime_new)
    """
    
    def __init__(self, max_depth: int = 2, learning_rate: float = 0.05):
        """
        Initialize hierarchical ensemble.
        
        Args:
            max_depth: Meta-learner tree depth (2 = shallow, prevents overfitting)
            learning_rate: XGBoost eta (0.05 = slow learning)
        """
        # Level 1: Specialized Base Learners
        self.pattern_model = LogisticRegression(
            C=1.0,
            solver='lbfgs',
            max_iter=500,
            random_state=42
        )
        
        self.research_model = xgb.XGBClassifier(
            max_depth=3,
            n_estimators=50,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            objective='binary:logistic',
            eval_metric='logloss',
            random_state=42
        )
        
        self.dark_pool_model = xgb.XGBClassifier(
            max_depth=2,
            n_estimators=30,
            learning_rate=0.1,
            subsample=0.8,
            objective='binary:logistic',
            eval_metric='logloss',
            random_state=42
        )
        
        # Level 2: Meta-Learner (constrained to prevent overfitting)
        self.meta_learner = xgb.XGBClassifier(
            max_depth=max_depth,           # Shallow tree (2)
            learning_rate=learning_rate,   # Slow learning (0.05)
            n_estimators=100,
            objective='binary:logistic',
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric='logloss',
            random_state=42
        )
        
        self.is_trained = False
        self.training_metrics = {}
        
    def train_ensemble(
        self,
        X_patterns: pd.DataFrame,
        X_research: pd.DataFrame,
        X_dark_pool: pd.DataFrame,
        y: pd.Series,
        regime_ids: Optional[pd.Series] = None,
        validation_split: float = 0.2
    ) -> Dict[str, float]:
        """
        Train hierarchical ensemble with out-of-fold predictions.
        
        Args:
            X_patterns: Pattern features (Elliott Wave, candlesticks, S/R)
            X_research: Research features (60 engineered)
            X_dark_pool: Dark pool features (IFI, A/D, OBV, VROC, SMI)
            y: Binary labels (1 = profitable, 0 = unprofitable)
            regime_ids: Regime classification (12 regimes)
            validation_split: Hold-out validation fraction
            
        Returns:
            dict: Training metrics (AUC, logloss, accuracy)
        """
        logger.info("Training Hierarchical Meta-Learner...")
        
        # Split into train/validation (time-series aware)
        split_idx = int(len(y) * (1 - validation_split))
        
        X_patterns_train, X_patterns_val = X_patterns.iloc[:split_idx], X_patterns.iloc[split_idx:]
        X_research_train, X_research_val = X_research.iloc[:split_idx], X_research.iloc[split_idx:]
        X_dark_pool_train, X_dark_pool_val = X_dark_pool.iloc[:split_idx], X_dark_pool.iloc[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # ===== LEVEL 1: Train Base Learners =====
        logger.info("Training Level 1 Base Learners...")
        
        # 1a. Pattern Model (linear, simple)
        self.pattern_model.fit(X_patterns_train, y_train)
        p_patterns_train = self.pattern_model.predict_proba(X_patterns_train)[:, 1]
        p_patterns_val = self.pattern_model.predict_proba(X_patterns_val)[:, 1]
        
        # 1b. Research Model (non-linear, many features)
        self.research_model.fit(
            X_research_train, y_train,
            eval_set=[(X_research_val, y_val)],
            verbose=False
        )
        p_research_train = self.research_model.predict_proba(X_research_train)[:, 1]
        p_research_val = self.research_model.predict_proba(X_research_val)[:, 1]
        
        # 1c. Dark Pool Model (institutional flow)
        self.dark_pool_model.fit(
            X_dark_pool_train, y_train,
            eval_set=[(X_dark_pool_val, y_val)],
            verbose=False
        )
        p_dark_pool_train = self.dark_pool_model.predict_proba(X_dark_pool_train)[:, 1]
        p_dark_pool_val = self.dark_pool_model.predict_proba(X_dark_pool_val)[:, 1]
        
        # ===== LEVEL 2: Build Meta-Features =====
        logger.info("Building meta-features for Level 2...")
        
        X_meta_train = pd.DataFrame({
            'score_pattern': p_patterns_train,
            'score_research': p_research_train,
            'score_dark_pool': p_dark_pool_train
        })
        
        X_meta_val = pd.DataFrame({
            'score_pattern': p_patterns_val,
            'score_research': p_research_val,
            'score_dark_pool': p_dark_pool_val
        })
        
        # Add regime indicators if provided
        if regime_ids is not None:
            regime_train = regime_ids.iloc[:split_idx].reset_index(drop=True)
            regime_val = regime_ids.iloc[split_idx:].reset_index(drop=True)
            
            # One-hot encode regimes
            regime_dummies_train = pd.get_dummies(regime_train, prefix='regime')
            regime_dummies_val = pd.get_dummies(regime_val, prefix='regime')
            
            X_meta_train = pd.concat([X_meta_train, regime_dummies_train], axis=1)
            X_meta_val = pd.concat([X_meta_val, regime_dummies_val], axis=1)
        
        # ===== LEVEL 2: Train Meta-Learner =====
        logger.info("Training Level 2 Meta-Learner...")
        
        self.meta_learner.fit(
            X_meta_train, y_train,
            eval_set=[(X_meta_val, y_val)],
            verbose=False
        )
        
        # Final predictions
        y_pred_train = self.meta_learner.predict_proba(X_meta_train)[:, 1]
        y_pred_val = self.meta_learner.predict_proba(X_meta_val)[:, 1]
        
        # ===== Compute Metrics =====
        train_auc = roc_auc_score(y_train, y_pred_train)
        val_auc = roc_auc_score(y_val, y_pred_val)
        train_logloss = log_loss(y_train, y_pred_train)
        val_logloss = log_loss(y_val, y_pred_val)
        
        self.training_metrics = {
            'train_auc': train_auc,
            'val_auc': val_auc,
            'train_logloss': train_logloss,
            'val_logloss': val_logloss,
            'n_train': len(y_train),
            'n_val': len(y_val)
        }
        
        self.is_trained = True
        
        logger.info(f"Training complete!")
        logger.info(f"  Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}")
        logger.info(f"  Train LogLoss: {train_logloss:.4f}, Val LogLoss: {val_logloss:.4f}")
        
        return self.training_metrics
    
    def predict(
        self,
        X_patterns: pd.DataFrame,
        X_research: pd.DataFrame,
        X_dark_pool: pd.DataFrame,
        regime_id: Optional[int] = None
    ) -> float:
        """
        Predict probability of profitable trade.
        
        Args:
            X_patterns: Pattern features for single sample
            X_research: Research features for single sample
            X_dark_pool: Dark pool features for single sample
            regime_id: Current market regime (0-11)
            
        Returns:
            float: Probability of profit (0-1)
        """
        if not self.is_trained:
            logger.warning("Model not trained! Returning neutral probability.")
            return 0.5
        
        # Level 1 predictions
        p_pattern = self.pattern_model.predict_proba(X_patterns)[:, 1][0]
        p_research = self.research_model.predict_proba(X_research)[:, 1][0]
        p_dark_pool = self.dark_pool_model.predict_proba(X_dark_pool)[:, 1][0]
        
        # Build meta-features
        X_meta = pd.DataFrame({
            'score_pattern': [p_pattern],
            'score_research': [p_research],
            'score_dark_pool': [p_dark_pool]
        })
        
        # Add regime if provided
        if regime_id is not None:
            # Get all regime columns from training
            if hasattr(self.meta_learner, 'feature_names_in_'):
                regime_cols = [c for c in self.meta_learner.feature_names_in_ if c.startswith('regime_')]
                # Initialize all regime columns to 0
                for col in regime_cols:
                    X_meta[col] = 0
                # Set the active regime to 1
                active_regime_col = f'regime_{regime_id}'
                if active_regime_col in X_meta.columns:
                    X_meta[active_regime_col] = 1
        
        # Level 2 meta-prediction
        final_prob = self.meta_learner.predict_proba(X_meta)[:, 1][0]
        
        return float(final_prob)
    
    def predict_with_components(
        self,
        X_patterns: pd.DataFrame,
        X_research: pd.DataFrame,
        X_dark_pool: pd.DataFrame,
        regime_id: Optional[int] = None
    ) -> Dict[str, float]:
        """
        Predict with component breakdowns for explainability.
        
        Returns:
            dict: {
                'final_prob': float,
                'pattern_score': float,
                'research_score': float,
                'dark_pool_score': float
            }
        """
        if not self.is_trained:
            return {
                'final_prob': 0.5,
                'pattern_score': 0.5,
                'research_score': 0.5,
                'dark_pool_score': 0.5
            }
        
        # Level 1 predictions
        p_pattern = self.pattern_model.predict_proba(X_patterns)[:, 1][0]
        p_research = self.research_model.predict_proba(X_research)[:, 1][0]
        p_dark_pool = self.dark_pool_model.predict_proba(X_dark_pool)[:, 1][0]
        
        # Build meta-features
        X_meta = pd.DataFrame({
            'score_pattern': [p_pattern],
            'score_research': [p_research],
            'score_dark_pool': [p_dark_pool]
        })
        
        if regime_id is not None:
            # Get all regime columns from training
            if hasattr(self.meta_learner, 'feature_names_in_'):
                regime_cols = [c for c in self.meta_learner.feature_names_in_ if c.startswith('regime_')]
                # Initialize all regime columns to 0
                for col in regime_cols:
                    X_meta[col] = 0
                # Set the active regime to 1
                active_regime_col = f'regime_{regime_id}'
                if active_regime_col in X_meta.columns:
                    X_meta[active_regime_col] = 1
        
        # Final prediction
        final_prob = self.meta_learner.predict_proba(X_meta)[:, 1][0]
        
        return {
            'final_prob': float(final_prob),
            'pattern_score': float(p_pattern),
            'research_score': float(p_research),
            'dark_pool_score': float(p_dark_pool)
        }
    
    def get_feature_importance(self) -> pd.DataFrame:
        """
        Get feature importance from Level 2 meta-learner.
        
        Returns:
            DataFrame with feature names and importance scores
        """
        if not self.is_trained:
            return pd.DataFrame()
        
        importance = self.meta_learner.feature_importances_
        
        # Get actual feature names from the trained model
        if hasattr(self.meta_learner, 'feature_names_in_'):
            feature_names = self.meta_learner.feature_names_in_
        else:
            feature_names = [f'feature_{i}' for i in range(len(importance))]
        
        return pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
    
    def save(self, filepath: str):
        """Save trained model to disk."""
        if not self.is_trained:
            logger.warning("Cannot save untrained model.")
            return
        
        model_data = {
            'pattern_model': self.pattern_model,
            'research_model': self.research_model,
            'dark_pool_model': self.dark_pool_model,
            'meta_learner': self.meta_learner,
            'training_metrics': self.training_metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        joblib.dump(model_data, filepath)
        logger.info(f"Model saved to {filepath}")
    
    def load(self, filepath: str):
        """Load trained model from disk."""
        model_data = joblib.load(filepath)
        
        self.pattern_model = model_data['pattern_model']
        self.research_model = model_data['research_model']
        self.dark_pool_model = model_data['dark_pool_model']
        self.meta_learner = model_data['meta_learner']
        self.training_metrics = model_data['training_metrics']
        self.is_trained = True
        
        logger.info(f"Model loaded from {filepath}")
        logger.info(f"  Training date: {model_data['timestamp']}")
        logger.info(f"  Validation AUC: {self.training_metrics['val_auc']:.4f}")


if __name__ == "__main__":
    # ===== EXAMPLE USAGE =====
    print("=" * 80)
    print("HIERARCHICAL META-LEARNER - TEST")
    print("=" * 80)
    
    # Generate synthetic training data
    np.random.seed(42)
    n_samples = 500
    
    # Pattern features (5 features: EMA cross, candlestick, S/R, etc.)
    X_patterns = pd.DataFrame(
        np.random.randn(n_samples, 5),
        columns=['ema_cross', 'candlestick_score', 'sr_proximity', 'trend_strength', 'breakout']
    )
    
    # Research features (20 features simplified)
    X_research = pd.DataFrame(
        np.random.randn(n_samples, 20),
        columns=[f'research_feat_{i}' for i in range(20)]
    )
    
    # Dark pool features (5 from Module 1)
    X_dark_pool = pd.DataFrame(
        np.random.uniform(0, 100, (n_samples, 5)),
        columns=['IFI', 'AD', 'OBV', 'VROC', 'SMI']
    )
    
    # Labels (simulate 58% win rate)
    y = pd.Series(np.random.choice([0, 1], size=n_samples, p=[0.42, 0.58]))
    
    # Regime IDs (12 regimes, but simplified to 3 for demo)
    regime_ids = pd.Series(np.random.choice([0, 1, 2], size=n_samples))
    
    # ===== Train Ensemble =====
    learner = HierarchicalMetaLearner(max_depth=2, learning_rate=0.05)
    metrics = learner.train_ensemble(
        X_patterns, X_research, X_dark_pool, y, regime_ids
    )
    
    # ===== Test Prediction =====
    print("\n" + "=" * 80)
    print("PREDICTION TEST")
    print("=" * 80)
    
    # Single sample
    X_patterns_test = X_patterns.iloc[[0]]
    X_research_test = X_research.iloc[[0]]
    X_dark_pool_test = X_dark_pool.iloc[[0]]
    
    prob = learner.predict(X_patterns_test, X_research_test, X_dark_pool_test, regime_id=0)
    print(f"\nPredicted Probability: {prob:.4f}")
    
    # With components
    components = learner.predict_with_components(
        X_patterns_test, X_research_test, X_dark_pool_test, regime_id=0
    )
    
    print(f"\nComponent Breakdown:")
    for key, value in components.items():
        print(f"  {key}: {value:.4f}")
    
    # Feature importance
    print(f"\n" + "=" * 80)
    print("FEATURE IMPORTANCE (Level 2 Meta-Learner)")
    print("=" * 80)
    importance_df = learner.get_feature_importance()
    print(importance_df.to_string(index=False))
    
    print("\nâœ… Meta-Learner implementation complete!")
