{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ddf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 0: SETUP & DATA LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "# Install TA-Lib (requires system library first)\n",
    "!wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
    "!tar -xzf ta-lib-0.4.0-src.tar.gz\n",
    "%cd ta-lib/\n",
    "!./configure --prefix=/usr > /dev/null 2>&1\n",
    "!make > /dev/null 2>&1\n",
    "!make install > /dev/null 2>&1\n",
    "%cd ..\n",
    "!pip install -q TA-Lib\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q yfinance scikit-learn lightgbm optuna pandas numpy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU check\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"üñ•Ô∏è GPU Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "except:\n",
    "    print(\"üñ•Ô∏è PyTorch not installed (not required for this notebook)\")\n",
    "\n",
    "# Download training data\n",
    "TICKERS = ['SPY', 'QQQ', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMD', 'META', 'GOOGL', 'AMZN']\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"\\nüì• Downloading data for {len(TICKERS)} tickers...\")\n",
    "all_data = {}\n",
    "for ticker in TICKERS:\n",
    "    df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False, auto_adjust=True)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    all_data[ticker] = df\n",
    "    print(f\"  ‚úì {ticker}: {len(df)} rows\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded: {sum(len(d) for d in all_data.values())} total rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd095d51",
   "metadata": {},
   "source": [
    "---\n",
    "# üìà PHASE 1: FORECAST ENGINE OPTIMIZATION\n",
    "\n",
    "The `forecast_engine.py` uses a model to predict price direction and generates 24-day forecasts.\n",
    "\n",
    "**What we'll optimize:**\n",
    "1. Best model type (HistGB vs LightGBM vs RandomForest)\n",
    "2. Optimal forecast horizon (7d vs 14d vs 21d)\n",
    "3. Decay parameters (when to start fading forecast)\n",
    "4. ATR scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c951e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 1A: FEATURE ENGINEERING FOR FORECASTER\n",
    "# ==============================================================================\n",
    "def engineer_forecast_features(df):\n",
    "    \"\"\"Engineer features for price direction prediction.\"\"\"\n",
    "    close = np.asarray(df['Close'].values, dtype='float64')\n",
    "    high = np.asarray(df['High'].values, dtype='float64')\n",
    "    low = np.asarray(df['Low'].values, dtype='float64')\n",
    "    volume = np.asarray(df['Volume'].values, dtype='float64')\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # RSI variants\n",
    "    features['rsi_7'] = talib.RSI(close, timeperiod=7)\n",
    "    features['rsi_14'] = talib.RSI(close, timeperiod=14)\n",
    "    features['rsi_21'] = talib.RSI(close, timeperiod=21)\n",
    "    \n",
    "    # MACD\n",
    "    macd, macd_signal, macd_hist = talib.MACD(close)\n",
    "    features['macd'] = macd\n",
    "    features['macd_signal'] = macd_signal\n",
    "    features['macd_hist'] = macd_hist\n",
    "    \n",
    "    # Trend indicators\n",
    "    features['adx'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "    features['cci'] = talib.CCI(high, low, close, timeperiod=14)\n",
    "    features['willr'] = talib.WILLR(high, low, close, timeperiod=14)\n",
    "    \n",
    "    # Volatility\n",
    "    features['atr_14'] = talib.ATR(high, low, close, timeperiod=14)\n",
    "    features['atr_pct'] = features['atr_14'] / close * 100\n",
    "    bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20)\n",
    "    features['bb_width'] = (bb_upper - bb_lower) / bb_middle\n",
    "    features['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower + 1e-9)\n",
    "    \n",
    "    # EMAs and trends\n",
    "    features['ema_8'] = talib.EMA(close, timeperiod=8)\n",
    "    features['ema_21'] = talib.EMA(close, timeperiod=21)\n",
    "    features['ema_50'] = talib.EMA(close, timeperiod=50)\n",
    "    features['trend_short'] = (close - features['ema_8']) / features['ema_8'] * 100\n",
    "    features['trend_med'] = (close - features['ema_21']) / features['ema_21'] * 100\n",
    "    features['trend_long'] = (close - features['ema_50']) / features['ema_50'] * 100\n",
    "    \n",
    "    # Momentum\n",
    "    features['mom_5'] = talib.MOM(close, timeperiod=5)\n",
    "    features['mom_10'] = talib.MOM(close, timeperiod=10)\n",
    "    features['roc_5'] = talib.ROC(close, timeperiod=5)\n",
    "    features['roc_10'] = talib.ROC(close, timeperiod=10)\n",
    "    \n",
    "    # Volume\n",
    "    features['obv'] = talib.OBV(close, volume)\n",
    "    features['vol_sma'] = talib.SMA(volume, timeperiod=20)\n",
    "    features['vol_ratio'] = volume / (features['vol_sma'] + 1e-9)\n",
    "    features['mfi'] = talib.MFI(high, low, close, volume, timeperiod=14)\n",
    "    \n",
    "    # Returns\n",
    "    features['ret_1d'] = pd.Series(close, index=df.index).pct_change(1).values\n",
    "    features['ret_5d'] = pd.Series(close, index=df.index).pct_change(5).values\n",
    "    features['ret_10d'] = pd.Series(close, index=df.index).pct_change(10).values\n",
    "    features['ret_21d'] = pd.Series(close, index=df.index).pct_change(21).values\n",
    "    \n",
    "    # Clean\n",
    "    features = features.replace([np.inf, -np.inf], np.nan)\n",
    "    features = features.bfill().ffill().fillna(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_labels(df, horizon, threshold=0.02):\n",
    "    \"\"\"Create direction labels: 0=BEARISH, 1=NEUTRAL, 2=BULLISH\"\"\"\n",
    "    close = df['Close'].values\n",
    "    \n",
    "    # Calculate future returns as numpy array\n",
    "    future_ret = np.zeros(len(close))\n",
    "    future_ret[:-horizon] = (close[horizon:] - close[:-horizon]) / close[:-horizon]\n",
    "    future_ret[-horizon:] = np.nan\n",
    "    \n",
    "    # Create labels as numpy array first, then convert to Series\n",
    "    labels = np.ones(len(close), dtype=int)  # Default NEUTRAL (1)\n",
    "    labels[future_ret > threshold] = 2   # BULLISH\n",
    "    labels[future_ret < -threshold] = 0  # BEARISH\n",
    "    \n",
    "    return pd.Series(labels, index=df.index)\n",
    "\n",
    "def create_adaptive_labels(df, horizon, atr_multiplier):\n",
    "    \"\"\"ATR-adaptive labels for varying volatility.\"\"\"\n",
    "    close = np.asarray(df['Close'].values, dtype='float64')\n",
    "    high = np.asarray(df['High'].values, dtype='float64')\n",
    "    low = np.asarray(df['Low'].values, dtype='float64')\n",
    "    \n",
    "    atr = talib.ATR(high, low, close, timeperiod=14)\n",
    "    atr_pct = atr / close\n",
    "    adaptive_thr = atr_pct * atr_multiplier\n",
    "    adaptive_thr = np.clip(adaptive_thr, 0.005, 0.10)\n",
    "    \n",
    "    # Calculate future returns as numpy array\n",
    "    future_ret = np.zeros(len(close))\n",
    "    future_ret[:-horizon] = (close[horizon:] - close[:-horizon]) / close[:-horizon]\n",
    "    future_ret[-horizon:] = np.nan\n",
    "    \n",
    "    # Create labels as numpy array\n",
    "    labels = np.ones(len(close), dtype=int)  # Default NEUTRAL (1)\n",
    "    labels[future_ret > adaptive_thr] = 2   # BULLISH\n",
    "    labels[future_ret < -adaptive_thr] = 0  # BEARISH\n",
    "    \n",
    "    return pd.Series(labels, index=df.index)\n",
    "\n",
    "print(\"‚úÖ Feature engineering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 1B: FIND OPTIMAL FORECAST MODEL & HORIZON\n",
    "# ==============================================================================\n",
    "from itertools import product\n",
    "\n",
    "print(\"üî¨ FORECASTER OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test configurations\n",
    "HORIZONS = [5, 7, 10, 14, 21]\n",
    "THRESHOLDS = [0.015, 0.02, 0.025, 0.03]\n",
    "\n",
    "model_configs = {\n",
    "    'HistGB': HistGradientBoostingClassifier(max_iter=200, max_depth=8, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'LogisticReg': LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for ticker in TICKERS[:5]:  # Test on subset first\n",
    "    df = all_data[ticker]\n",
    "    features = engineer_forecast_features(df)\n",
    "    \n",
    "    for horizon, threshold in product(HORIZONS, THRESHOLDS):\n",
    "        labels = create_labels(df, horizon, threshold)\n",
    "        \n",
    "        # Align and clean\n",
    "        valid_idx = features.dropna().index.intersection(labels.dropna().index)\n",
    "        X = features.loc[valid_idx].iloc[:-horizon]\n",
    "        y = labels.loc[valid_idx].iloc[:-horizon]\n",
    "        \n",
    "        if len(X) < 200:\n",
    "            continue\n",
    "        \n",
    "        # Time series split\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        for model_name, model in model_configs.items():\n",
    "            try:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                # Calculate actual trading performance\n",
    "                test_df = df.iloc[split_idx:split_idx + len(y_test)]\n",
    "                actual_returns = test_df['Close'].pct_change(horizon).shift(-horizon).iloc[:-horizon]\n",
    "                \n",
    "                # Only trade when model predicts BULLISH (2)\n",
    "                buy_signals = y_pred[:-horizon] == 2\n",
    "                if buy_signals.sum() > 0:\n",
    "                    trade_returns = actual_returns.iloc[:len(buy_signals)][buy_signals]\n",
    "                    win_rate = (trade_returns > 0).mean() * 100\n",
    "                    avg_return = trade_returns.mean() * 100\n",
    "                else:\n",
    "                    win_rate = 0\n",
    "                    avg_return = 0\n",
    "                \n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'model': model_name,\n",
    "                    'horizon': horizon,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': acc,\n",
    "                    'win_rate': win_rate,\n",
    "                    'avg_return': avg_return,\n",
    "                    'n_trades': buy_signals.sum()\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "# Analyze results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS (by Win Rate):\")\n",
    "top_configs = results_df.groupby(['model', 'horizon', 'threshold']).agg({\n",
    "    'win_rate': 'mean',\n",
    "    'avg_return': 'mean',\n",
    "    'n_trades': 'sum'\n",
    "}).sort_values('win_rate', ascending=False).head(10)\n",
    "print(top_configs)\n",
    "\n",
    "# Best config\n",
    "best_row = top_configs.iloc[0]\n",
    "best_model, best_horizon, best_threshold = top_configs.index[0]\n",
    "print(f\"\\nüèÜ BEST CONFIG: {best_model}, Horizon={best_horizon}d, Threshold={best_threshold}\")\n",
    "print(f\"   Win Rate: {best_row['win_rate']:.1f}%\")\n",
    "print(f\"   Avg Return: {best_row['avg_return']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 1C: OPTIMIZE FORECAST DECAY PARAMETERS\n",
    "# ==============================================================================\n",
    "print(\"\\nüî¨ OPTIMIZING FORECAST DECAY PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different decay configurations\n",
    "DECAY_STARTS = [5, 7, 10, 12, 15]\n",
    "DECAY_RATES = [0.05, 0.1, 0.15, 0.2]  # How fast to decay per day\n",
    "\n",
    "decay_results = []\n",
    "\n",
    "for ticker in TICKERS[:3]:\n",
    "    df = all_data[ticker]\n",
    "    features = engineer_forecast_features(df)\n",
    "    labels = create_labels(df, best_horizon, best_threshold)\n",
    "    \n",
    "    valid_idx = features.dropna().index.intersection(labels.dropna().index)\n",
    "    X = features.loc[valid_idx].iloc[:-best_horizon]\n",
    "    y = labels.loc[valid_idx].iloc[:-best_horizon]\n",
    "    \n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = HistGradientBoostingClassifier(max_iter=200, max_depth=8, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get probabilities\n",
    "    proba = model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    test_df = df.iloc[split_idx:split_idx + len(y_test)]\n",
    "    \n",
    "    for decay_start, decay_rate in product(DECAY_STARTS, DECAY_RATES):\n",
    "        # Simulate forecast accuracy with decay\n",
    "        forecast_errors = []\n",
    "        \n",
    "        for i in range(len(proba) - 24):\n",
    "            confidence = proba[i].max()\n",
    "            predicted_direction = 1 if proba[i][2] > proba[i][0] else -1\n",
    "            \n",
    "            # Generate 24-day forecast with decay\n",
    "            cumulative_error = 0\n",
    "            for day in range(1, 25):\n",
    "                # Apply decay\n",
    "                if day > decay_start:\n",
    "                    decay_factor = 1 - decay_rate * (day - decay_start)\n",
    "                    decay_factor = max(0, decay_factor)\n",
    "                else:\n",
    "                    decay_factor = 1.0\n",
    "                \n",
    "                effective_conf = confidence * decay_factor\n",
    "                \n",
    "                if i + day < len(test_df):\n",
    "                    actual_move = (test_df['Close'].iloc[i + day] / test_df['Close'].iloc[i] - 1) * 100\n",
    "                    predicted_move = predicted_direction * effective_conf * 2  # Scaled prediction\n",
    "                    cumulative_error += abs(actual_move - predicted_move)\n",
    "            \n",
    "            forecast_errors.append(cumulative_error / 24)\n",
    "        \n",
    "        decay_results.append({\n",
    "            'ticker': ticker,\n",
    "            'decay_start': decay_start,\n",
    "            'decay_rate': decay_rate,\n",
    "            'mean_error': np.mean(forecast_errors)\n",
    "        })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_results)\n",
    "best_decay = decay_df.groupby(['decay_start', 'decay_rate'])['mean_error'].mean().idxmin()\n",
    "print(f\"\\nüèÜ BEST DECAY CONFIG: Start at day {best_decay[0]}, Rate={best_decay[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2755f",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ñ PHASE 2: AI RECOMMENDER OPTIMIZATION\n",
    "\n",
    "The `ai_recommender.py` predicts BUY/HOLD/SELL signals.\n",
    "\n",
    "**What we'll optimize:**\n",
    "1. Best features (feature importance ranking)\n",
    "2. Optimal label threshold (ATR-adaptive vs fixed)\n",
    "3. Model hyperparameters\n",
    "4. Confidence calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2A: FEATURE IMPORTANCE ANALYSIS\n",
    "# ==============================================================================\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "\n",
    "print(\"üî¨ AI RECOMMENDER FEATURE IMPORTANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all ticker data\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    df = all_data[ticker]\n",
    "    features = engineer_forecast_features(df)\n",
    "    labels = create_labels(df, 7, 0.02)\n",
    "    \n",
    "    valid_idx = features.dropna().index.intersection(labels.dropna().index)\n",
    "    X = features.loc[valid_idx].iloc[:-7]\n",
    "    y = labels.loc[valid_idx].iloc[:-7]\n",
    "    \n",
    "    all_features.append(X)\n",
    "    all_labels.append(y)\n",
    "\n",
    "X_combined = pd.concat(all_features)\n",
    "y_combined = pd.concat(all_labels)\n",
    "\n",
    "# Feature importance via mutual information\n",
    "mi_scores = mutual_info_classif(X_combined, y_combined, random_state=42)\n",
    "f_scores, _ = f_classif(X_combined, y_combined)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_combined.columns,\n",
    "    'mi_score': mi_scores,\n",
    "    'f_score': f_scores\n",
    "})\n",
    "\n",
    "# Normalize and combine\n",
    "feature_importance['mi_norm'] = feature_importance['mi_score'] / feature_importance['mi_score'].max()\n",
    "feature_importance['f_norm'] = feature_importance['f_score'] / feature_importance['f_score'].max()\n",
    "feature_importance['combined'] = (feature_importance['mi_norm'] + feature_importance['f_norm']) / 2\n",
    "feature_importance = feature_importance.sort_values('combined', ascending=False)\n",
    "\n",
    "print(\"\\nüìä TOP 15 FEATURES FOR AI RECOMMENDER:\")\n",
    "print(feature_importance[['feature', 'combined']].head(15).to_string())\n",
    "\n",
    "TOP_FEATURES = feature_importance['feature'].head(15).tolist()\n",
    "print(f\"\\n‚úÖ Selected {len(TOP_FEATURES)} top features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2B: ADAPTIVE THRESHOLD OPTIMIZATION\n",
    "# ==============================================================================\n",
    "print(\"\\nüî¨ OPTIMIZING ADAPTIVE LABEL THRESHOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Note: create_adaptive_labels is already defined in Phase 1A cell\n",
    "# No need to redefine it here\n",
    "\n",
    "# Test different ATR multipliers\n",
    "ATR_MULTIPLIERS = [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]\n",
    "\n",
    "threshold_results = []\n",
    "\n",
    "for atr_mult in ATR_MULTIPLIERS:\n",
    "    all_wr = []\n",
    "    all_returns = []\n",
    "    \n",
    "    for ticker in TICKERS[:5]:\n",
    "        df = all_data[ticker]\n",
    "        features = engineer_forecast_features(df)[TOP_FEATURES]\n",
    "        labels = create_adaptive_labels(df, 7, atr_mult)\n",
    "        \n",
    "        valid_idx = features.dropna().index.intersection(labels.dropna().index)\n",
    "        X = features.loc[valid_idx].iloc[:-7]\n",
    "        y = labels.loc[valid_idx].iloc[:-7]\n",
    "        \n",
    "        if len(X) < 200:\n",
    "            continue\n",
    "        \n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        model = HistGradientBoostingClassifier(max_iter=200, random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        test_df = df.iloc[split_idx:split_idx + len(y_test)]\n",
    "        actual_returns = test_df['Close'].pct_change(7).shift(-7).iloc[:-7]\n",
    "        \n",
    "        buy_signals = y_pred[:-7] == 2\n",
    "        if buy_signals.sum() > 0:\n",
    "            trade_returns = actual_returns.iloc[:len(buy_signals)][buy_signals]\n",
    "            all_wr.append((trade_returns > 0).mean() * 100)\n",
    "            all_returns.append(trade_returns.mean() * 100)\n",
    "    \n",
    "    if all_wr:\n",
    "        threshold_results.append({\n",
    "            'atr_multiplier': atr_mult,\n",
    "            'avg_win_rate': np.mean(all_wr),\n",
    "            'avg_return': np.mean(all_returns)\n",
    "        })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "print(\"\\nüìä ATR MULTIPLIER COMPARISON:\")\n",
    "print(threshold_df.to_string(index=False))\n",
    "\n",
    "best_atr_mult = threshold_df.loc[threshold_df['avg_win_rate'].idxmax(), 'atr_multiplier']\n",
    "print(f\"\\nüèÜ BEST ATR MULTIPLIER: {best_atr_mult}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7efe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2C: HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
    "# ==============================================================================\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "print(\"\\nüî¨ HYPERPARAMETER OPTIMIZATION (Optuna)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare combined dataset\n",
    "all_X = []\n",
    "all_y = []\n",
    "for ticker in TICKERS:\n",
    "    df = all_data[ticker]\n",
    "    features = engineer_forecast_features(df)[TOP_FEATURES]\n",
    "    labels = create_adaptive_labels(df, 7, best_atr_mult)\n",
    "    \n",
    "    valid_idx = features.dropna().index.intersection(labels.dropna().index)\n",
    "    X = features.loc[valid_idx].iloc[:-7]\n",
    "    y = labels.loc[valid_idx].iloc[:-7]\n",
    "    all_X.append(X)\n",
    "    all_y.append(y)\n",
    "\n",
    "X_full = pd.concat(all_X)\n",
    "y_full = pd.concat(all_y)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 50),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.0, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = HistGradientBoostingClassifier(**params, random_state=42)\n",
    "    \n",
    "    # Time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = cross_val_score(model, X_scaled, y_full, cv=tscv, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nüèÜ BEST HYPERPARAMETERS:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(f\"   Best CV Accuracy: {study.best_value:.4f}\")\n",
    "\n",
    "BEST_AI_PARAMS = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0505d0",
   "metadata": {},
   "source": [
    "---\n",
    "# üí∞ PHASE 3: RISK MANAGER OPTIMIZATION\n",
    "\n",
    "Optimize position sizing and risk parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 3: RISK MANAGER OPTIMIZATION\n",
    "# ==============================================================================\n",
    "print(\"\\nüî¨ RISK MANAGER OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different risk parameters on backtest\n",
    "RISK_PER_TRADE = [0.005, 0.01, 0.015, 0.02, 0.025]  # 0.5% to 2.5%\n",
    "MAX_DAILY_LOSS = [0.01, 0.02, 0.03, 0.05]  # 1% to 5%\n",
    "MAX_POSITIONS = [3, 5, 7, 10]\n",
    "\n",
    "risk_results = []\n",
    "\n",
    "for risk_per_trade, max_daily, max_pos in product(RISK_PER_TRADE[:3], MAX_DAILY_LOSS[:3], MAX_POSITIONS[:3]):\n",
    "    # Simulate portfolio with these risk params\n",
    "    account = 10000\n",
    "    daily_pnl = 0\n",
    "    trades = 0\n",
    "    wins = 0\n",
    "    total_pnl = 0\n",
    "    max_drawdown = 0\n",
    "    peak = account\n",
    "    \n",
    "    for ticker in TICKERS[:5]:\n",
    "        df = all_data[ticker]\n",
    "        \n",
    "        for i in range(100, len(df) - 10, 5):  # Every 5 days\n",
    "            # Check daily loss limit\n",
    "            if daily_pnl < -max_daily * account:\n",
    "                continue\n",
    "            \n",
    "            # Position size based on risk\n",
    "            position_size = account * risk_per_trade / 0.05  # Assume 5% stop\n",
    "            position_size = min(position_size, account / max_pos)\n",
    "            \n",
    "            # Simulate trade (random for now, just testing risk limits)\n",
    "            returns = (df['Close'].iloc[i+5] / df['Close'].iloc[i] - 1)\n",
    "            trade_pnl = position_size * returns\n",
    "            \n",
    "            account += trade_pnl\n",
    "            daily_pnl += trade_pnl\n",
    "            total_pnl += trade_pnl\n",
    "            trades += 1\n",
    "            if trade_pnl > 0:\n",
    "                wins += 1\n",
    "            \n",
    "            # Track drawdown\n",
    "            if account > peak:\n",
    "                peak = account\n",
    "            drawdown = (peak - account) / peak\n",
    "            max_drawdown = max(max_drawdown, drawdown)\n",
    "    \n",
    "    if trades > 0:\n",
    "        risk_results.append({\n",
    "            'risk_per_trade': risk_per_trade,\n",
    "            'max_daily_loss': max_daily,\n",
    "            'max_positions': max_pos,\n",
    "            'total_return': (account - 10000) / 10000 * 100,\n",
    "            'max_drawdown': max_drawdown * 100,\n",
    "            'win_rate': wins / trades * 100,\n",
    "            'sharpe': total_pnl / (np.std([total_pnl]) + 0.001)\n",
    "        })\n",
    "\n",
    "risk_df = pd.DataFrame(risk_results)\n",
    "risk_df['risk_adj_return'] = risk_df['total_return'] / (risk_df['max_drawdown'] + 1)\n",
    "\n",
    "print(\"\\nüìä TOP 5 RISK CONFIGURATIONS (by Risk-Adjusted Return):\")\n",
    "print(risk_df.nlargest(5, 'risk_adj_return').to_string(index=False))\n",
    "\n",
    "best_risk = risk_df.loc[risk_df['risk_adj_return'].idxmax()]\n",
    "print(f\"\\nüèÜ OPTIMAL RISK SETTINGS:\")\n",
    "print(f\"   Risk per Trade: {best_risk['risk_per_trade']*100:.1f}%\")\n",
    "print(f\"   Max Daily Loss: {best_risk['max_daily_loss']*100:.1f}%\")\n",
    "print(f\"   Max Positions: {int(best_risk['max_positions'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8ea67",
   "metadata": {},
   "source": [
    "---\n",
    "# üåç PHASE 4: MARKET REGIME OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ccc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 4: MARKET REGIME THRESHOLDS\n",
    "# ==============================================================================\n",
    "print(\"\\nüî¨ MARKET REGIME THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different regime classification thresholds\n",
    "BULL_THRESHOLDS = [3, 5, 7, 10]  # 21d return > X% = bull\n",
    "BEAR_THRESHOLDS = [-3, -5, -7, -10]\n",
    "ADX_TREND_THRESHOLDS = [20, 25, 30, 35]\n",
    "\n",
    "def classify_regime(df, bull_thr, bear_thr, adx_thr):\n",
    "    \"\"\"Classify market regime.\"\"\"\n",
    "    close = np.asarray(df['Close'].values, dtype='float64')\n",
    "    high = np.asarray(df['High'].values, dtype='float64')\n",
    "    low = np.asarray(df['Low'].values, dtype='float64')\n",
    "    \n",
    "    # Calculate 21-day returns as numpy array\n",
    "    ret_21d = np.zeros(len(close))\n",
    "    ret_21d[21:] = (close[21:] - close[:-21]) / close[:-21] * 100\n",
    "    ret_21d[:21] = np.nan\n",
    "    \n",
    "    # ADX as numpy array\n",
    "    adx = talib.ADX(high, low, close, timeperiod=14)\n",
    "    \n",
    "    # Create regimes as numpy array first\n",
    "    regimes = np.array(['sideways'] * len(close), dtype=object)\n",
    "    \n",
    "    # Apply conditions using numpy boolean arrays\n",
    "    bull_mask = (ret_21d > bull_thr) & (adx > adx_thr)\n",
    "    bear_mask = ret_21d < bear_thr\n",
    "    \n",
    "    regimes[bull_mask] = 'bull'\n",
    "    regimes[bear_mask] = 'bear'\n",
    "    \n",
    "    return pd.Series(regimes, index=df.index)\n",
    "\n",
    "regime_results = []\n",
    "\n",
    "for bull_thr, bear_thr, adx_thr in product(BULL_THRESHOLDS, BEAR_THRESHOLDS, ADX_TREND_THRESHOLDS):\n",
    "    regime_performance = {'bull': [], 'bear': [], 'sideways': []}\n",
    "    \n",
    "    for ticker in TICKERS[:5]:\n",
    "        df = all_data[ticker]\n",
    "        regimes = classify_regime(df, bull_thr, bear_thr, adx_thr)\n",
    "        \n",
    "        # Calculate forward returns by regime\n",
    "        fwd_ret = df['Close'].pct_change(5).shift(-5) * 100\n",
    "        \n",
    "        for regime in ['bull', 'bear', 'sideways']:\n",
    "            mask = regimes == regime\n",
    "            if mask.sum() > 10:\n",
    "                regime_returns = fwd_ret[mask].dropna()\n",
    "                regime_performance[regime].extend(regime_returns.tolist())\n",
    "    \n",
    "    # We want bull regime to have positive returns, bear negative\n",
    "    bull_ret = np.mean(regime_performance['bull']) if regime_performance['bull'] else 0\n",
    "    bear_ret = np.mean(regime_performance['bear']) if regime_performance['bear'] else 0\n",
    "    \n",
    "    # Score: bull should be positive, bear should be negative (good classification)\n",
    "    score = bull_ret - bear_ret  # Higher is better\n",
    "    \n",
    "    regime_results.append({\n",
    "        'bull_thr': bull_thr,\n",
    "        'bear_thr': bear_thr,\n",
    "        'adx_thr': adx_thr,\n",
    "        'bull_ret': bull_ret,\n",
    "        'bear_ret': bear_ret,\n",
    "        'score': score\n",
    "    })\n",
    "\n",
    "regime_df = pd.DataFrame(regime_results)\n",
    "print(\"\\nüìä TOP 5 REGIME CONFIGURATIONS:\")\n",
    "print(regime_df.nlargest(5, 'score').to_string(index=False))\n",
    "\n",
    "best_regime = regime_df.loc[regime_df['score'].idxmax()]\n",
    "print(f\"\\nüèÜ OPTIMAL REGIME THRESHOLDS:\")\n",
    "print(f\"   Bull: 21d return > {best_regime['bull_thr']}%\")\n",
    "print(f\"   Bear: 21d return < {best_regime['bear_thr']}%\")\n",
    "print(f\"   ADX Trend: > {best_regime['adx_thr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a597fe",
   "metadata": {},
   "source": [
    "---\n",
    "# üì¶ PHASE 5: SAVE OPTIMIZED CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 5: GENERATE OPTIMIZED CONFIGURATION FILE\n",
    "# ==============================================================================\n",
    "import json\n",
    "\n",
    "print(\"\\nüì¶ SAVING OPTIMIZED CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimized_config = {\n",
    "    'generated_at': datetime.now().isoformat(),\n",
    "    'training_data': {\n",
    "        'tickers': TICKERS,\n",
    "        'start_date': START_DATE,\n",
    "        'end_date': END_DATE\n",
    "    },\n",
    "    'forecast_engine': {\n",
    "        'best_model': best_model,\n",
    "        'horizon_days': int(best_horizon),\n",
    "        'threshold': float(best_threshold),\n",
    "        'decay_start_day': int(best_decay[0]),\n",
    "        'decay_rate': float(best_decay[1])\n",
    "    },\n",
    "    'ai_recommender': {\n",
    "        'top_features': TOP_FEATURES,\n",
    "        'atr_multiplier': float(best_atr_mult),\n",
    "        'model_params': BEST_AI_PARAMS\n",
    "    },\n",
    "    'risk_manager': {\n",
    "        'risk_per_trade': float(best_risk['risk_per_trade']),\n",
    "        'max_daily_loss': float(best_risk['max_daily_loss']),\n",
    "        'max_positions': int(best_risk['max_positions'])\n",
    "    },\n",
    "    'market_regime': {\n",
    "        'bull_threshold': float(best_regime['bull_thr']),\n",
    "        'bear_threshold': float(best_regime['bear_thr']),\n",
    "        'adx_trend_threshold': float(best_regime['adx_thr'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open('OPTIMIZED_FULL_STACK_CONFIG.json', 'w') as f:\n",
    "    json.dump(optimized_config, f, indent=2)\n",
    "\n",
    "print(json.dumps(optimized_config, indent=2))\n",
    "print(\"\\n‚úÖ Configuration saved to OPTIMIZED_FULL_STACK_CONFIG.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 5B: GENERATE PYTHON CONFIG FILE\n",
    "# ==============================================================================\n",
    "config_code = f'''\"\"\"\n",
    "OPTIMIZED FULL STACK CONFIGURATION\n",
    "===================================\n",
    "Generated by COLAB_FULL_STACK_OPTIMIZER.ipynb\n",
    "Date: {datetime.now().isoformat()}\n",
    "\n",
    "This file contains optimized parameters for all production modules.\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# FORECAST ENGINE SETTINGS\n",
    "# ==============================================================================\n",
    "FORECAST_CONFIG = {{\n",
    "    'model_type': '{best_model}',\n",
    "    'horizon_days': {best_horizon},\n",
    "    'prediction_threshold': {best_threshold},\n",
    "    'decay_start_day': {best_decay[0]},\n",
    "    'decay_rate': {best_decay[1]},\n",
    "    'max_forecast_days': 24\n",
    "}}\n",
    "\n",
    "# ==============================================================================\n",
    "# AI RECOMMENDER SETTINGS\n",
    "# ==============================================================================\n",
    "AI_RECOMMENDER_CONFIG = {{\n",
    "    'top_features': {TOP_FEATURES},\n",
    "    'atr_multiplier': {best_atr_mult},\n",
    "    'model_params': {BEST_AI_PARAMS}\n",
    "}}\n",
    "\n",
    "# ==============================================================================\n",
    "# RISK MANAGER SETTINGS\n",
    "# ==============================================================================\n",
    "RISK_CONFIG = {{\n",
    "    'risk_per_trade': {best_risk['risk_per_trade']},  # {best_risk['risk_per_trade']*100:.1f}%\n",
    "    'max_daily_loss': {best_risk['max_daily_loss']},  # {best_risk['max_daily_loss']*100:.1f}%\n",
    "    'max_positions': {int(best_risk['max_positions'])},\n",
    "    'default_stop_loss_pct': 0.05,  # 5%\n",
    "    'default_take_profit_pct': 0.10  # 10%\n",
    "}}\n",
    "\n",
    "# ==============================================================================\n",
    "# MARKET REGIME SETTINGS\n",
    "# ==============================================================================\n",
    "REGIME_CONFIG = {{\n",
    "    'bull_threshold': {best_regime['bull_thr']},  # 21d return > X%\n",
    "    'bear_threshold': {best_regime['bear_thr']},  # 21d return < X%\n",
    "    'adx_trend_threshold': {best_regime['adx_thr']},\n",
    "    'lookback_days': 21\n",
    "}}\n",
    "\n",
    "# ==============================================================================\n",
    "# COMBINED PRODUCTION STACK CONFIG\n",
    "# ==============================================================================\n",
    "PRODUCTION_CONFIG = {{\n",
    "    'forecast': FORECAST_CONFIG,\n",
    "    'ai_recommender': AI_RECOMMENDER_CONFIG,\n",
    "    'risk': RISK_CONFIG,\n",
    "    'regime': REGIME_CONFIG\n",
    "}}\n",
    "'''\n",
    "\n",
    "with open('optimized_stack_config.py', 'w') as f:\n",
    "    f.write(config_code)\n",
    "\n",
    "print(config_code)\n",
    "print(\"\\n‚úÖ Python config saved to optimized_stack_config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb6e5c",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ SUMMARY\n",
    "\n",
    "Run this notebook in Colab Pro with GPU for best performance.\n",
    "\n",
    "After running, copy these files back to your project:\n",
    "1. `OPTIMIZED_FULL_STACK_CONFIG.json`\n",
    "2. `optimized_stack_config.py`\n",
    "\n",
    "Then update your modules to use these optimized parameters!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
