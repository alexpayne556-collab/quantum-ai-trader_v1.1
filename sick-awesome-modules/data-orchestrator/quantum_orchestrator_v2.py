"""
Quantum Data Orchestrator v2
Production-grade async data fetching with advanced features.

DESIGN DECISIONS:
1. Semaphore-based rate limiting â†’ More accurate than simple counters
2. Exponential backoff â†’ Handle transient errors gracefully
3. Metrics tracking â†’ Inform routing decisions based on source performance
4. Circuit breaker integration â†’ Avoid hammering failing sources
5. Session reuse â†’ Significant performance boost
6. Plugin pattern â†’ Easy to add new source fetchers
"""

import asyncio
import aiohttp
import logging
from typing import Dict, List, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import deque
from abc import ABC, abstractmethod
import pandas as pd
import time

try:
    from quantum_api_config_v2 import get_config, SourceMetadata, QuantumAPIConfig
except ImportError:
    from backend.quantum_api_config_v2 import get_config, SourceMetadata, QuantumAPIConfig

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class FetchResult:
    """
    Unified result format for data fetching.
    
    WHY: Standardized format makes it easy for consumers (AI modules,
    web API) to process results regardless of source.
    """
    ticker: str
    source: str
    success: bool
    timestamp: datetime
    latency_ms: float  # Track for metrics
    data: Optional[pd.DataFrame] = None
    candles: int = 0
    error: Optional[str] = None
    metadata: Dict = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        """Convert to JSON-serializable dict."""
        return {
            'ticker': self.ticker,
            'source': self.source,
            'success': self.success,
            'timestamp': self.timestamp.isoformat(),
            'latency_ms': self.latency_ms,
            'data': self.data.to_dict('records') if self.data is not None else None,
            'candles': self.candles,
            'error': self.error,
            'metadata': self.metadata,
            'status': 'success' if self.success else 'failed'
        }


@dataclass
class SourceMetrics:
    """
    Track source performance metrics.
    
    WHY: Metrics inform smart routing decisions. If a source is slow or
    unreliable, we can deprioritize it dynamically.
    """
    source_name: str
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_latency_ms: float = 0.0
    recent_results: deque = field(default_factory=lambda: deque(maxlen=100))
    
    @property
    def success_rate(self) -> float:
        """Calculate success rate (0.0 to 1.0)."""
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests
    
    @property
    def average_latency_ms(self) -> float:
        """Calculate average latency."""
        if self.successful_requests == 0:
            return 0.0
        return self.total_latency_ms / self.successful_requests
    
    @property
    def recent_success_rate(self) -> float:
        """Success rate for last 100 requests."""
        if not self.recent_results:
            return 0.0
        return sum(self.recent_results) / len(self.recent_results)
    
    def record_success(self, latency_ms: float):
        """Record a successful request."""
        self.total_requests += 1
        self.successful_requests += 1
        self.total_latency_ms += latency_ms
        self.recent_results.append(1)
    
    def record_failure(self):
        """Record a failed request."""
        self.total_requests += 1
        self.failed_requests += 1
        self.recent_results.append(0)
    
    def get_summary(self) -> Dict:
        """Get metrics summary."""
        return {
            'source': self.source_name,
            'total_requests': self.total_requests,
            'success_rate': f"{self.success_rate:.1%}",
            'recent_success_rate': f"{self.recent_success_rate:.1%}",
            'avg_latency_ms': f"{self.average_latency_ms:.0f}ms"
        }


class BaseFetcher(ABC):
    """
    Abstract base for source fetchers.
    
    WHY: Plugin pattern makes it trivial to add new sources.
    Each source just implements fetch() method.
    """
    
    def __init__(self, source: SourceMetadata, session: aiohttp.ClientSession):
        self.source = source
        self.session = session
    
    @abstractmethod
    async def fetch(self, ticker: str, days: int) -> FetchResult:
        """Fetch data for ticker. Each source implements this."""
        pass
    
    async def fetch_with_retry(self, ticker: str, days: int, max_retries: int = 2) -> FetchResult:
        """
        Fetch with exponential backoff for transient errors.
        
        WHY: Network blips and temporary 500 errors shouldn't fail the fetch.
        Exponential backoff gives the service time to recover.
        """
        for attempt in range(max_retries + 1):
            result = await self.fetch(ticker, days)
            
            # Success or non-retryable error â†’ return immediately
            if result.success or not self._is_retryable_error(result.error):
                return result
            
            # Retryable error â†’ wait and try again
            if attempt < max_retries:
                wait_time = (2 ** attempt) * 1.0  # 1s, 2s, 4s
                logger.debug(f"Retry {attempt + 1}/{max_retries} for {ticker} from {self.source.name} after {wait_time}s")
                await asyncio.sleep(wait_time)
        
        return result
    
    def _is_retryable_error(self, error: Optional[str]) -> bool:
        """
        Determine if error is retryable.
        
        WHY: Not all errors should be retried. Auth errors (403) won't
        be fixed by retrying. Timeouts and 500s might be.
        """
        if not error:
            return False
        
        retryable_indicators = [
            'timeout',
            'connection',
            'HTTP 500',
            'HTTP 502',
            'HTTP 503',
            'HTTP 504'
        ]
        
        return any(indicator.lower() in error.lower() for indicator in retryable_indicators)


class PolygonFetcher(BaseFetcher):
    """Polygon.io data fetcher."""
    
    async def fetch(self, ticker: str, days: int) -> FetchResult:
        start_time = time.time()
        
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            url = f"{self.source.base_url}/v2/aggs/ticker/{ticker}/range/1/day/{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}"
            params = {'apiKey': self.source.key, 'adjusted': 'true', 'sort': 'asc'}
            
            async with self.session.get(url, params=params) as response:
                latency_ms = (time.time() - start_time) * 1000
                
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('resultsCount', 0) > 0:
                        results = data['results']
                        df = pd.DataFrame(results)
                        
                        df.rename(columns={
                            't': 'timestamp', 'o': 'open', 'h': 'high',
                            'l': 'low', 'c': 'close', 'v': 'volume'
                        }, inplace=True)
                        
                        df['date'] = pd.to_datetime(df['timestamp'], unit='ms')
                        df.set_index('date', inplace=True)
                        df = df[['open', 'high', 'low', 'close', 'volume']].copy()
                        
                        return FetchResult(
                            ticker=ticker, source='Polygon', success=True,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            data=df, candles=len(df),
                            metadata={'resultsCount': data.get('resultsCount')}
                        )
                    else:
                        return FetchResult(
                            ticker=ticker, source='Polygon', success=False,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            error='No data returned'
                        )
                else:
                    error_text = await response.text()
                    return FetchResult(
                        ticker=ticker, source='Polygon', success=False,
                        timestamp=datetime.now(), latency_ms=latency_ms,
                        error=f'HTTP {response.status}: {error_text[:100]}'
                    )
        except asyncio.TimeoutError:
            return FetchResult(
                ticker=ticker, source='Polygon', success=False,
                timestamp=datetime.now(), latency_ms=(time.time() - start_time) * 1000,
                error='Request timeout'
            )
        except Exception as e:
            return FetchResult(
                ticker=ticker, source='Polygon', success=False,
                timestamp=datetime.now(), latency_ms=(time.time() - start_time) * 1000,
                error=str(e)
            )


class FMPFetcher(BaseFetcher):
    """Financial Modeling Prep data fetcher."""
    
    async def fetch(self, ticker: str, days: int) -> FetchResult:
        start_time = time.time()
        
        try:
            url = f"{self.source.base_url}/historical-price-full/{ticker}"
            params = {'apikey': self.source.key}
            
            async with self.session.get(url, params=params) as response:
                latency_ms = (time.time() - start_time) * 1000
                
                if response.status == 200:
                    data = await response.json()
                    
                    if 'historical' in data and len(data['historical']) > 0:
                        df = pd.DataFrame(data['historical'])
                        df['date'] = pd.to_datetime(df['date'])
                        df.set_index('date', inplace=True)
                        df.sort_index(inplace=True)
                        
                        cutoff_date = datetime.now() - timedelta(days=days)
                        df = df[df.index >= cutoff_date]
                        df = df[['open', 'high', 'low', 'close', 'volume']].copy()
                        
                        return FetchResult(
                            ticker=ticker, source='FMP', success=True,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            data=df, candles=len(df),
                            metadata={'symbol': data.get('symbol')}
                        )
                    else:
                        return FetchResult(
                            ticker=ticker, source='FMP', success=False,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            error='No historical data'
                        )
                else:
                    error_text = await response.text()
                    return FetchResult(
                        ticker=ticker, source='FMP', success=False,
                        timestamp=datetime.now(), latency_ms=latency_ms,
                        error=f'HTTP {response.status}: {error_text[:100]}'
                    )
        except Exception as e:
            return FetchResult(
                ticker=ticker, source='FMP', success=False,
                timestamp=datetime.now(), latency_ms=(time.time() - start_time) * 1000,
                error=str(e)
            )


class AlphaVantageFetcher(BaseFetcher):
    """Alpha Vantage data fetcher."""
    
    async def fetch(self, ticker: str, days: int) -> FetchResult:
        start_time = time.time()
        
        try:
            url = f"{self.source.base_url}/query"
            params = {
                'function': 'TIME_SERIES_DAILY_ADJUSTED',
                'symbol': ticker,
                'apikey': self.source.key,
                'outputsize': 'full'
            }
            
            async with self.session.get(url, params=params) as response:
                latency_ms = (time.time() - start_time) * 1000
                
                if response.status == 200:
                    data = await response.json()
                    
                    if 'Time Series (Daily)' in data:
                        time_series = data['Time Series (Daily)']
                        df = pd.DataFrame.from_dict(time_series, orient='index')
                        df.index = pd.to_datetime(df.index)
                        df.sort_index(inplace=True)
                        
                        df.rename(columns={
                            '1. open': 'open', '2. high': 'high', '3. low': 'low',
                            '4. close': 'close', '6. volume': 'volume'
                        }, inplace=True)
                        
                        for col in ['open', 'high', 'low', 'close', 'volume']:
                            df[col] = pd.to_numeric(df[col])
                        
                        cutoff_date = datetime.now() - timedelta(days=days)
                        df = df[df.index >= cutoff_date]
                        df = df[['open', 'high', 'low', 'close', 'volume']].copy()
                        
                        return FetchResult(
                            ticker=ticker, source='AlphaVantage', success=True,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            data=df, candles=len(df)
                        )
                    else:
                        error_msg = data.get('Note') or data.get('Error Message') or 'Unknown error'
                        return FetchResult(
                            ticker=ticker, source='AlphaVantage', success=False,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            error=error_msg
                        )
                else:
                    return FetchResult(
                        ticker=ticker, source='AlphaVantage', success=False,
                        timestamp=datetime.now(), latency_ms=latency_ms,
                        error=f'HTTP {response.status}'
                    )
        except Exception as e:
            return FetchResult(
                ticker=ticker, source='AlphaVantage', success=False,
                timestamp=datetime.now(), latency_ms=(time.time() - start_time) * 1000,
                error=str(e)
            )


class EODHDFetcher(BaseFetcher):
    """EODHD data fetcher."""
    
    async def fetch(self, ticker: str, days: int) -> FetchResult:
        start_time = time.time()
        
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            url = f"{self.source.base_url}/eod/{ticker}.US"
            params = {
                'api_token': self.source.key,
                'from': start_date.strftime('%Y-%m-%d'),
                'to': end_date.strftime('%Y-%m-%d'),
                'fmt': 'json'
            }
            
            async with self.session.get(url, params=params) as response:
                latency_ms = (time.time() - start_time) * 1000
                
                if response.status == 200:
                    data = await response.json()
                    
                    if isinstance(data, list) and len(data) > 0:
                        df = pd.DataFrame(data)
                        df['date'] = pd.to_datetime(df['date'])
                        df.set_index('date', inplace=True)
                        df.sort_index(inplace=True)
                        df = df[['open', 'high', 'low', 'close', 'volume']].copy()
                        
                        return FetchResult(
                            ticker=ticker, source='EODHD', success=True,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            data=df, candles=len(df)
                        )
                    else:
                        return FetchResult(
                            ticker=ticker, source='EODHD', success=False,
                            timestamp=datetime.now(), latency_ms=latency_ms,
                            error='No data returned'
                        )
                else:
                    return FetchResult(
                        ticker=ticker, source='EODHD', success=False,
                        timestamp=datetime.now(), latency_ms=latency_ms,
                        error=f'HTTP {response.status}'
                    )
        except Exception as e:
            return FetchResult(
                ticker=ticker, source='EODHD', success=False,
                timestamp=datetime.now(), latency_ms=(time.time() - start_time) * 1000,
                error=str(e)
            )


class QuantumOrchestrator:
    """
    Advanced data orchestration engine with production features.
    
    KEY FEATURES:
    1. Semaphore-based rate limiting (more accurate than counters)
    2. Circuit breaker integration (avoid hammering broken sources)
    3. Metrics tracking (success rates, latency)
    4. Exponential backoff (handle transient errors)
    5. Session reuse (performance boost)
    """
    
    def __init__(self, config: Optional[QuantumAPIConfig] = None):
        """
        Initialize orchestrator.
        
        WHY: Dependency injection allows testing with mock config,
        but defaults to global singleton for simplicity.
        """
        self.config = config or get_config()
        self.session: Optional[aiohttp.ClientSession] = None
        
        # WHY: Semaphores provide more accurate rate limiting than counters
        self._semaphores: Dict[str, asyncio.Semaphore] = {}
        
        # Track performance metrics for each source
        self.metrics: Dict[str, SourceMetrics] = {}
        
        # Initialize fetchers (plugin pattern)
        self._fetcher_registry: Dict[str, type] = {
            'polygon': PolygonFetcher,
            'fmp': FMPFetcher,
            'alphavantage': AlphaVantageFetcher,
            'eodhd': EODHDFetcher
        }
        
        self._initialize_metrics()
    
    def _initialize_metrics(self):
        """Initialize metrics for all valid sources."""
        for source in self.config.get_valid_sources():
            self.metrics[source.name] = SourceMetrics(source.name)
            # WHY: Semaphore limits concurrent requests, preventing rate limit violations
            self._semaphores[source.name] = asyncio.Semaphore(
                max(1, source.rate_limit_per_minute // 12)  # Spread over 5-second windows
            )
    
    async def __aenter__(self):
        """
        Async context manager entry.
        
        WHY: Session reuse significantly improves performance via connection pooling.
        """
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100)  # Connection pool
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.session:
            await self.session.close()
    
    async def _fetch_from_source(self, source: SourceMetadata, ticker: str, days: int) -> FetchResult:
        """
        Fetch from a specific source with rate limiting and metrics tracking.
        
        WHY: Centralized fetching logic ensures consistent rate limiting,
        metrics tracking, and error handling across all sources.
        """
        # Check circuit breaker
        if not self.config.is_source_available(source.name):
            logger.debug(f"Skipping {source.name} (circuit breaker open)")
            return FetchResult(
                ticker=ticker, source=source.name, success=False,
                timestamp=datetime.now(), latency_ms=0,
                error='Circuit breaker open'
            )
        
        # WHY: Semaphore ensures we don't exceed rate limits
        async with self._semaphores[source.name]:
            # Get appropriate fetcher
            fetcher_class = self._fetcher_registry.get(source.name)
            if not fetcher_class:
                return FetchResult(
                    ticker=ticker, source=source.name, success=False,
                    timestamp=datetime.now(), latency_ms=0,
                    error='No fetcher available'
                )
            
            fetcher = fetcher_class(source, self.session)
            result = await fetcher.fetch_with_retry(ticker, days)
            
            # Track metrics
            metrics = self.metrics[source.name]
            if result.success:
                metrics.record_success(result.latency_ms)
                self.config.mark_source_success(source.name)
            else:
                metrics.record_failure()
                self.config.mark_source_failure(source.name)
            
            return result
    
    async def fetch_with_fallback(self, ticker: str, days: int = 90) -> FetchResult:
        """
        Fetch data with intelligent fallback through priority-ordered sources.
        
        WHY: Fallback ensures high availability even if individual sources fail.
        Circuit breaker prevents wasting time on broken sources.
        """
        sources = self.config.get_valid_sources()
        
        if not sources:
            return FetchResult(
                ticker=ticker, source='None', success=False,
                timestamp=datetime.now(), latency_ms=0,
                error='No valid API sources configured'
            )
        
        logger.info(f"Fetching {ticker} with {len(sources)} sources available...")
        
        for source in sources:
            result = await self._fetch_from_source(source, ticker, days)
            
            if result.success:
                logger.info(f"âœ“ {ticker}: {result.candles} candles from {result.source} ({result.latency_ms:.0f}ms)")
                return result
            else:
                logger.warning(f"âœ— {source.name} failed for {ticker}: {result.error}")
        
        # All sources failed
        return FetchResult(
            ticker=ticker, source='All', success=False,
            timestamp=datetime.now(), latency_ms=0,
            error=f'All {len(sources)} sources failed'
        )
    
    async def fetch_multiple(self, tickers: List[str], days: int = 90) -> Dict[str, FetchResult]:
        """
        Fetch data for multiple tickers in parallel.
        
        WHY: Parallel fetching is ~2.5x faster than sequential for multiple tickers.
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"PARALLEL FETCH: {len(tickers)} tickers")
        logger.info(f"{'='*70}\n")
        
        tasks = [self.fetch_with_fallback(ticker, days) for ticker in tickers]
        results = await asyncio.gather(*tasks)
        
        result_dict = {result.ticker: result for result in results}
        
        # Summary
        successful = sum(1 for r in results if r.success)
        logger.info(f"\n{'='*70}")
        logger.info(f"FETCH COMPLETE: {successful}/{len(tickers)} successful")
        logger.info(f"{'='*70}\n")
        
        return result_dict
    
    def get_metrics_summary(self) -> Dict[str, Dict]:
        """
        Get performance metrics for all sources.
        
        WHY: Metrics help identify underperforming sources and inform
        routing decisions (e.g., prefer faster, more reliable sources).
        """
        return {name: metrics.get_summary() for name, metrics in self.metrics.items()}
    
    def register_fetcher(self, source_name: str, fetcher_class: type):
        """
        Register a custom fetcher (for extensibility).
        
        WHY: Makes it easy to add new sources (Reddit, insider feeds, etc.)
        without modifying core orchestrator code.
        """
        self._fetcher_registry[source_name.lower()] = fetcher_class
        logger.info(f"âœ“ Registered fetcher for {source_name}")


# ============================================================================
# CONVENIENCE FUNCTIONS
# ============================================================================

async def fetch_ticker(ticker: str, days: int = 90, config: Optional[QuantumAPIConfig] = None) -> FetchResult:
    """Convenience function to fetch a single ticker."""
    async with QuantumOrchestrator(config) as orchestrator:
        return await orchestrator.fetch_with_fallback(ticker, days)


async def fetch_tickers(tickers: List[str], days: int = 90, config: Optional[QuantumAPIConfig] = None) -> Dict[str, FetchResult]:
    """Convenience function to fetch multiple tickers in parallel."""
    async with QuantumOrchestrator(config) as orchestrator:
        return await orchestrator.fetch_multiple(tickers, days)


# ============================================================================
# TEST FUNCTIONS
# ============================================================================

async def test_single_ticker():
    """Test single ticker fetch with metrics."""
    print("\n" + "="*70)
    print("TEST 1: SINGLE TICKER FETCH WITH METRICS")
    print("="*70 + "\n")
    
    async with QuantumOrchestrator() as orch:
        result = await orch.fetch_with_fallback("SPY", days=30)
        
        print(f"Result: {result.ticker} | Source: {result.source} | Success: {result.success}")
        print(f"Latency: {result.latency_ms:.0f}ms | Candles: {result.candles}")
        
        if result.success:
            print(f"\nData Preview:")
            print(result.data.tail())
        
        print(f"\nðŸ“Š Metrics Summary:")
        for source, metrics in orch.get_metrics_summary().items():
            print(f"   {source}: {metrics}")


async def test_parallel_with_metrics():
    """Test parallel fetch with detailed metrics."""
    print("\n" + "="*70)
    print("TEST 2: PARALLEL FETCH WITH PERFORMANCE TRACKING")
    print("="*70 + "\n")
    
    tickers = ["AAPL", "MSFT", "GOOGL", "TSLA", "NVDA"]
    
    async with QuantumOrchestrator() as orch:
        results = await orch.fetch_multiple(tickers, days=30)
        
        print(f"\nðŸ“Š Results:")
        for ticker, result in results.items():
            status = "âœ“" if result.success else "âœ—"
            print(f"  {status} {ticker:6} | {result.source:15} | {result.latency_ms:4.0f}ms | {result.candles} candles")
        
        print(f"\nðŸ“ˆ Source Performance Metrics:")
        for source, metrics in orch.get_metrics_summary().items():
            print(f"   {metrics}")


async def test_circuit_breaker():
    """Test circuit breaker functionality."""
    print("\n" + "="*70)
    print("TEST 3: CIRCUIT BREAKER PATTERN")
    print("="*70 + "\n")
    
    config = get_config()
    
    # Simulate failures
    print("Simulating 3 failures for Polygon...")
    for i in range(3):
        config.mark_source_failure('polygon')
    
    print(f"Polygon available: {config.is_source_available('polygon')}")
    print(f"Circuit breaker status: {'OPEN' if not config.is_source_available('polygon') else 'CLOSED'}")
    
    # Check recovery
    print("\nMarking success to reset circuit breaker...")
    config.mark_source_success('polygon')
    print(f"Polygon available: {config.is_source_available('polygon')}")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("QUANTUM ORCHESTRATOR v2 - ADVANCED TEST SUITE")
    print("="*70)
    
    async def run_all_tests():
        await test_single_ticker()
        await test_parallel_with_metrics()
        await test_circuit_breaker()
        
        print("\n" + "="*70)
        print("âœ… ALL TESTS COMPLETE")
        print("="*70 + "\n")
    
    asyncio.run(run_all_tests())
