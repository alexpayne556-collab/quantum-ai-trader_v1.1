{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730513f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 1: ENVIRONMENT SETUP & GPU CHECK\n",
    "# ==========================================\n",
    "\n",
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nâœ… PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected - training will be SLOW on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9369def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 2: INSTALL REQUIRED PACKAGES\n",
    "# ==========================================\n",
    "\n",
    "!pip install -q yfinance pandas-ta xgboost scikit-learn joblib python-dotenv requests fredapi\n",
    "\n",
    "print(\"\\nâœ… Packages installed successfully\")\n",
    "print(\"   - yfinance: Historical OHLCV data\")\n",
    "print(\"   - pandas-ta: Technical indicators\")\n",
    "print(\"   - xgboost: GPU-accelerated gradient boosting\")\n",
    "print(\"   - scikit-learn: Random Forest, Gradient Boosting, preprocessing\")\n",
    "print(\"   - fredapi: FRED economic data (VIX, yields)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b5a3f",
   "metadata": {},
   "source": [
    "## Step 3: Mount Google Drive (Save Models Here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8378837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 3: MOUNT GOOGLE DRIVE\n",
    "# ==========================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# Create workspace structure\n",
    "workspace = '/content/drive/MyDrive/underdog_trader'\n",
    "os.makedirs(workspace, exist_ok=True)\n",
    "os.makedirs(f'{workspace}/models', exist_ok=True)\n",
    "os.makedirs(f'{workspace}/data', exist_ok=True)\n",
    "os.makedirs(f'{workspace}/results', exist_ok=True)\n",
    "\n",
    "print(f\"\\nâœ… Workspace created: {workspace}\")\n",
    "print(f\"   - models/: Trained model files (.pkl)\")\n",
    "print(f\"   - data/: Cached OHLCV data (.parquet)\")\n",
    "print(f\"   - results/: Training metrics (.json)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa629e",
   "metadata": {},
   "source": [
    "## Step 4: Clone Repository & Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 4: CLONE QUANTUM-AI-TRADER REPO\n",
    "# ==========================================\n",
    "\n",
    "# Clone repository (contains our custom modules)\n",
    "!git clone https://github.com/alexpayne556-collab/quantum-ai-trader_v1.1.git /content/quantum-ai-trader\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.append('/content/quantum-ai-trader/src/python')\n",
    "\n",
    "print(\"âœ… Repository cloned successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 5: IMPORT CUSTOM MODULES\n",
    "# ==========================================\n",
    "\n",
    "from multi_model_ensemble import MultiModelEnsemble\n",
    "from feature_engine import FeatureEngine\n",
    "from regime_classifier import RegimeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print(\"âœ… Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46235370",
   "metadata": {},
   "source": [
    "## Step 5: Define Alpha 76 Watchlist + Kodiak AI Clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d33604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 6: ALPHA 76 WATCHLIST\n",
    "# ==========================================\n",
    "\n",
    "# Alpha 76 high-velocity small/mid-cap stocks\n",
    "# NOTE: KDK = Kodiak Gas Services (energy infrastructure, NOT Kodiak Robotics/AI)\n",
    "ALPHA_76 = [\n",
    "    # Autonomous & AI Hardware (15 tickers)\n",
    "    'SYM', 'IONQ', 'RGTI', 'QUBT', 'AMBA', 'LAZR', 'INVZ', 'OUST', 'AEVA', 'SERV',\n",
    "    'MBLY', 'TSLA', 'KDK', 'AI', 'SOUN',\n",
    "    \n",
    "    # Space Economy (12 tickers)\n",
    "    'RKLB', 'ASTS', 'LUNR', 'JOBY', 'ACHR', 'PL', 'SPIR', 'IRDM', 'SPCE', 'RDW', 'MAXR', 'IRTC',\n",
    "    \n",
    "    # Biotech (16 tickers)\n",
    "    'VKTX', 'NTLA', 'BEAM', 'CRSP', 'EDIT', 'VERV', 'BLUE', 'FATE', 'AKRO', 'KOD',\n",
    "    'CYTK', 'LEGN', 'RARE', 'SRPT', 'BMRN', 'ALNY',\n",
    "    \n",
    "    # Green Energy & Grid (9 tickers)\n",
    "    'FLNC', 'NXT', 'BE', 'ARRY', 'ENPH', 'ENOV', 'QS', 'VST', 'AES',\n",
    "    \n",
    "    # Fintech & Digital Assets (9 tickers)\n",
    "    'SOFI', 'COIN', 'HOOD', 'UPST', 'AFRM', 'LC', 'MARA', 'SQ', 'NU',\n",
    "    \n",
    "    # Next-Gen Consumer & Software (15 tickers)\n",
    "    'APP', 'DUOL', 'PATH', 'S', 'CELH', 'ONON', 'SOUN', 'FOUR', 'NET', 'GTLB',\n",
    "    'DDOG', 'SNOW', 'PLTR', 'RBLX', 'U'\n",
    "]\n",
    "\n",
    "print(f\"âœ… Alpha 76 Watchlist: {len(ALPHA_76)} tickers\")\n",
    "print(f\"   Sectors: Autonomous, Space, Biotech, Energy, Fintech, Software\")\n",
    "print(f\"\\n   NOTE: KDK = Kodiak Gas Services (energy infrastructure for AI data centers)\")\n",
    "print(f\"         NOT Kodiak Robotics/AI (which is PRIVATE, not publicly traded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cac13",
   "metadata": {},
   "source": [
    "## Step 6: Download Historical Data (2 Years, 1hr Bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 7: DOWNLOAD OHLCV DATA\n",
    "# ==========================================\n",
    "\n",
    "def download_alpha76_data(tickers: list, period: str = '2y', interval: str = '1h') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download OHLCV data for all tickers\n",
    "    \n",
    "    Expected failures: 5-10 tickers (delisted, insufficient history)\n",
    "    Expected success: 65-70 tickers with ~3,500 bars each = ~250k total rows\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸšœ Harvesting {len(tickers)} tickers...\")\n",
    "    print(f\"   Period: {period} | Interval: {interval}\")\n",
    "    print(f\"   Expected time: 10-15 minutes\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        try:\n",
    "            # Download data\n",
    "            df = yf.download(ticker, period=period, interval=interval, auto_adjust=True, progress=False)\n",
    "            \n",
    "            if df.empty or len(df) < 200:\n",
    "                failed_tickers.append(ticker)\n",
    "                print(f\"   [{i}/{len(tickers)}] âš ï¸ {ticker}: Insufficient data ({len(df)} bars)\")\n",
    "                continue\n",
    "            \n",
    "            # Add ticker column\n",
    "            df['ticker'] = ticker\n",
    "            df['timestamp'] = df.index\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            all_data.append(df)\n",
    "            print(f\"   [{i}/{len(tickers)}] âœ… {ticker}: {len(df)} bars\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_tickers.append(ticker)\n",
    "            print(f\"   [{i}/{len(tickers)}] âŒ {ticker}: {e}\")\n",
    "    \n",
    "    # Combine all data\n",
    "    result = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… Download Complete\")\n",
    "    print(f\"   Success: {len(tickers) - len(failed_tickers)} tickers\")\n",
    "    print(f\"   Failed: {len(failed_tickers)} tickers {failed_tickers if failed_tickers else ''}\")\n",
    "    print(f\"   Total rows: {len(result):,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Execute download\n",
    "raw_data = download_alpha76_data(ALPHA_76)\n",
    "\n",
    "# Save to cache\n",
    "cache_path = f'{workspace}/data/alpha76_raw_data.parquet'\n",
    "raw_data.to_parquet(cache_path)\n",
    "print(f\"âœ… Data cached to: {cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489b433",
   "metadata": {},
   "source": [
    "## Step 7: Feature Engineering (49 Technical Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2efb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 8: CALCULATE FEATURES\n",
    "# ==========================================\n",
    "\n",
    "# Initialize feature engine\n",
    "feature_engine = FeatureEngine()\n",
    "\n",
    "print(\"ðŸ§  Engineering features for all tickers...\")\n",
    "print(f\"   Expected time: 5-10 minutes\\n\")\n",
    "\n",
    "# Group by ticker and calculate features\n",
    "featured_data = []\n",
    "\n",
    "for ticker in raw_data['ticker'].unique():\n",
    "    ticker_df = raw_data[raw_data['ticker'] == ticker].copy()\n",
    "    \n",
    "    try:\n",
    "        # Calculate 49 technical indicators\n",
    "        features_df = feature_engine.calculate_all_features(ticker_df)\n",
    "        \n",
    "        # Calculate MAX EXCURSION target (not just close-to-close)\n",
    "        # BUY: Price hits +3% at ANY point in next 5 bars\n",
    "        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=5)\n",
    "        features_df['fwd_max_high'] = features_df['High'].rolling(window=indexer).max()\n",
    "        features_df['fwd_min_low'] = features_df['Low'].rolling(window=indexer).min()\n",
    "        \n",
    "        # Target labels\n",
    "        buy_threshold = 0.03  # +3%\n",
    "        sell_threshold = -0.02  # -2%\n",
    "        \n",
    "        features_df['fwd_max_return'] = (features_df['fwd_max_high'] - features_df['Close']) / features_df['Close']\n",
    "        features_df['fwd_min_return'] = (features_df['fwd_min_low'] - features_df['Close']) / features_df['Close']\n",
    "        \n",
    "        # Labels: BUY=1, SELL=-1, HOLD=0\n",
    "        features_df['target'] = 0  # Default: HOLD\n",
    "        features_df.loc[features_df['fwd_max_return'] >= buy_threshold, 'target'] = 1  # BUY\n",
    "        features_df.loc[features_df['fwd_min_return'] <= sell_threshold, 'target'] = -1  # SELL\n",
    "        \n",
    "        # Drop forward-looking columns (data leakage prevention)\n",
    "        features_df.drop(columns=['fwd_max_high', 'fwd_min_low', 'fwd_max_return', 'fwd_min_return'], inplace=True)\n",
    "        \n",
    "        # Drop NaN rows (first 200 bars due to long-term MAs)\n",
    "        features_df.dropna(inplace=True)\n",
    "        \n",
    "        featured_data.append(features_df)\n",
    "        print(f\"   âœ… {ticker}: {len(features_df)} rows, {features_df['target'].value_counts().to_dict()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {ticker}: {e}\")\n",
    "\n",
    "# Combine all featured data\n",
    "full_dataset = pd.concat(featured_data, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Feature Engineering Complete\")\n",
    "print(f\"   Total rows: {len(full_dataset):,}\")\n",
    "print(f\"   Features: {len(feature_engine.get_feature_names())} technical indicators\")\n",
    "print(f\"   Target distribution: {full_dataset['target'].value_counts().to_dict()}\")\n",
    "print(f\"   BUY class: {(full_dataset['target'] == 1).sum() / len(full_dataset) * 100:.1f}%\")\n",
    "print(f\"   SELL class: {(full_dataset['target'] == -1).sum() / len(full_dataset) * 100:.1f}%\")\n",
    "print(f\"   HOLD class: {(full_dataset['target'] == 0).sum() / len(full_dataset) * 100:.1f}%\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save featured data\n",
    "feature_cache_path = f'{workspace}/data/alpha76_featured_data.parquet'\n",
    "full_dataset.to_parquet(feature_cache_path)\n",
    "print(f\"âœ… Featured data cached to: {feature_cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae25971",
   "metadata": {},
   "source": [
    "## Step 8: Train Multi-Model Ensemble (XGBoost + RF + GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 9: TRAIN ENSEMBLE\n",
    "# ==========================================\n",
    "\n",
    "# Prepare data\n",
    "X = full_dataset[feature_engine.get_feature_names()]\n",
    "y = full_dataset['target']\n",
    "\n",
    "# Convert to binary classification (BUY=1 vs SELL/HOLD=0)\n",
    "# We focus on BUY precision (SELL is for risk management)\n",
    "y_binary = (y == 1).astype(int)\n",
    "\n",
    "# Train/Test split (80/20 with time-series split)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y_binary.iloc[:split_idx], y_binary.iloc[split_idx:]\n",
    "\n",
    "print(f\"âœ… Data Split:\")\n",
    "print(f\"   Train: {len(X_train):,} rows ({y_train.sum():,} BUY signals)\")\n",
    "print(f\"   Test:  {len(X_test):,} rows ({y_test.sum():,} BUY signals)\")\n",
    "\n",
    "# Initialize ensemble\n",
    "ensemble = MultiModelEnsemble(use_gpu=True)\n",
    "\n",
    "# Train with 3-fold time-series cross-validation\n",
    "print(f\"\\nðŸ‹ï¸ Training 3-Model Ensemble...\")\n",
    "print(f\"   Expected time: 30-60 minutes on T4 GPU\")\n",
    "print(f\"   Models: XGBoost (GPU), Random Forest (CPU), Gradient Boosting (CPU)\\n\")\n",
    "\n",
    "ensemble.train(X_train, y_train, n_splits=3)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\nðŸ“Š Evaluating on Test Set...\")\n",
    "predictions = ensemble.predict(X_test)\n",
    "probabilities = ensemble.predict_proba(X_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, probabilities[:, 1])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸ† BASELINE RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Precision: {precision:.2%} (% of BUY signals that were correct)\")\n",
    "print(f\"   Recall:    {recall:.2%} (% of real opportunities captured)\")\n",
    "print(f\"   F1 Score:  {f1:.2%}\")\n",
    "print(f\"   ROC-AUC:   {roc_auc:.3f}\")\n",
    "print(f\"\\n   Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'buy_class_ratio': float(y_train.sum() / len(y_train))\n",
    "}\n",
    "\n",
    "results_path = f'{workspace}/results/baseline_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89eaf5",
   "metadata": {},
   "source": [
    "## Step 9: Save Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970160c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 10: SAVE TRAINED MODELS\n",
    "# ==========================================\n",
    "\n",
    "# Save ensemble models\n",
    "model_path = f'{workspace}/models/alpha76_ensemble_v1.pkl'\n",
    "joblib.dump(ensemble, model_path)\n",
    "\n",
    "print(f\"âœ… Models saved to: {model_path}\")\n",
    "print(f\"\\nðŸ“¦ Download Instructions:\")\n",
    "print(f\"   1. Go to Google Drive: {workspace}/models/\")\n",
    "print(f\"   2. Right-click 'alpha76_ensemble_v1.pkl' â†’ Download\")\n",
    "print(f\"   3. Place in your local project: models/ folder\")\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(f\"   1. Share PERPLEXITY_OPTIMIZATION_BRIEF.md with Perplexity AI\")\n",
    "print(f\"   2. Ask 10 critical questions about hyperparameter tuning\")\n",
    "print(f\"   3. Implement recommendations in Week 1\")\n",
    "print(f\"   4. Target: 65-70% precision (10-point improvement)\")\n",
    "print(f\"\\nIntelligence edge, not speed edge. ðŸš‚\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
