"""
INTEGRATED SYSTEM TEST - End-to-End Validation
===============================================

Tests ALL modules working together to detect trading opportunities BEFORE they happen.

Test Objectives:
1. Can we detect dark pool accumulation? (Module 1)
2. Can meta-learner predict moves with >50% accuracy? (Module 2)
3. Do microstructure features catch institutional flow? (Module 5)
4. Does sentiment divergence predict reversals? (Module 6)
5. Do cross-asset lags give us early warning? (Module 7)
6. Can we identify "double down" stocks with high conviction?

Success Criteria:
- Predict direction correctly >55% of time (baseline random = 50%)
- Detect institutional accumulation BEFORE price moves
- Signal confluence identifies high-conviction setups
- System flags opportunities 1-3 days BEFORE major moves

Author: Quantum AI Trader
"""

import sys
import numpy as np
import pandas as pd
from pathlib import Path
import logging
from datetime import datetime, timedelta
import yfinance as yf

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from features.dark_pool_signals import DarkPoolSignals
from features.microstructure import MicrostructureFeatures
from features.sentiment_features import SentimentFeatures
from features.cross_asset_lags import CrossAssetLagFeatures
from features.feature_selector import FeatureSelector
from models.meta_learner import HierarchicalMetaLearner
from models.calibrator import ProbabilityCalibrator
from monitoring.drift_detector import DriftDetector

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


class IntegratedSystemValidator:
    """
    End-to-end system validation with real market data.
    Tests if our modules can actually predict opportunities.
    """
    
    def __init__(self):
        self.results = {}
        # Don't initialize DarkPoolSignals here - create per ticker
        
    def fetch_real_data(self, ticker: str, period: str = "6mo") -> pd.DataFrame:
        """Fetch real market data from yfinance."""
        logger.info(f"Fetching {ticker} data for {period}...")
        
        try:
            df = yf.download(ticker, period=period, interval="1d", progress=False)
            if df.empty:
                raise ValueError(f"No data returned for {ticker}")
            
            logger.info(f"âœ… Retrieved {len(df)} days of data")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Failed to fetch {ticker}: {e}")
            return pd.DataFrame()
    
    def compute_forward_returns(self, df: pd.DataFrame, days: int = 5) -> pd.Series:
        """
        Compute forward returns (what we're trying to predict).
        
        forward_return = (Close[t+5] - Close[t]) / Close[t]
        """
        forward_prices = df['Close'].shift(-days)
        forward_returns = (forward_prices - df['Close']) / df['Close']
        return forward_returns
    
    def test_dark_pool_predictive_power(self, ticker: str = "NVDA") -> dict:
        """
        Test 1: Can dark pool signals predict future moves?
        
        Hypothesis: High institutional flow (IFI > 70) should predict
        positive returns 1-5 days ahead.
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST 1: DARK POOL PREDICTIVE POWER")
        logger.info("=" * 80)
        
        # Fetch data
        df = self.fetch_real_data(ticker, period="6mo")
        if df.empty:
            return {'success': False, 'error': 'No data'}
        
        # Compute dark pool features
        dark_pool = DarkPoolSignals(ticker)
        df_signals = dark_pool.compute_all_features(df)
        
        # Compute forward returns (1, 3, 5 days)
        df_signals['return_1d'] = self.compute_forward_returns(df_signals, days=1)
        df_signals['return_3d'] = self.compute_forward_returns(df_signals, days=3)
        df_signals['return_5d'] = self.compute_forward_returns(df_signals, days=5)
        
        # Remove NaN rows
        df_signals = df_signals.dropna()
        
        if len(df_signals) < 50:
            return {'success': False, 'error': 'Insufficient data after NaN removal'}
        
        # Test hypothesis: High IFI â†’ positive returns
        high_ifi = df_signals['ifi'] > 70
        low_ifi = df_signals['ifi'] < 30
        
        results = {
            'ticker': ticker,
            'samples': len(df_signals),
            'high_ifi_count': high_ifi.sum(),
            'low_ifi_count': low_ifi.sum(),
            'high_ifi_return_1d': df_signals.loc[high_ifi, 'return_1d'].mean(),
            'high_ifi_return_3d': df_signals.loc[high_ifi, 'return_3d'].mean(),
            'high_ifi_return_5d': df_signals.loc[high_ifi, 'return_5d'].mean(),
            'low_ifi_return_1d': df_signals.loc[low_ifi, 'return_1d'].mean(),
            'low_ifi_return_3d': df_signals.loc[low_ifi, 'return_3d'].mean(),
            'low_ifi_return_5d': df_signals.loc[low_ifi, 'return_5d'].mean(),
            'baseline_return_5d': df_signals['return_5d'].mean(),
        }
        
        # Check if high IFI beats baseline
        edge_5d = results['high_ifi_return_5d'] - results['baseline_return_5d']
        results['edge_5d_pct'] = edge_5d * 100
        results['success'] = edge_5d > 0
        
        logger.info(f"\nðŸ“Š Dark Pool Analysis ({ticker}):")
        logger.info(f"  Samples: {results['samples']}")
        logger.info(f"  High IFI days: {results['high_ifi_count']}")
        logger.info(f"  Low IFI days: {results['low_ifi_count']}")
        logger.info(f"\n  High IFI â†’ 5d return: {results['high_ifi_return_5d']*100:.2f}%")
        logger.info(f"  Low IFI â†’ 5d return: {results['low_ifi_return_5d']*100:.2f}%")
        logger.info(f"  Baseline 5d return: {results['baseline_return_5d']*100:.2f}%")
        logger.info(f"\n  ðŸŽ¯ Edge: {results['edge_5d_pct']:.2f}% {'âœ…' if results['success'] else 'âŒ'}")
        
        self.results['dark_pool'] = results
        return results
    
    def test_microstructure_signals(self, ticker: str = "AAPL") -> dict:
        """
        Test 2: Do microstructure features detect institutional flow?
        
        Hypothesis: High institutional activity + positive CLV should
        predict upward moves.
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST 2: MICROSTRUCTURE SIGNALS")
        logger.info("=" * 80)
        
        df = self.fetch_real_data(ticker, period="6mo")
        if df.empty:
            return {'success': False, 'error': 'No data'}
        
        # Compute microstructure features
        df_micro = MicrostructureFeatures.compute_all_features(df)
        
        # Forward returns
        df_micro['return_5d'] = self.compute_forward_returns(df_micro, days=5)
        df_micro = df_micro.dropna()
        
        if len(df_micro) < 50:
            return {'success': False, 'error': 'Insufficient data'}
        
        # Test hypothesis: High institutional activity + bullish CLV â†’ positive returns
        institutional_signal = (
            (df_micro['institutional_spike'] == 1) &
            (df_micro['order_flow_clv'] > 0.3)
        )
        
        results = {
            'ticker': ticker,
            'samples': len(df_micro),
            'signal_count': institutional_signal.sum(),
            'signal_return_5d': df_micro.loc[institutional_signal, 'return_5d'].mean(),
            'baseline_return_5d': df_micro['return_5d'].mean(),
        }
        
        if results['signal_count'] > 0:
            edge_5d = results['signal_return_5d'] - results['baseline_return_5d']
            results['edge_5d_pct'] = edge_5d * 100
            results['success'] = edge_5d > 0
        else:
            results['edge_5d_pct'] = 0
            results['success'] = False
        
        logger.info(f"\nðŸ“Š Microstructure Analysis ({ticker}):")
        logger.info(f"  Samples: {results['samples']}")
        logger.info(f"  Institutional signals: {results['signal_count']}")
        if results['signal_count'] > 0:
            logger.info(f"  Signal â†’ 5d return: {results['signal_return_5d']*100:.2f}%")
            logger.info(f"  Baseline: {results['baseline_return_5d']*100:.2f}%")
            logger.info(f"  ðŸŽ¯ Edge: {results['edge_5d_pct']:.2f}% {'âœ…' if results['success'] else 'âŒ'}")
        else:
            logger.info(f"  âš ï¸ No institutional signals detected")
        
        self.results['microstructure'] = results
        return results
    
    def test_meta_learner_training(self) -> dict:
        """
        Test 3: Can meta-learner learn patterns from historical data?
        
        Uses synthetic data to validate training pipeline works.
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST 3: META-LEARNER TRAINING")
        logger.info("=" * 80)
        
        # Generate synthetic training data
        np.random.seed(42)
        n_samples = 1000
        
        # Synthetic features (pattern, research, dark pool scores)
        X = pd.DataFrame({
            'score_pattern': np.random.uniform(0, 1, n_samples),
            'score_research': np.random.uniform(0, 1, n_samples),
            'score_dark_pool': np.random.uniform(0, 1, n_samples),
        })
        
        # Synthetic target (weighted combination + noise)
        y = (
            X['score_pattern'] * 0.3 +
            X['score_research'] * 0.5 +
            X['score_dark_pool'] * 0.2 +
            np.random.normal(0, 0.1, n_samples)
        )
        y = (y > 0.5).astype(int)
        
        # Train meta-learner
        logger.info(f"Training on {n_samples} samples...")
        meta = HierarchicalMetaLearner(random_state=42)
        
        metrics = meta.train_ensemble(
            X, y,
            regime_ids=np.random.randint(0, 3, n_samples)
        )
        
        results = {
            'train_auc': metrics['train_auc'],
            'val_auc': metrics['val_auc'],
            'train_logloss': metrics['train_logloss'],
            'val_logloss': metrics['val_logloss'],
            'success': metrics['val_auc'] > 0.5,  # Better than random
        }
        
        logger.info(f"\nðŸ“Š Meta-Learner Training:")
        logger.info(f"  Train AUC: {results['train_auc']:.4f}")
        logger.info(f"  Val AUC: {results['val_auc']:.4f}")
        logger.info(f"  Val LogLoss: {results['val_logloss']:.4f}")
        logger.info(f"  ðŸŽ¯ Validation: {'âœ… Better than random' if results['success'] else 'âŒ No edge'}")
        
        self.results['meta_learner'] = results
        return results
    
    def identify_double_down_opportunities(self, tickers: list = None) -> pd.DataFrame:
        """
        Test 4: Identify "double down" stocks with high conviction signals.
        
        Criteria:
        - Dark pool accumulation (IFI > 75)
        - Institutional activity spike
        - Positive microstructure (CLV > 0.5)
        - Multiple signals agree
        """
        logger.info("\n" + "=" * 80)
        logger.info("TEST 4: DOUBLE DOWN OPPORTUNITY SCANNER")
        logger.info("=" * 80)
        
        if tickers is None:
            tickers = ['NVDA', 'AAPL', 'MSFT', 'TSLA', 'AMD']
        
        opportunities = []
        
        for ticker in tickers:
            logger.info(f"\nScanning {ticker}...")
            
            df = self.fetch_real_data(ticker, period="3mo")
            if df.empty:
                continue
            
            # Compute features
            dark_pool = DarkPoolSignals(ticker)
            df_signals = dark_pool.compute_all_features(df)
            df_micro = MicrostructureFeatures.compute_all_features(df)
            
            # Get latest signals
            latest = df_signals.iloc[-1]
            latest_micro = df_micro.iloc[-1]
            
            # Count agreeing bullish signals
            signals = {
                'dark_pool_bullish': latest['ifi'] > 75,
                'institutional_active': latest_micro['institutional_spike'] == 1,
                'positive_flow': latest_micro['order_flow_clv'] > 0.5,
                'accumulation': latest['smi'] > 60,
                'volume_surge': latest['vroc'] > 50,
            }
            
            agreement_count = sum(signals.values())
            
            # High conviction if 3+ signals agree
            if agreement_count >= 3:
                opportunities.append({
                    'ticker': ticker,
                    'date': df.index[-1].strftime('%Y-%m-%d'),
                    'agreement_count': agreement_count,
                    'signals': signals,
                    'ifi': latest['ifi'],
                    'smi': latest['smi'],
                    'clv': latest_micro['order_flow_clv'],
                    'institutional_spike': latest_micro['institutional_spike'],
                    'conviction': 'HIGH' if agreement_count >= 4 else 'MEDIUM',
                })
        
        df_opps = pd.DataFrame(opportunities)
        
        if len(df_opps) > 0:
            logger.info(f"\nðŸŽ¯ FOUND {len(df_opps)} HIGH-CONVICTION OPPORTUNITIES:")
            for _, opp in df_opps.iterrows():
                logger.info(f"\n  {opp['ticker']} - {opp['conviction']} CONVICTION")
                logger.info(f"    Signals agreeing: {opp['agreement_count']}/5")
                logger.info(f"    IFI: {opp['ifi']:.1f}")
                logger.info(f"    SMI: {opp['smi']:.1f}")
                logger.info(f"    CLV: {opp['clv']:.3f}")
                logger.info(f"    Details: {opp['signals']}")
        else:
            logger.info("\nâš ï¸ No high-conviction opportunities found in current scan")
        
        self.results['opportunities'] = df_opps
        return df_opps
    
    def generate_training_requirements_report(self) -> dict:
        """
        Generate report on what each module needs for production deployment.
        """
        logger.info("\n" + "=" * 80)
        logger.info("TRAINING REQUIREMENTS ANALYSIS")
        logger.info("=" * 80)
        
        requirements = {
            'meta_learner': {
                'module': 'HierarchicalMetaLearner',
                'status': 'Needs training on real data',
                'data_needed': [
                    '100 tickers Ã— 5 years historical data',
                    '60 engineered features per sample',
                    'Minimum 10,000 samples (100 per ticker)',
                    'Class labels: win/loss (binary)',
                    'Regime IDs: 0-11 (12 market regimes)',
                ],
                'training_time': '3-4 hours on Colab Pro T4 GPU',
                'output': 'meta_learner_trained.joblib (serialized model)',
                'validation': 'Val AUC > 0.55 (baseline random = 0.50)',
            },
            'feature_selector': {
                'module': 'FeatureSelector',
                'status': 'Needs training on feature matrix',
                'data_needed': [
                    'Same 60 features as meta-learner',
                    'Will reduce to top 20 features',
                    'Uses correlation filter + RF importance',
                ],
                'training_time': '5-10 minutes',
                'output': 'feature_selector_fitted.joblib',
                'validation': 'Top 20 features have importance > 0.01',
            },
            'calibrator': {
                'module': 'ProbabilityCalibrator',
                'status': 'Needs live predictions to fit',
                'data_needed': [
                    'Rolling 100 trades (raw_score, actual_outcome)',
                    'Minimum 30 samples to start calibration',
                    'Updates daily with new trades',
                ],
                'training_time': 'Real-time (streaming)',
                'output': 'calibrator_state.joblib (rolling window)',
                'validation': 'ECE < 0.05 (well-calibrated)',
            },
            'drift_detector': {
                'module': 'DriftDetector',
                'status': 'Needs training baseline',
                'data_needed': [
                    'Training feature matrix (baseline distribution)',
                    'Same 60 features used for meta-learner',
                    'Monitors production data vs baseline',
                ],
                'training_time': '1-2 minutes',
                'output': 'drift_detector_baseline.json',
                'validation': 'Detects >20% feature drift â†’ triggers retrain',
            },
            'dark_pool_signals': {
                'module': 'DarkPoolSignals',
                'status': 'âœ… READY (no training needed)',
                'data_needed': ['OHLCV data only'],
                'training_time': 'N/A (rule-based)',
                'output': 'Real-time features',
                'validation': '11/12 tests passing',
            },
            'microstructure': {
                'module': 'MicrostructureFeatures',
                'status': 'âœ… READY (no training needed)',
                'data_needed': ['OHLCV data only'],
                'training_time': 'N/A (computed features)',
                'output': 'Real-time features',
                'validation': '5/6 tests passing',
            },
            'sentiment': {
                'module': 'SentimentFeatures',
                'status': 'âœ… READY (no training needed)',
                'data_needed': ['EODHD sentiment API data'],
                'training_time': 'N/A (rule-based)',
                'output': 'Real-time features',
                'validation': '6/6 tests passing',
            },
            'cross_asset': {
                'module': 'CrossAssetLagFeatures',
                'status': 'âœ… READY (no training needed)',
                'data_needed': ['BTC, VIX, 10Y yield data'],
                'training_time': 'N/A (computed features)',
                'output': 'Real-time features',
                'validation': '5/6 tests passing',
            },
        }
        
        logger.info("\nðŸ“‹ MODULE TRAINING STATUS:\n")
        
        ready_count = 0
        needs_training = []
        
        for module_name, req in requirements.items():
            status_icon = 'âœ…' if req['status'].startswith('âœ…') else 'â³'
            logger.info(f"{status_icon} {req['module']}:")
            logger.info(f"   Status: {req['status']}")
            
            if not req['status'].startswith('âœ…'):
                needs_training.append(module_name)
            else:
                ready_count += 1
        
        logger.info(f"\n{'=' * 80}")
        logger.info(f"SUMMARY: {ready_count}/{len(requirements)} modules ready")
        logger.info(f"Needs training: {needs_training}")
        logger.info(f"{'=' * 80}")
        
        return requirements
    
    def run_full_validation(self):
        """Run all tests and generate comprehensive report."""
        logger.info("\n" + "=" * 80)
        logger.info("INTEGRATED SYSTEM VALIDATION - START")
        logger.info("=" * 80)
        logger.info(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Test 1: Dark pool predictive power
        try:
            self.test_dark_pool_predictive_power(ticker='NVDA')
        except Exception as e:
            logger.error(f"âŒ Test 1 failed: {e}")
        
        # Test 2: Microstructure signals
        try:
            self.test_microstructure_signals(ticker='AAPL')
        except Exception as e:
            logger.error(f"âŒ Test 2 failed: {e}")
        
        # Test 3: Meta-learner training
        try:
            self.test_meta_learner_training()
        except Exception as e:
            logger.error(f"âŒ Test 3 failed: {e}")
        
        # Test 4: Opportunity scanner
        try:
            self.identify_double_down_opportunities()
        except Exception as e:
            logger.error(f"âŒ Test 4 failed: {e}")
        
        # Training requirements
        try:
            requirements = self.generate_training_requirements_report()
        except Exception as e:
            logger.error(f"âŒ Requirements analysis failed: {e}")
        
        # Final summary
        logger.info("\n" + "=" * 80)
        logger.info("VALIDATION SUMMARY")
        logger.info("=" * 80)
        
        if 'dark_pool' in self.results:
            dp = self.results['dark_pool']
            logger.info(f"\nâœ… Dark Pool Test: Edge = {dp.get('edge_5d_pct', 0):.2f}%")
        
        if 'microstructure' in self.results:
            micro = self.results['microstructure']
            logger.info(f"âœ… Microstructure Test: Edge = {micro.get('edge_5d_pct', 0):.2f}%")
        
        if 'meta_learner' in self.results:
            ml = self.results['meta_learner']
            logger.info(f"âœ… Meta-Learner Test: Val AUC = {ml['val_auc']:.4f}")
        
        if 'opportunities' in self.results:
            opps = self.results['opportunities']
            if isinstance(opps, pd.DataFrame) and len(opps) > 0:
                logger.info(f"âœ… Opportunities Found: {len(opps)} high-conviction setups")
        
        logger.info("\n" + "=" * 80)
        logger.info("NEXT STEPS:")
        logger.info("=" * 80)
        logger.info("1. Train meta-learner on 100 tickers Ã— 5 years")
        logger.info("2. Fit feature selector on training data")
        logger.info("3. Deploy models to production")
        logger.info("4. Enable live prediction with calibration")
        logger.info("5. Monitor drift and retrain as needed")
        logger.info("=" * 80)


if __name__ == "__main__":
    print("=" * 80)
    print("QUANTUM AI TRADER - INTEGRATED SYSTEM VALIDATION")
    print("=" * 80)
    
    validator = IntegratedSystemValidator()
    validator.run_full_validation()
