{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß SETUP - Check GPU and install dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_available = 'NVIDIA' in str(gpu_info)\n",
    "print(f\"üéÆ GPU Available: {gpu_available}\")\n",
    "if gpu_available:\n",
    "    !nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q lightgbm xgboost catboost yfinance ta pandas numpy scikit-learn joblib\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af699377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ Imports loaded!\")\n",
    "print(f\"üìÖ Current date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ YOUR EXACT WATCHLIST - 50 Tickers\n",
    "TICKERS = [\n",
    "    'APLD', 'SERV', 'MRVL', 'HOOD', 'LUNR', 'BAC', 'QCOM', 'UUUU',\n",
    "    'TSLA', 'AMD', 'NOW', 'NVDA', 'MU', 'PG', 'DLB', 'XME',\n",
    "    'KRYS', 'LEU', 'QTUM', 'SPY', 'UNH', 'WMT', 'OKLO', 'RXRX',\n",
    "    'MTZ', 'SNOW', 'GRRR', 'BSX', 'LLY', 'VOO', 'GEO', 'CXW',\n",
    "    'LYFT', 'MNDY', 'BA', 'LAC', 'INTC', 'ALK', 'LMT', 'CRDO',\n",
    "    'ANET', 'META', 'RIVN', 'GOOGL', 'HL', 'TEM', 'TDOC', 'KMTS',\n",
    "    'SCHA', 'B'\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "START_DATE = '2015-01-01'\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Targets - Aggressive\n",
    "TARGETS = {\n",
    "    'quick_5pct': {'gain': 0.05, 'days': 3},    # 5% in 3 days (PRIMARY)\n",
    "    'swing_7pct': {'gain': 0.07, 'days': 5},    # 7% in 5 days\n",
    "    'explosive_10pct': {'gain': 0.10, 'days': 5}, # 10% in 5 days\n",
    "    'momentum_15pct': {'gain': 0.15, 'days': 10}  # 15% in 10 days\n",
    "}\n",
    "\n",
    "print(f\"üéØ {len(TICKERS)} tickers loaded\")\n",
    "print(f\"üìÖ Training period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"\\nüéØ Targets:\")\n",
    "for name, params in TARGETS.items():\n",
    "    print(f\"   {name}: {params['gain']*100}% in {params['days']} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba93f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä FEATURE ENGINEERING - All the features that matter\n",
    "def calculate_features(df):\n",
    "    \"\"\"Calculate all features - based on your top 30 importance list\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Flatten multi-index if needed\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    \n",
    "    close = df['Close']\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    volume = df['Volume']\n",
    "    open_price = df['Open']\n",
    "    \n",
    "    # =====================================================\n",
    "    # TOP FEATURES (from your importance ranking)\n",
    "    # =====================================================\n",
    "    \n",
    "    # 1-2. MACD (TOP FEATURE!)\n",
    "    ema5 = close.ewm(span=5).mean()\n",
    "    ema13 = close.ewm(span=13).mean()\n",
    "    ema12 = close.ewm(span=12).mean()\n",
    "    ema26 = close.ewm(span=26).mean()\n",
    "    \n",
    "    df['MACD_5_13'] = ema5 - ema13\n",
    "    df['MACD_Signal_5_13'] = df['MACD_5_13'].ewm(span=9).mean()\n",
    "    df['MACD_Hist_5_13'] = df['MACD_5_13'] - df['MACD_Signal_5_13']\n",
    "    \n",
    "    df['MACD'] = ema12 - ema26\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']\n",
    "    \n",
    "    # 3. Gap\n",
    "    df['Gap'] = (open_price - close.shift(1)) / close.shift(1)\n",
    "    \n",
    "    # 4-5, 10, 25. Returns\n",
    "    df['Return_1d'] = close.pct_change(1)\n",
    "    df['Return_2d'] = close.pct_change(2)\n",
    "    df['Return_3d'] = close.pct_change(3)\n",
    "    df['Return_5d'] = close.pct_change(5)\n",
    "    \n",
    "    # 5. Range vs ATR\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = tr.rolling(14).mean()\n",
    "    df['ATR_7'] = tr.rolling(7).mean()\n",
    "    df['Range_vs_ATR'] = (high - low) / df['ATR_14']\n",
    "    df['ATR_Ratio'] = df['ATR_7'] / df['ATR_14']\n",
    "    \n",
    "    # 6. CMF (Chaikin Money Flow)\n",
    "    mfm = ((close - low) - (high - close)) / (high - low + 1e-8)\n",
    "    mfv = mfm * volume\n",
    "    df['CMF'] = mfv.rolling(20).sum() / volume.rolling(20).sum()\n",
    "    \n",
    "    # 7. MFI (Money Flow Index)\n",
    "    typical_price = (high + low + close) / 3\n",
    "    raw_mf = typical_price * volume\n",
    "    mf_positive = raw_mf.where(typical_price > typical_price.shift(1), 0).rolling(14).sum()\n",
    "    mf_negative = raw_mf.where(typical_price < typical_price.shift(1), 0).rolling(14).sum()\n",
    "    df['MFI'] = 100 - (100 / (1 + mf_positive / (mf_negative + 1e-8)))\n",
    "    \n",
    "    # 8-9. Volume Ratios\n",
    "    df['Vol_Ratio_50'] = volume / volume.rolling(50).mean()\n",
    "    df['Vol_Ratio_20'] = volume / volume.rolling(20).mean()\n",
    "    df['Vol_Ratio_10'] = volume / volume.rolling(10).mean()\n",
    "    \n",
    "    # 11. OBV Slope\n",
    "    obv = (np.sign(close.diff()) * volume).cumsum()\n",
    "    df['OBV'] = obv\n",
    "    df['OBV_Slope'] = obv.diff(5) / 5\n",
    "    \n",
    "    # 12. ADX\n",
    "    plus_dm = high.diff().where((high.diff() > low.diff().abs()) & (high.diff() > 0), 0)\n",
    "    minus_dm = low.diff().abs().where((low.diff().abs() > high.diff()) & (low.diff() < 0), 0)\n",
    "    \n",
    "    atr_14 = tr.rolling(14).mean()\n",
    "    df['PLUS_DI'] = 100 * (plus_dm.rolling(14).mean() / atr_14)\n",
    "    df['MINUS_DI'] = 100 * (minus_dm.rolling(14).mean() / atr_14)\n",
    "    \n",
    "    dx = 100 * abs(df['PLUS_DI'] - df['MINUS_DI']) / (df['PLUS_DI'] + df['MINUS_DI'] + 1e-8)\n",
    "    df['ADX'] = dx.rolling(14).mean()\n",
    "    \n",
    "    # 13. RSI Momentum\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-8)\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    df['RSI_Momentum'] = df['RSI'] - df['RSI'].shift(5)\n",
    "    \n",
    "    # 15-18. Candlestick patterns\n",
    "    body = abs(close - open_price)\n",
    "    total_range = high - low + 1e-8\n",
    "    df['body_to_range'] = body / total_range\n",
    "    df['Upper_Wick'] = (high - pd.concat([close, open_price], axis=1).max(axis=1)) / total_range\n",
    "    df['Lower_Wick'] = (pd.concat([close, open_price], axis=1).min(axis=1) - low) / total_range\n",
    "    df['lower_shadow_ratio'] = df['Lower_Wick'] / (df['body_to_range'] + 1e-8)\n",
    "    df['Wick_Ratio'] = df['Upper_Wick'] / (df['Lower_Wick'] + 1e-8)\n",
    "    \n",
    "    # 16, 20. Volume-Price\n",
    "    df['Vol_Price_Trend'] = (volume * close.pct_change()).cumsum()\n",
    "    df['AD'] = ((close - low) - (high - close)) / (high - low + 1e-8) * volume\n",
    "    df['AD'] = df['AD'].cumsum()\n",
    "    \n",
    "    # 21. Relative Strength vs SPY (sector proxy)\n",
    "    df['rs_vs_sector_20d'] = close.pct_change(20)  # Will compare to SPY later\n",
    "    \n",
    "    # 24. Trend slope\n",
    "    df['trend_slope_20'] = (close - close.shift(20)) / close.shift(20)\n",
    "    df['trend_slope_10'] = (close - close.shift(10)) / close.shift(10)\n",
    "    \n",
    "    # 27-28. Bollinger Bands\n",
    "    sma20 = close.rolling(20).mean()\n",
    "    std20 = close.rolling(20).std()\n",
    "    sma50 = close.rolling(50).mean()\n",
    "    std50 = close.rolling(50).std()\n",
    "    \n",
    "    df['BB_Upper'] = sma20 + 2 * std20\n",
    "    df['BB_Lower'] = sma20 - 2 * std20\n",
    "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / sma20\n",
    "    df['BB_Width_50'] = (4 * std50) / sma50\n",
    "    df['BB_Position'] = (close - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'] + 1e-8)\n",
    "    \n",
    "    # 29-30. Stochastic\n",
    "    lowest_14 = low.rolling(14).min()\n",
    "    highest_14 = high.rolling(14).max()\n",
    "    df['Stoch_K'] = 100 * (close - lowest_14) / (highest_14 - lowest_14 + 1e-8)\n",
    "    df['Stoch_D'] = df['Stoch_K'].rolling(3).mean()\n",
    "    \n",
    "    # =====================================================\n",
    "    # EXTRA FEATURES - EMA Ribbon, SMAs\n",
    "    # =====================================================\n",
    "    \n",
    "    for period in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f'SMA_{period}'] = close.rolling(period).mean()\n",
    "        df[f'EMA_{period}'] = close.ewm(span=period).mean()\n",
    "        df[f'Close_vs_SMA_{period}'] = close / df[f'SMA_{period}']\n",
    "        df[f'Close_vs_EMA_{period}'] = close / df[f'EMA_{period}']\n",
    "    \n",
    "    # EMA Ribbon spread\n",
    "    df['EMA_Ribbon_Spread'] = (df['EMA_10'] - df['EMA_50']) / close\n",
    "    \n",
    "    # Momentum indicators\n",
    "    df['ROC_10'] = close.pct_change(10) * 100\n",
    "    df['ROC_20'] = close.pct_change(20) * 100\n",
    "    \n",
    "    # Volatility\n",
    "    df['Volatility_20'] = close.pct_change().rolling(20).std() * np.sqrt(252)\n",
    "    df['Volatility_10'] = close.pct_change().rolling(10).std() * np.sqrt(252)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Feature engineering function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• DOWNLOAD ALL DATA WITH PROGRESS\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "all_data = {}\n",
    "failed_tickers = []\n",
    "\n",
    "print(f\"üì• Downloading {len(TICKERS)} tickers...\")\n",
    "print(f\"   Period: {START_DATE} to {END_DATE}\")\n",
    "print()\n",
    "\n",
    "for ticker in tqdm(TICKERS, desc=\"Downloading\"):\n",
    "    try:\n",
    "        df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
    "        if len(df) > 100:\n",
    "            df = calculate_features(df)\n",
    "            df['Ticker'] = ticker\n",
    "            all_data[ticker] = df\n",
    "        else:\n",
    "            failed_tickers.append(ticker)\n",
    "    except Exception as e:\n",
    "        failed_tickers.append(ticker)\n",
    "\n",
    "print(f\"\\n‚úÖ Downloaded {len(all_data)} tickers successfully\")\n",
    "if failed_tickers:\n",
    "    print(f\"‚ö†Ô∏è Failed: {failed_tickers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ CREATE TARGET LABELS\n",
    "def create_targets(df, targets_config):\n",
    "    \"\"\"Create all target columns\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for name, params in targets_config.items():\n",
    "        days = params['days']\n",
    "        gain = params['gain']\n",
    "        \n",
    "        # Look ahead return\n",
    "        future_high = df['High'].rolling(days).max().shift(-days)\n",
    "        future_return = (future_high - df['Close']) / df['Close']\n",
    "        \n",
    "        # Binary target: 1 if hit target gain, 0 otherwise\n",
    "        df[f'Target_{name}'] = (future_return >= gain).astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply to all data\n",
    "for ticker in all_data:\n",
    "    all_data[ticker] = create_targets(all_data[ticker], TARGETS)\n",
    "\n",
    "print(\"‚úÖ Targets created!\")\n",
    "print(\"\\nüìä Target distribution for quick_5pct:\")\n",
    "combined = pd.concat(all_data.values())\n",
    "print(combined['Target_quick_5pct'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß PREPARE TRAINING DATA\n",
    "# Combine all ticker data\n",
    "combined_df = pd.concat(all_data.values()).reset_index()\n",
    "\n",
    "# Feature columns (exclude targets, ticker, date)\n",
    "exclude_cols = ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'] + \\\n",
    "               [f'Target_{t}' for t in TARGETS.keys()]\n",
    "\n",
    "feature_cols = [c for c in combined_df.columns if c not in exclude_cols and \n",
    "               combined_df[c].dtype in ['float64', 'int64']]\n",
    "\n",
    "print(f\"üìä Total samples: {len(combined_df):,}\")\n",
    "print(f\"üìä Features: {len(feature_cols)}\")\n",
    "print(f\"\\nüîù Feature columns: {feature_cols[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ecdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ CLEAN DATA\n",
    "# Drop rows with NaN targets (future data not available)\n",
    "target_col = 'Target_quick_5pct'  # Primary target\n",
    "\n",
    "clean_df = combined_df.dropna(subset=[target_col] + feature_cols)\n",
    "print(f\"üìä Clean samples: {len(clean_df):,}\")\n",
    "\n",
    "X = clean_df[feature_cols].values\n",
    "y = clean_df[target_col].values\n",
    "dates = clean_df['Date'].values\n",
    "\n",
    "# Fill any remaining NaN in features\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "print(f\"\\n‚úÖ X shape: {X.shape}\")\n",
    "print(f\"‚úÖ y shape: {y.shape}\")\n",
    "print(f\"‚úÖ Target distribution: {np.mean(y):.1%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° GPU-ACCELERATED LIGHTGBM TRAINING\n",
    "# Check if GPU is available for LightGBM\n",
    "\n",
    "# LightGBM parameters optimized for GPU\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 127,\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Try to use GPU if available\n",
    "try:\n",
    "    lgb_params['device'] = 'gpu'\n",
    "    lgb_params['gpu_platform_id'] = 0\n",
    "    lgb_params['gpu_device_id'] = 0\n",
    "    print(\"üéÆ GPU mode enabled for LightGBM!\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è GPU not available, using CPU\")\n",
    "\n",
    "print(\"\\nüìã LightGBM Parameters:\")\n",
    "for k, v in lgb_params.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a66d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ WALK-FORWARD VALIDATION WITH CONFIDENCE TRACKING\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "n_splits = 20\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "fold_results = []\n",
    "all_predictions = []\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "\n",
    "print(f\"üîÑ Running {n_splits}-fold walk-forward validation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "    # Train\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[test_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Track best model\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "    \n",
    "    # Track confidence-based win rates\n",
    "    for conf_threshold in [0.6, 0.7, 0.8, 0.85, 0.9, 0.95]:\n",
    "        high_conf_mask = y_pred >= conf_threshold\n",
    "        if high_conf_mask.sum() > 0:\n",
    "            high_conf_wr = y_test[high_conf_mask].mean()\n",
    "            all_predictions.append({\n",
    "                'fold': fold,\n",
    "                'threshold': conf_threshold,\n",
    "                'signals': high_conf_mask.sum(),\n",
    "                'wins': y_test[high_conf_mask].sum(),\n",
    "                'win_rate': high_conf_wr\n",
    "            })\n",
    "    \n",
    "    fold_results.append({'fold': fold, 'auc': auc, 'samples': len(y_test)})\n",
    "    \n",
    "    if fold % 4 == 0:\n",
    "        print(f\"Fold {fold+1:2d}: AUC = {auc:.4f}, Samples = {len(y_test):,}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "avg_auc = np.mean([r['auc'] for r in fold_results])\n",
    "print(f\"\\nüìä Average AUC: {avg_auc:.4f}\")\n",
    "print(f\"üèÜ Best AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä CONFIDENCE-BASED WIN RATE ANALYSIS\n",
    "pred_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "print(\"\\nüìà WIN RATE BY CONFIDENCE THRESHOLD:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for threshold in [0.6, 0.7, 0.8, 0.85, 0.9, 0.95]:\n",
    "    mask = pred_df['threshold'] == threshold\n",
    "    subset = pred_df[mask]\n",
    "    total_signals = subset['signals'].sum()\n",
    "    total_wins = subset['wins'].sum()\n",
    "    avg_wr = total_wins / total_signals if total_signals > 0 else 0\n",
    "    \n",
    "    emoji = \"üî•\" if avg_wr > 0.80 else \"‚úÖ\" if avg_wr > 0.70 else \"\"\n",
    "    print(f\"   >{threshold*100:.0f}% conf: {total_signals:5,} signals, {avg_wr*100:5.1f}% win rate {emoji}\")\n",
    "\n",
    "print(\"\\nüí° Use >85% confidence for ELITE signals!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• TRAIN FINAL UNRESTRICTED MODEL ON ALL DATA\n",
    "print(\"üî• Training FINAL unrestricted model on ALL data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# More aggressive params for final model\n",
    "final_params = lgb_params.copy()\n",
    "final_params['num_leaves'] = 255\n",
    "final_params['max_depth'] = 15\n",
    "final_params['learning_rate'] = 0.02\n",
    "\n",
    "# Train on all data\n",
    "full_train_data = lgb.Dataset(X, label=y)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    final_params,\n",
    "    full_train_data,\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Final model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä FEATURE IMPORTANCE\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': final_model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîù TOP 20 MOST IMPORTANT FEATURES:\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in importance_df.head(20).iterrows():\n",
    "    bar = '‚ñà' * int(row['importance'] / importance_df['importance'].max() * 20)\n",
    "    print(f\"{row['feature']:25} {row['importance']:8.0f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09082b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ SAVE MODEL AND ARTIFACTS\n",
    "import json\n",
    "from google.colab import files\n",
    "\n",
    "# Create model directory\n",
    "!mkdir -p quantum_models\n",
    "\n",
    "# Save LightGBM model\n",
    "final_model.save_model('quantum_models/unrestricted_model.txt')\n",
    "print(\"‚úÖ Model saved: unrestricted_model.txt\")\n",
    "\n",
    "# Save feature columns\n",
    "with open('quantum_models/feature_cols.json', 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print(\"‚úÖ Features saved: feature_cols.json\")\n",
    "\n",
    "# Save training stats\n",
    "stats = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'tickers': TICKERS,\n",
    "    'start_date': START_DATE,\n",
    "    'end_date': END_DATE,\n",
    "    'total_samples': len(X),\n",
    "    'n_features': len(feature_cols),\n",
    "    'best_auc': float(best_auc),\n",
    "    'avg_auc': float(avg_auc),\n",
    "    'target': 'quick_5pct (5% in 3 days)',\n",
    "    'params': final_params\n",
    "}\n",
    "\n",
    "with open('quantum_models/training_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2, default=str)\n",
    "print(\"‚úÖ Stats saved: training_stats.json\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv('quantum_models/feature_importance.csv', index=False)\n",
    "print(\"‚úÖ Feature importance saved!\")\n",
    "\n",
    "print(\"\\nüì¶ All artifacts saved to quantum_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ TODAY'S PREDICTIONS - RUN THE ORACLE\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîÆ QUANTUM ORACLE - TODAY'S PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "todays_predictions = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        # Get fresh data\n",
    "        df = yf.download(ticker, period='6mo', progress=False)\n",
    "        if len(df) < 50:\n",
    "            continue\n",
    "            \n",
    "        df = calculate_features(df)\n",
    "        \n",
    "        # Get latest features\n",
    "        latest_features = df[feature_cols].iloc[-1:].values\n",
    "        latest_features = np.nan_to_num(latest_features, nan=0.0)\n",
    "        \n",
    "        # Predict\n",
    "        confidence = final_model.predict(latest_features)[0]\n",
    "        \n",
    "        # Get supporting info\n",
    "        rsi = df['RSI'].iloc[-1] if 'RSI' in df.columns else 50\n",
    "        macd_hist = df['MACD_Hist_5_13'].iloc[-1] if 'MACD_Hist_5_13' in df.columns else 0\n",
    "        vol_ratio = df['Vol_Ratio_20'].iloc[-1] if 'Vol_Ratio_20' in df.columns else 1\n",
    "        \n",
    "        todays_predictions.append({\n",
    "            'ticker': ticker,\n",
    "            'confidence': confidence,\n",
    "            'signal': 'STRONG BUY' if confidence > 0.85 else 'BUY' if confidence > 0.70 else 'HOLD' if confidence > 0.50 else 'AVOID',\n",
    "            'rsi': rsi,\n",
    "            'macd_hist': macd_hist,\n",
    "            'vol_ratio': vol_ratio,\n",
    "            'price': float(df['Close'].iloc[-1])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "# Sort by confidence\n",
    "todays_predictions = sorted(todays_predictions, key=lambda x: -x['confidence'])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüü¢ BUY SIGNALS (Confidence > 70%):\")\n",
    "print(\"-\" * 70)\n",
    "buy_count = 0\n",
    "for p in todays_predictions:\n",
    "    if p['confidence'] >= 0.70:\n",
    "        buy_count += 1\n",
    "        emoji = \"üî•\" if p['confidence'] >= 0.85 else \"‚úÖ\"\n",
    "        print(f\"{emoji} {p['ticker']:5} | {p['signal']:11} | Conf: {p['confidence']*100:5.1f}% | RSI: {p['rsi']:.0f} | Vol: {p['vol_ratio']:.1f}x | ${p['price']:.2f}\")\n",
    "\n",
    "if buy_count == 0:\n",
    "    print(\"   No high-confidence buy signals today\")\n",
    "\n",
    "print(f\"\\nüìä Total tickers analyzed: {len(todays_predictions)}\")\n",
    "print(f\"üéØ Buy signals (>70% conf): {sum(1 for p in todays_predictions if p['confidence'] >= 0.70)}\")\n",
    "print(f\"üî• Elite signals (>85% conf): {sum(1 for p in todays_predictions if p['confidence'] >= 0.85)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ SAVE TODAY'S PREDICTIONS\n",
    "pred_output = {\n",
    "    'generated_at': datetime.now().isoformat(),\n",
    "    'model': 'unrestricted_model',\n",
    "    'target': '5% in 3 days',\n",
    "    'predictions': todays_predictions\n",
    "}\n",
    "\n",
    "with open('quantum_models/todays_predictions.json', 'w') as f:\n",
    "    json.dump(pred_output, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Today's predictions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ DOWNLOAD ALL ARTIFACTS\n",
    "print(\"\\nüì¶ Packaging model artifacts for download...\")\n",
    "\n",
    "!zip -r quantum_models.zip quantum_models/\n",
    "print(\"\\n‚úÖ Created quantum_models.zip\")\n",
    "print(\"\\nüì• Click below to download:\")\n",
    "\n",
    "files.download('quantum_models.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef58108",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ NEXT STEPS\n",
    "\n",
    "1. **Download** `quantum_models.zip` \n",
    "2. **Extract** to your local `quantum-ai-trader_v1.1` folder\n",
    "3. **Run** the local dashboard with the new model\n",
    "\n",
    "### Model Files:\n",
    "- `unrestricted_model.txt` - The trained LightGBM model\n",
    "- `feature_cols.json` - List of required features\n",
    "- `training_stats.json` - Training metadata\n",
    "- `feature_importance.csv` - Feature rankings\n",
    "- `todays_predictions.json` - Latest predictions\n",
    "\n",
    "### Expected Performance:\n",
    "- **Target**: 5% in 3 days\n",
    "- **Elite signals (>85% conf)**: ~90% win rate\n",
    "- **Expected value per trade**: +4.33%\n",
    "\n",
    "üî• **Let's beat your 7% day!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
