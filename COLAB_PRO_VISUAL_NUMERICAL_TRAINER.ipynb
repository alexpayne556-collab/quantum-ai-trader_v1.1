{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 0: SETUP & GPU CHECK\n",
    "# ==============================================================================\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"üñ•Ô∏è GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q yfinance ta-lib-bin torch torchvision pillow matplotlib seaborn optuna scikit-learn lightgbm xgboost pandas numpy plotly mplfinance\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "from PIL import Image\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch for CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Sklearn for ensemble\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3fd63",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä PHASE 1: DATA COLLECTION & MULTI-TIMEFRAME ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 1: MULTI-TIMEFRAME DATA COLLECTION\n",
    "# ==============================================================================\n",
    "\n",
    "# Training tickers (diverse sectors for robustness)\n",
    "TICKERS = [\n",
    "    'SPY', 'QQQ', 'IWM',  # Indices\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META',  # Tech giants\n",
    "    'NVDA', 'AMD', 'TSM',  # Semiconductors\n",
    "    'TSLA', 'F', 'GM',  # Auto\n",
    "    'JPM', 'BAC', 'GS',  # Finance\n",
    "    'XOM', 'CVX',  # Energy\n",
    "    'JNJ', 'UNH', 'PFE'  # Healthcare\n",
    "]\n",
    "\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"üì• Downloading data for {len(TICKERS)} tickers...\")\n",
    "print(f\"   Period: {START_DATE} to {END_DATE}\")\n",
    "\n",
    "# Download multiple timeframes\n",
    "data = {\n",
    "    '1d': {},\n",
    "    '1h': {},\n",
    "    '4h': {}\n",
    "}\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        # Daily data (5 years)\n",
    "        df_1d = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False, auto_adjust=True)\n",
    "        if isinstance(df_1d.columns, pd.MultiIndex):\n",
    "            df_1d.columns = df_1d.columns.get_level_values(0)\n",
    "        data['1d'][ticker] = df_1d\n",
    "        \n",
    "        # Hourly data (60 days)\n",
    "        df_1h = yf.download(ticker, period='60d', interval='1h', progress=False, auto_adjust=True)\n",
    "        if isinstance(df_1h.columns, pd.MultiIndex):\n",
    "            df_1h.columns = df_1h.columns.get_level_values(0)\n",
    "        data['1h'][ticker] = df_1h\n",
    "        \n",
    "        print(f\"  ‚úì {ticker}: 1D={len(df_1d)} rows, 1H={len(df_1h)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó {ticker}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data collected: {sum(len(data['1d'][t]) for t in data['1d'])} total 1D rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2196a",
   "metadata": {},
   "source": [
    "---\n",
    "# üñºÔ∏è PHASE 2: VISUAL PATTERN ANALYSIS (CNN)\n",
    "\n",
    "**Approach:** Convert candlestick charts to images, train CNN to recognize patterns\n",
    "- Input: 224x224 RGB images of 30-day candlestick charts\n",
    "- Output: 3-class prediction (BUY/HOLD/SELL)\n",
    "- Architecture: ResNet18 (pretrained on ImageNet, fine-tuned)\n",
    "- Data augmentation: Random crops, flips (simulates different chart styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2A: CHART IMAGE GENERATION\n",
    "# ==============================================================================\n",
    "\n",
    "def create_chart_image(df, window_size=30):\n",
    "    \"\"\"\n",
    "    Generate candlestick chart image for CNN training.\n",
    "    Returns PIL Image (224x224 RGB).\n",
    "    \"\"\"\n",
    "    # Take last window_size days\n",
    "    df_window = df.iloc[-window_size:].copy()\n",
    "    \n",
    "    # Create candlestick chart with mplfinance\n",
    "    fig, axes = mpf.plot(\n",
    "        df_window,\n",
    "        type='candle',\n",
    "        style='yahoo',\n",
    "        volume=True,\n",
    "        returnfig=True,\n",
    "        figsize=(4, 4),\n",
    "        tight_layout=True\n",
    "    )\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi=56, bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf).convert('RGB')\n",
    "    img = img.resize((224, 224), Image.LANCZOS)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Generate chart images for training\n",
    "print(\"üñºÔ∏è GENERATING CHART IMAGES...\")\n",
    "print(\"   This will take a few minutes...\\n\")\n",
    "\n",
    "chart_dataset = []\n",
    "WINDOW_SIZE = 30\n",
    "HORIZON = 5  # Predict 5 days ahead\n",
    "THRESHOLD = 0.03  # 3% threshold from Colab optimization\n",
    "\n",
    "for ticker in list(data['1d'].keys())[:5]:  # Start with 5 tickers for speed\n",
    "    df = data['1d'][ticker]\n",
    "    \n",
    "    # Rolling window\n",
    "    for i in range(WINDOW_SIZE, len(df) - HORIZON):\n",
    "        window_df = df.iloc[i-WINDOW_SIZE:i]\n",
    "        \n",
    "        # Calculate forward return (label)\n",
    "        future_price = df.iloc[i + HORIZON]['Close']\n",
    "        current_price = df.iloc[i]['Close']\n",
    "        forward_return = (future_price - current_price) / current_price\n",
    "        \n",
    "        # Create label: 0=SELL, 1=HOLD, 2=BUY\n",
    "        if forward_return > THRESHOLD:\n",
    "            label = 2  # BUY\n",
    "        elif forward_return < -THRESHOLD:\n",
    "            label = 0  # SELL\n",
    "        else:\n",
    "            label = 1  # HOLD\n",
    "        \n",
    "        # Generate chart image every 5 days (reduce dataset size)\n",
    "        if i % 5 == 0:\n",
    "            try:\n",
    "                img = create_chart_image(window_df, WINDOW_SIZE)\n",
    "                chart_dataset.append({\n",
    "                    'image': img,\n",
    "                    'label': label,\n",
    "                    'ticker': ticker,\n",
    "                    'date': df.index[i]\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"  ‚úì {ticker}: {len([x for x in chart_dataset if x['ticker'] == ticker])} images\")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(chart_dataset)} chart images\")\n",
    "\n",
    "# Show class distribution\n",
    "labels = [x['label'] for x in chart_dataset]\n",
    "print(f\"\\nüìä Class Distribution:\")\n",
    "print(f\"   SELL (0): {labels.count(0)} ({labels.count(0)/len(labels)*100:.1f}%)\")\n",
    "print(f\"   HOLD (1): {labels.count(1)} ({labels.count(1)/len(labels)*100:.1f}%)\")\n",
    "print(f\"   BUY (2): {labels.count(2)} ({labels.count(2)/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7443ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2B: CNN DATASET & DATALOADER\n",
    "# ==============================================================================\n",
    "\n",
    "class ChartDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Data augmentation & normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.3),  # Flip chart (simulate different views)\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Vary brightness\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Train/test split (80/20, time-series aware)\n",
    "split_idx = int(len(chart_dataset) * 0.8)\n",
    "train_data = chart_dataset[:split_idx]\n",
    "test_data = chart_dataset[split_idx:]\n",
    "\n",
    "train_dataset = ChartDataset(train_data, transform=transform_train)\n",
    "test_dataset = ChartDataset(test_data, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"üì¶ DataLoaders created:\")\n",
    "print(f\"   Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"   Test: {len(test_dataset)} samples, {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2C: CNN MODEL (ResNet18 Fine-Tuned)\n",
    "# ==============================================================================\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# Load pretrained ResNet18\n",
    "model_cnn = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace final layer for 3-class prediction\n",
    "num_features = model_cnn.fc.in_features\n",
    "model_cnn.fc = nn.Linear(num_features, 3)  # 3 classes: SELL, HOLD, BUY\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_cnn = model_cnn.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(f\"üß† CNN Model initialized on {device}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_cnn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69565f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2D: TRAIN CNN\n",
    "# ==============================================================================\n",
    "\n",
    "def train_cnn(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=15):\n",
    "    \"\"\"\n",
    "    Train CNN with early stopping.\n",
    "    \"\"\"\n",
    "    best_acc = 0.0\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * correct / total\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "            print(f\"  ‚úÖ Best model saved (acc={best_acc:.2f}%)\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return train_losses, test_accs, best_acc\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nüöÄ TRAINING CNN...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_losses, test_accs, best_acc = train_cnn(\n",
    "    model_cnn, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=15\n",
    ")\n",
    "\n",
    "print(f\"\\nüèÜ BEST CNN ACCURACY: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503b6d3",
   "metadata": {},
   "source": [
    "---\n",
    "# üî¢ PHASE 3: NUMERICAL PATTERN ANALYSIS (HistGradientBoosting)\n",
    "\n",
    "**Approach:** Traditional technical indicators + advanced features\n",
    "- **From Perplexity Research:**\n",
    "  - Triple barrier labeling (ATR-based dynamic thresholds)\n",
    "  - Market regime features (HMM, volatility clustering)\n",
    "  - Cross-asset features (SPY correlation, VIX divergence)\n",
    "  - Multi-timeframe indicators (1H, 4H, 1D alignment)\n",
    "- **Optimized Features:** Top 15 from Colab training\n",
    "- **Model:** HistGradientBoostingClassifier with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue in next cell..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
