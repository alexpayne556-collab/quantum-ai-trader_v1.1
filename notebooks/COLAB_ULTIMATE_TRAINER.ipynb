{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964003a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import subprocess\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "    print(\"‚úÖ GPU DETECTED:\")\n",
    "    print(gpu_info)\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è NO GPU DETECTED - Training will be slower\")\n",
    "    print(\"   Enable GPU: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488aa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a958e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory (adjust path to your folder)\n",
    "import os\n",
    "WORKSPACE_DIR = '/content/drive/MyDrive/quantum-ai-trader_v1.1'\n",
    "\n",
    "# If folder doesn't exist, clone from GitHub\n",
    "if not os.path.exists(WORKSPACE_DIR):\n",
    "    print(\"üì• Cloning repository...\")\n",
    "    !git clone https://github.com/alexpayne556-collab/quantum-ai-trader_v1.1.git $WORKSPACE_DIR\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ Workspace found\")\n",
    "\n",
    "os.chdir(WORKSPACE_DIR)\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ML requirements\n",
    "print(\"üì¶ Installing ML packages...\")\n",
    "!pip install -q -r requirements_ml.txt\n",
    "print(\"‚úÖ Packages installed\")\n",
    "\n",
    "# Verify GPU support\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "print(f\"\\n‚úÖ XGBoost version: {xgb.__version__}\")\n",
    "print(f\"‚úÖ LightGBM version: {lgb.__version__}\")\n",
    "print(f\"‚úÖ XGBoost GPU support: {xgb.build_info()['USE_CUDA']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461cad4d",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e13969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, WORKSPACE_DIR)\n",
    "\n",
    "from src.ml.dataset_loader import DatasetLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìÇ Loading training dataset...\")\n",
    "\n",
    "# Option 1: Load from existing CSV\n",
    "loader = DatasetLoader(data_dir='data/training')\n",
    "\n",
    "# Try to load pre-built dataset\n",
    "try:\n",
    "    dataset = loader.load_from_csv('data/training/training_dataset.csv')\n",
    "    print(\"‚úÖ Loaded existing dataset\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è No existing dataset found\")\n",
    "    print(\"üì• Building dataset from scratch...\")\n",
    "    \n",
    "    # Option 2: Build from scratch (simplified)\n",
    "    tickers = ['NVDA', 'TSLA', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NFLX',\n",
    "               'AMD', 'INTC', 'ORCL', 'CRM', 'ADBE', 'PYPL', 'SQ', 'SHOP']\n",
    "    \n",
    "    dataset = loader.download_and_build_dataset(\n",
    "        tickers=tickers,\n",
    "        period='2y',\n",
    "        min_samples_per_ticker=100\n",
    "    )\n",
    "    \n",
    "    # Save for future use\n",
    "    loader.save_dataset(dataset, 'data/training/training_dataset.csv')\n",
    "\n",
    "# Validate dataset\n",
    "is_valid, message = loader.validate_dataset(dataset)\n",
    "if not is_valid:\n",
    "    raise ValueError(f\"Dataset validation failed: {message}\")\n",
    "\n",
    "print(\"\\n‚úÖ DATASET READY\")\n",
    "print(f\"   Samples: {len(dataset['X']):,}\")\n",
    "print(f\"   Features: {dataset['X'].shape[1]}\")\n",
    "print(f\"   Tickers: {dataset['tickers'].nunique()}\")\n",
    "print(f\"   Label distribution: {dict(dataset['y'].value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e085c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initialize Trident Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.train_trident import TridenTrainer\n",
    "\n",
    "print(\"üîß Initializing Trident Trainer...\")\n",
    "\n",
    "trainer = TridenTrainer(\n",
    "    use_gpu=True,                    # Enable GPU acceleration\n",
    "    optimize_hyperparams=True,       # Run Optuna optimization\n",
    "    n_trials=50,                     # 50 trials per model (150 total per cluster)\n",
    "    cv_folds=5,                      # 5-fold cross-validation\n",
    "    n_clusters=5,                    # 5 ticker clusters\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print(f\"   GPU enabled: {trainer.use_gpu}\")\n",
    "print(f\"   Optimization trials: {trainer.n_trials} per model\")\n",
    "print(f\"   CV folds: {trainer.cv_folds}\")\n",
    "print(f\"   Clusters: {trainer.n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c03932",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Train Trident Ensemble\n",
    "\n",
    "‚è±Ô∏è **Expected time:** 2.5-5 hours on Colab Pro GPU\n",
    "\n",
    "**What happens:**\n",
    "1. Cluster tickers into 5 groups (K-Means)\n",
    "2. For each cluster, train 3 models:\n",
    "   - XGBoost (pure tabular)\n",
    "   - LightGBM (speed + microstructure)\n",
    "   - CatBoost (categorical + robust)\n",
    "3. Optuna optimization (50 trials √ó 3 models √ó 5 clusters = 750 trials)\n",
    "4. PurgedKFold CV (5 folds, 1% embargo)\n",
    "5. SHAP feature importance\n",
    "6. Save 15 models + reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07125ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING TRIDENT TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Expected duration: 2.5-5 hours\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# TRAIN\n",
    "results = trainer.train(\n",
    "    X=dataset['X'],\n",
    "    y=dataset['y'],\n",
    "    tickers=dataset['tickers'],\n",
    "    ticker_features=dataset['ticker_features']\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "hours, remainder = divmod(elapsed, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Duration: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "print(f\"\\nModels trained: {len(results['models'])}\")\n",
    "print(f\"Clusters created: {results['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94015e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cluster assignments\n",
    "print(\"\\nüìä CLUSTER ASSIGNMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cluster_df = pd.DataFrame([\n",
    "    {'Cluster': k, 'Name': v['name'], 'Tickers': ', '.join(v['tickers'][:5]) + '...'}\n",
    "    for k, v in results['clusters'].items()\n",
    "])\n",
    "print(cluster_df.to_string(index=False))\n",
    "\n",
    "# Display CV accuracies\n",
    "print(\"\\nüìà CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cluster_id in range(results['n_clusters']):\n",
    "    print(f\"\\nCluster {cluster_id}: {results['clusters'][cluster_id]['name']}\")\n",
    "    for model_name in ['xgb', 'lgb', 'cat']:\n",
    "        key = f'cluster_{cluster_id}_{model_name}'\n",
    "        if key in results['models']:\n",
    "            model_info = results['models'][key]\n",
    "            print(f\"   {model_name.upper()}: {model_info['cv_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ab9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SHAP feature importance (top 10 global)\n",
    "print(\"\\nüîç TOP 10 GLOBAL FEATURES (SHAP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'shap_importances' in results:\n",
    "    # Average SHAP values across all models\n",
    "    all_importances = {}\n",
    "    for cluster_id in range(results['n_clusters']):\n",
    "        key = f'cluster_{cluster_id}'\n",
    "        if key in results['shap_importances']:\n",
    "            for feat, val in results['shap_importances'][key].items():\n",
    "                if feat not in all_importances:\n",
    "                    all_importances[feat] = []\n",
    "                all_importances[feat].append(val)\n",
    "    \n",
    "    # Average and sort\n",
    "    avg_importances = {k: np.mean(v) for k, v in all_importances.items()}\n",
    "    top_10 = sorted(avg_importances.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    for i, (feat, importance) in enumerate(top_10, 1):\n",
    "        print(f\"{i:2d}. {feat:30s} {importance:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SHAP importances not computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2169d",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Save Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cf20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "output_dir = '/content/drive/MyDrive/trident_models'\n",
    "\n",
    "print(f\"üíæ Saving models to {output_dir}...\")\n",
    "\n",
    "trainer.save_models(\n",
    "    output_dir=output_dir,\n",
    "    results=results\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ MODELS SAVED\")\n",
    "print(f\"   Location: {output_dir}\")\n",
    "print(f\"   Files:\")\n",
    "print(f\"      - 15 model files (cluster_X_{{xgb,lgb,cat}}.*)\")\n",
    "print(f\"      - cluster_assignments.json\")\n",
    "print(f\"      - training_report.md\")\n",
    "print(f\"      - ticker_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a0ea3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.inference_engine import TridenInference\n",
    "\n",
    "print(\"üß™ Testing inference engine...\")\n",
    "\n",
    "# Initialize inference\n",
    "engine = TridenInference(model_dir=output_dir)\n",
    "\n",
    "# Get a sample from dataset\n",
    "sample_idx = 0\n",
    "sample_ticker = dataset['tickers'].iloc[sample_idx]\n",
    "sample_features = dataset['X'].iloc[sample_idx]\n",
    "\n",
    "# Predict\n",
    "prediction = engine.predict(\n",
    "    ticker=sample_ticker,\n",
    "    features=sample_features\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ INFERENCE TEST\")\n",
    "print(f\"   Ticker: {prediction['ticker']}\")\n",
    "print(f\"   Signal: {prediction['signal']}\")\n",
    "print(f\"   Confidence: {prediction['confidence']:.1f}%\")\n",
    "print(f\"   Probability: {prediction['probability']:.3f}\")\n",
    "print(f\"   Cluster ID: {prediction['cluster_id']}\")\n",
    "print(f\"   Model votes: {prediction['model_votes']}\")\n",
    "print(f\"   Timestamp: {prediction['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eaa17b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: View Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training report\n",
    "report_path = f\"{output_dir}/training_report.md\"\n",
    "\n",
    "if os.path.exists(report_path):\n",
    "    with open(report_path, 'r') as f:\n",
    "        report = f.read()\n",
    "    \n",
    "    from IPython.display import Markdown\n",
    "    display(Markdown(report))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdd962",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ TRAINING COMPLETE\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **Download models** from Google Drive to your local workspace\n",
    "2. **Run backtest** using `src/ml/backtest_trident.py`\n",
    "3. **Analyze SHAP** using `notebooks/SHAP_ANALYSIS.ipynb`\n",
    "4. **Test in production** using inference_engine.py\n",
    "5. **Build Portfolio Tracker** (Day 7)\n",
    "6. **Build Watchlist Engine** (Day 8)\n",
    "7. **Build Ultimate Companion** (Week 2)\n",
    "\n",
    "**Expected Performance:**\n",
    "- Baseline: 71.1% WR\n",
    "- After Trident: 75-80% WR ‚ú®\n",
    "- Sharpe Ratio: 2.5-3.5\n",
    "- Max Drawdown: -10% to -15%\n",
    "\n",
    "**Ready to make 15%/day sustainable!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
