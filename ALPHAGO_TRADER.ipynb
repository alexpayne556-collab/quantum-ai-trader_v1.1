{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de116de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Setup\n",
    "!pip install -q yfinance lightgbm torch numpy pandas\n",
    "import numpy as np, pandas as pd, yfinance as yf, torch, torch.nn as nn\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "import random, json, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'üéÆ Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc80273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Your 50 Tickers\n",
    "TICKERS = ['APLD','SERV','MRVL','HOOD','LUNR','BAC','QCOM','UUUU','TSLA','AMD',\n",
    "           'NOW','NVDA','MU','PG','DLB','XME','KRYS','LEU','QTUM','SPY',\n",
    "           'UNH','WMT','OKLO','RXRX','MTZ','SNOW','GRRR','BSX','LLY','VOO',\n",
    "           'GEO','CXW','LYFT','MNDY','BA','LAC','INTC','ALK','LMT','CRDO',\n",
    "           'ANET','META','RIVN','GOOGL','HL','TEM','TDOC','KMTS','SCHA','B']\n",
    "print(f'üìä {len(TICKERS)} tickers loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f12d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Download ALL historical data (2015-now)\n",
    "print('üì• Downloading 10 years of data...')\n",
    "ALL_DATA = {}\n",
    "for t in TICKERS:\n",
    "    try:\n",
    "        df = yf.download(t, start='2015-01-01', progress=False)\n",
    "        if len(df) > 500:\n",
    "            if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)\n",
    "            ALL_DATA[t] = df\n",
    "    except: pass\n",
    "print(f'‚úÖ {len(ALL_DATA)} tickers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ca7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Trading Environment V4 - ALPHAGO MINDSET\n",
    "# Never get stuck. Losses = Learning. Save cash for dips. Lock in what works.\n",
    "\n",
    "class StrategyDNA:\n",
    "    \"\"\"IMMUTABLE PATTERNS - The Secret Sauce (Proven through backtesting)\"\"\"\n",
    "    \n",
    "    # DIP BUY - Where BIG WINS come from\n",
    "    DIP_BUY = {'min_drop': -0.08, 'max_rsi': 35, 'min_vol': 1.3}\n",
    "    \n",
    "    # PROFIT TAKE - Like your HOOD trade (8%!)\n",
    "    PROFIT_TAKE = {'min_gain': 0.05, 'target': 0.08, 'max_rsi': 70}\n",
    "    \n",
    "    # CUT LOSS - Never let losers run\n",
    "    CUT_LOSS = {'max_loss': -0.05, 'max_days': 3}\n",
    "    \n",
    "    # CASH MANAGEMENT - Always ready for opportunities\n",
    "    CASH = {'min_reserve': 0.20, 'max_position': 0.15, 'dip_deploy': 0.25}\n",
    "\n",
    "\n",
    "class TradingEnv:\n",
    "    \"\"\"\n",
    "    AlphaGo Trading Environment - KEY PRINCIPLES:\n",
    "    1. Losses are LEARNING opportunities, not failures\n",
    "    2. Cash preservation = Future opportunity\n",
    "    3. Never get stuck in losing positions\n",
    "    4. Adapt strategy based on what's working\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, cash=10000):\n",
    "        self.data = data\n",
    "        self.start_cash = cash\n",
    "        self.dna = StrategyDNA\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cash = self.start_cash\n",
    "        self.positions = {}\n",
    "        self.day_trades = deque(maxlen=5)\n",
    "        self.day = 60\n",
    "        self.history = []\n",
    "        \n",
    "        # Performance tracking for adaptation\n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "        self.total_trades = 0\n",
    "        self.losing_streak = 0\n",
    "        self.recent_actions = deque(maxlen=20)\n",
    "        \n",
    "        # Learning state\n",
    "        self.pattern_hits = {'dip_buy': 0, 'profit_take': 0, 'cut_loss': 0}\n",
    "        self.pattern_success = {'dip_buy': 0, 'profit_take': 0, 'cut_loss': 0}\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Rich state with AlphaGo features\"\"\"\n",
    "        states = []\n",
    "        for t, df in self.data.items():\n",
    "            if self.day >= len(df): continue\n",
    "            \n",
    "            # Price data\n",
    "            c = df['Close'].iloc[max(0,self.day-20):self.day+1].values\n",
    "            v = df['Volume'].iloc[max(0,self.day-20):self.day+1].values\n",
    "            h = df['High'].iloc[max(0,self.day-20):self.day+1].values\n",
    "            l = df['Low'].iloc[max(0,self.day-20):self.day+1].values\n",
    "            if len(c) < 10: continue\n",
    "            \n",
    "            # Basic features\n",
    "            ret5 = (c[-1]/c[-6]-1) if len(c)>5 else 0\n",
    "            ret10 = (c[-1]/c[-11]-1) if len(c)>10 else 0\n",
    "            vol_r = v[-1]/(np.mean(v)+1) if len(v)>1 else 1\n",
    "            rsi = self._rsi(c)\n",
    "            \n",
    "            # DIP DETECTION - This is where BIG WINS come from\n",
    "            high_20 = max(c)\n",
    "            low_20 = min(c)\n",
    "            drawdown = (c[-1] - high_20) / high_20\n",
    "            recovery = (c[-1] - low_20) / (high_20 - low_20 + 0.001)\n",
    "            \n",
    "            # VOLATILITY - Squeeze before explosion\n",
    "            atr = np.mean(h[-14:] - l[-14:]) if len(h) >= 14 else 0\n",
    "            atr_pct = atr / c[-1] if c[-1] > 0 else 0\n",
    "            vol_squeeze = 1 if atr_pct < 0.02 else 0\n",
    "            vol_spike = 1 if vol_r > 2.0 else 0\n",
    "            \n",
    "            # MOMENTUM QUALITY\n",
    "            if len(c) >= 10:\n",
    "                mom_5 = c[-1] - c[-6]\n",
    "                mom_10 = c[-6] - c[-11] if len(c) > 10 else 0\n",
    "                momentum_accel = 1 if mom_5 > mom_10 else -1\n",
    "            else:\n",
    "                momentum_accel = 0\n",
    "            \n",
    "            # SECRET SAUCE SIGNALS - Encoded from proven patterns\n",
    "            dip_buy_signal = 1 if (drawdown < self.dna.DIP_BUY['min_drop'] and \n",
    "                                   rsi < self.dna.DIP_BUY['max_rsi'] and \n",
    "                                   vol_r > self.dna.DIP_BUY['min_vol']) else 0\n",
    "            \n",
    "            profit_take_signal = 1 if (recovery > 0.85 and \n",
    "                                       rsi > self.dna.PROFIT_TAKE['max_rsi']) else 0\n",
    "            \n",
    "            # POSITION CONTEXT - Are we in this trade?\n",
    "            in_position = 1 if t in self.positions else 0\n",
    "            position_pnl = 0\n",
    "            days_held = 0\n",
    "            if in_position:\n",
    "                pos = self.positions[t]\n",
    "                position_pnl = (c[-1] / pos['entry'] - 1)\n",
    "                days_held = self.day - pos['day']\n",
    "            \n",
    "            # PORTFOLIO CONTEXT - Cash available for opportunities\n",
    "            total_value = self.cash + sum(\n",
    "                pos['shares'] * self.data[tick]['Close'].iloc[min(self.day, len(self.data[tick])-1)]\n",
    "                for tick, pos in self.positions.items() if tick in self.data\n",
    "            )\n",
    "            cash_ratio = self.cash / total_value if total_value > 0 else 1\n",
    "            \n",
    "            states.append({\n",
    "                't': t, 'p': c[-1],\n",
    "                'r5': ret5, 'r10': ret10,\n",
    "                'vol': vol_r, 'rsi': rsi,\n",
    "                'drawdown': drawdown,\n",
    "                'recovery': recovery,\n",
    "                'vol_squeeze': vol_squeeze,\n",
    "                'vol_spike': vol_spike,\n",
    "                'mom_accel': momentum_accel,\n",
    "                'dip_buy': dip_buy_signal,\n",
    "                'profit_take': profit_take_signal,\n",
    "                'in_position': in_position,\n",
    "                'position_pnl': position_pnl,\n",
    "                'days_held': days_held,\n",
    "                'cash_ratio': cash_ratio,\n",
    "                'losing_streak': min(self.losing_streak, 5) / 5  # Normalized\n",
    "            })\n",
    "        return states\n",
    "    \n",
    "    def _rsi(self, c):\n",
    "        d = np.diff(c)\n",
    "        g = np.mean(d[d>0]) if len(d[d>0])>0 else 0\n",
    "        l = abs(np.mean(d[d<0])) if len(d[d<0])>0 else 1e-8\n",
    "        return 100 - 100/(1+g/l)\n",
    "    \n",
    "    def _can_day_trade(self):\n",
    "        recent = [d for d in self.day_trades if d >= self.day - 5]\n",
    "        return len(recent) < 3\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Execute actions with AlphaGo reward structure:\n",
    "        - BIG rewards for following proven patterns\n",
    "        - PENALTIES for holding losers\n",
    "        - BONUS for cash preservation during bad times\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        trades_log = []\n",
    "        \n",
    "        for act in actions:\n",
    "            t, a = act['t'], act['a']\n",
    "            if t not in self.data: continue\n",
    "            df = self.data[t]\n",
    "            if self.day >= len(df): continue\n",
    "            price = df['Close'].iloc[self.day]\n",
    "            \n",
    "            state = next((s for s in self._get_state() if s['t'] == t), None)\n",
    "            if not state: continue\n",
    "            \n",
    "            if a == 'buy':\n",
    "                # Check cash management rules\n",
    "                max_position_value = self.cash * self.dna.CASH['max_position']\n",
    "                min_cash_after = self.cash * self.dna.CASH['min_reserve']\n",
    "                \n",
    "                if self.cash > price and self.cash - price > min_cash_after:\n",
    "                    shares = min(int(max_position_value / price), int((self.cash - min_cash_after) / price))\n",
    "                    \n",
    "                    if shares > 0:\n",
    "                        cost = shares * price\n",
    "                        self.cash -= cost\n",
    "                        \n",
    "                        if t in self.positions:\n",
    "                            old = self.positions[t]\n",
    "                            total_shares = old['shares'] + shares\n",
    "                            avg_price = (old['shares']*old['entry'] + cost) / total_shares\n",
    "                            self.positions[t] = {'shares': total_shares, 'entry': avg_price, 'day': old['day']}\n",
    "                        else:\n",
    "                            self.positions[t] = {'shares': shares, 'entry': price, 'day': self.day}\n",
    "                        \n",
    "                        trades_log.append(f'BUY {t}')\n",
    "                        self.recent_actions.append('buy')\n",
    "                        \n",
    "                        # ALPHAGO REWARD: Following the SECRET SAUCE\n",
    "                        if state['dip_buy']:\n",
    "                            reward += 75  # BIG reward for dip buying\n",
    "                            self.pattern_hits['dip_buy'] += 1\n",
    "                        elif state['drawdown'] < -0.05:\n",
    "                            reward += 30  # Smaller dip\n",
    "                        elif state['vol_squeeze']:\n",
    "                            reward += 20  # Squeeze breakout potential\n",
    "                        else:\n",
    "                            reward += 5  # Any buy\n",
    "            \n",
    "            elif a == 'sell' and t in self.positions:\n",
    "                pos = self.positions[t]\n",
    "                if pos['day'] == self.day: continue\n",
    "                if pos['day'] == self.day - 1 and not self._can_day_trade(): continue\n",
    "                \n",
    "                proceeds = pos['shares'] * price\n",
    "                profit = proceeds - (pos['shares'] * pos['entry'])\n",
    "                pct = profit / (pos['shares'] * pos['entry']) * 100\n",
    "                days_held = self.day - pos['day']\n",
    "                \n",
    "                self.cash += proceeds\n",
    "                self.total_trades += 1\n",
    "                \n",
    "                # ALPHAGO REWARD STRUCTURE - Lock in what works\n",
    "                if pct >= 8:  # BIG WIN (HOOD trade territory)\n",
    "                    reward += profit * 5\n",
    "                    self.wins += 1\n",
    "                    self.losing_streak = 0\n",
    "                    if state['profit_take']:\n",
    "                        self.pattern_success['profit_take'] += 1\n",
    "                elif pct >= 5:  # Good win\n",
    "                    reward += profit * 3\n",
    "                    self.wins += 1\n",
    "                    self.losing_streak = 0\n",
    "                elif pct >= 2:  # Small win\n",
    "                    reward += profit * 2\n",
    "                    self.wins += 1\n",
    "                    self.losing_streak = 0\n",
    "                elif pct >= 0:  # Breakeven-ish\n",
    "                    reward += profit\n",
    "                    self.wins += 1\n",
    "                elif pct > -3:  # Small loss - GOOD for cutting early\n",
    "                    reward += profit * 0.3  # Less penalty\n",
    "                    self.losses += 1\n",
    "                    self.losing_streak += 1\n",
    "                    self.pattern_hits['cut_loss'] += 1\n",
    "                    self.pattern_success['cut_loss'] += 1\n",
    "                else:  # Big loss - should have cut earlier\n",
    "                    reward += profit * 0.1  # Big penalty\n",
    "                    self.losses += 1\n",
    "                    self.losing_streak += 1\n",
    "                \n",
    "                # BONUS for smart profit taking\n",
    "                if state['profit_take'] and pct > 5:\n",
    "                    reward += 40\n",
    "                    self.pattern_hits['profit_take'] += 1\n",
    "                \n",
    "                # BONUS for quick loss cutting\n",
    "                if pct < -2 and days_held <= 3:\n",
    "                    reward += 20  # Good discipline!\n",
    "                \n",
    "                if pos['day'] >= self.day - 1:\n",
    "                    self.day_trades.append(self.day)\n",
    "                \n",
    "                del self.positions[t]\n",
    "                trades_log.append(f'SELL {t} {pct:+.1f}%')\n",
    "                self.recent_actions.append('sell')\n",
    "            \n",
    "            elif a == 'hold':\n",
    "                self.recent_actions.append('hold')\n",
    "                \n",
    "                # ALPHAGO: Penalize holding losers too long\n",
    "                if t in self.positions:\n",
    "                    pos = self.positions[t]\n",
    "                    if t in self.data and self.day < len(self.data[t]):\n",
    "                        current = self.data[t]['Close'].iloc[self.day]\n",
    "                        pnl_pct = (current / pos['entry'] - 1)\n",
    "                        days_held = self.day - pos['day']\n",
    "                        \n",
    "                        # PENALTY for holding losers - AlphaGo adapts!\n",
    "                        if pnl_pct < self.dna.CUT_LOSS['max_loss']:\n",
    "                            reward -= 30  # Should have cut!\n",
    "                        elif pnl_pct < 0 and days_held >= self.dna.CUT_LOSS['max_days']:\n",
    "                            reward -= 20  # 3 days red = cut\n",
    "                        elif days_held > 15:\n",
    "                            reward -= 10  # Don't marry positions\n",
    "                \n",
    "                # BONUS for holding cash during losing streak\n",
    "                if self.losing_streak >= 2:\n",
    "                    cash_ratio = self.cash / self.start_cash\n",
    "                    if cash_ratio > 0.3:\n",
    "                        reward += 5  # Good cash preservation\n",
    "        \n",
    "        self.day += 1\n",
    "        \n",
    "        # Calculate portfolio value\n",
    "        port_val = self.cash\n",
    "        for t, pos in self.positions.items():\n",
    "            if t in self.data and self.day < len(self.data[t]):\n",
    "                port_val += pos['shares'] * self.data[t]['Close'].iloc[self.day]\n",
    "        \n",
    "        done = self.day >= min(len(df) for df in self.data.values()) - 1\n",
    "        self.history.append({'day': self.day, 'value': port_val})\n",
    "        \n",
    "        win_rate = self.wins / max(1, self.total_trades)\n",
    "        return self._get_state(), reward, done, {\n",
    "            'value': port_val, \n",
    "            'trades': trades_log,\n",
    "            'win_rate': win_rate, \n",
    "            'total_trades': self.total_trades,\n",
    "            'losing_streak': self.losing_streak,\n",
    "            'pattern_hits': self.pattern_hits\n",
    "        }\n",
    "\n",
    "print('‚úÖ Environment V4 - ALPHAGO MINDSET')\n",
    "print('   üß¨ Strategy DNA: Encoded winning patterns')\n",
    "print('   üìâ Dip buying: +75 reward on confirmed dips')\n",
    "print('   üí∞ Profit taking: +40 bonus at right time')\n",
    "print('   ‚úÇÔ∏è  Loss cutting: Rewarded for quick exits')\n",
    "print('   üíµ Cash preservation: Bonuses during streaks')\n",
    "print('   üîÑ Adaptation: Penalizes stuck behavior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22289744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Neural Network Brain V4 - ALPHAGO ARCHITECTURE\n",
    "# Multiple pathways for different market conditions\n",
    "# Learns WHEN to use WHICH strategy\n",
    "\n",
    "class TradingBrain(nn.Module):\n",
    "    \"\"\"\n",
    "    AlphaGo-style architecture:\n",
    "    - Separate pathways for different strategies\n",
    "    - Value head estimates future portfolio value\n",
    "    - Policy head picks optimal action\n",
    "    - Strategy selector chooses which pathway to weight\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features=17, hidden=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        \n",
    "        # STRATEGY PATHWAYS - Each learns a different style\n",
    "        \n",
    "        # 1. DIP BUYER - Specializes in oversold bounces\n",
    "        self.dip_pathway = nn.Sequential(\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "        )\n",
    "        \n",
    "        # 2. MOMENTUM - Rides trends\n",
    "        self.momentum_pathway = nn.Sequential(\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "        )\n",
    "        \n",
    "        # 3. RISK MANAGER - Knows when to exit\n",
    "        self.risk_pathway = nn.Sequential(\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "        )\n",
    "        \n",
    "        # 4. CASH MANAGER - Preserves capital\n",
    "        self.cash_pathway = nn.Sequential(\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "        )\n",
    "        \n",
    "        # Strategy combiner - learns WHEN to use WHICH\n",
    "        self.strategy_gate = nn.Sequential(\n",
    "            nn.Linear(hidden, 4),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Final decision layers\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 3)  # buy/hold/sell\n",
    "        )\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize with action bias - AVOID HOLD PARALYSIS\n",
    "        with torch.no_grad():\n",
    "            self.policy_head[-1].bias[0] = 0.3   # Favor buy\n",
    "            self.policy_head[-1].bias[1] = -0.5  # Discourage hold\n",
    "            self.policy_head[-1].bias[2] = 0.2   # Allow sell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode features\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Run through all strategy pathways\n",
    "        dip_out = self.dip_pathway(features)\n",
    "        mom_out = self.momentum_pathway(features)\n",
    "        risk_out = self.risk_pathway(features)\n",
    "        cash_out = self.cash_pathway(features)\n",
    "        \n",
    "        # Get strategy weights (which pathway to trust)\n",
    "        weights = self.strategy_gate(features)  # [batch, 4]\n",
    "        \n",
    "        # Weighted combination of pathways\n",
    "        combined = torch.cat([\n",
    "            dip_out * weights[:, 0:1],\n",
    "            mom_out * weights[:, 1:2],\n",
    "            risk_out * weights[:, 2:3],\n",
    "            cash_out * weights[:, 3:4],\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Policy with temperature for sharper decisions\n",
    "        logits = self.policy_head(combined) * 2.5\n",
    "        policy = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Value estimate\n",
    "        value = self.value_head(combined)\n",
    "        \n",
    "        return policy, value, weights\n",
    "\n",
    "\n",
    "class ActionSelector:\n",
    "    \"\"\"\n",
    "    Intelligent action selection that respects Strategy DNA\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def select(probs, state, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Select action considering:\n",
    "        1. Neural network probabilities\n",
    "        2. Strategy DNA signals\n",
    "        3. Exploration with signal awareness\n",
    "        \"\"\"\n",
    "        dip_signal = state.get('dip_buy', 0)\n",
    "        profit_signal = state.get('profit_take', 0)\n",
    "        in_position = state.get('in_position', 0)\n",
    "        position_pnl = state.get('position_pnl', 0)\n",
    "        days_held = state.get('days_held', 0)\n",
    "        losing_streak = state.get('losing_streak', 0)\n",
    "        \n",
    "        # Random exploration with signal bias\n",
    "        if random.random() < epsilon:\n",
    "            if dip_signal and not in_position:\n",
    "                # Dip signal + not in position = heavy buy bias\n",
    "                weights = [0.80, 0.10, 0.10]\n",
    "            elif profit_signal and in_position and position_pnl > 0.05:\n",
    "                # Profit signal + winning position = heavy sell bias\n",
    "                weights = [0.10, 0.10, 0.80]\n",
    "            elif in_position and position_pnl < -0.03 and days_held >= 2:\n",
    "                # Losing position held too long = sell bias\n",
    "                weights = [0.10, 0.20, 0.70]\n",
    "            elif losing_streak > 2:\n",
    "                # On losing streak = conservative (hold/small positions)\n",
    "                weights = [0.20, 0.60, 0.20]\n",
    "            else:\n",
    "                # Normal exploration\n",
    "                weights = [0.35, 0.25, 0.40]\n",
    "            \n",
    "            return random.choices([0, 1, 2], weights=weights)[0]\n",
    "        \n",
    "        # Otherwise use network prediction\n",
    "        return torch.argmax(probs).item()\n",
    "\n",
    "\n",
    "brain = TradingBrain().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(brain.parameters(), lr=0.0003, weight_decay=0.01)\n",
    "selector = ActionSelector()\n",
    "\n",
    "n_params = sum(p.numel() for p in brain.parameters())\n",
    "print(f'üß† Brain V4 - ALPHAGO ARCHITECTURE')\n",
    "print(f'   Parameters: {n_params:,}')\n",
    "print(f'   üìâ Dip pathway - Oversold bounces')\n",
    "print(f'   üìà Momentum pathway - Trend riding')\n",
    "print(f'   üõ°Ô∏è  Risk pathway - Exit timing')\n",
    "print(f'   üíµ Cash pathway - Capital preservation')\n",
    "print(f'   üéØ Strategy gate - Learns WHEN to use WHICH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: State Processing & Training Functions V4\n",
    "# AlphaGo-style learning: Never get stuck, always adapting\n",
    "\n",
    "def state_to_tensor(s):\n",
    "    \"\"\"Convert state to 17-feature tensor for V4 brain\"\"\"\n",
    "    return torch.tensor([\n",
    "        # Market features\n",
    "        s['r5'],                           # 5-day return\n",
    "        s['r10'],                          # 10-day return  \n",
    "        s['vol'],                          # Volume ratio\n",
    "        s['rsi'] / 100,                    # RSI normalized\n",
    "        s['p'] / 1000,                     # Price normalized\n",
    "        s['drawdown'],                     # Distance from 20d high\n",
    "        s['recovery'],                     # Position in range\n",
    "        \n",
    "        # Technical signals\n",
    "        s['vol_squeeze'],                  # Volatility squeeze\n",
    "        s['vol_spike'],                    # Volume spike\n",
    "        s['mom_accel'],                    # Momentum acceleration\n",
    "        \n",
    "        # SECRET SAUCE signals\n",
    "        s['dip_buy'],                      # DIP BUY SIGNAL\n",
    "        s['profit_take'],                  # PROFIT TAKE SIGNAL\n",
    "        \n",
    "        # Position context\n",
    "        s['in_position'],                  # Currently holding\n",
    "        s['position_pnl'],                 # Current P&L\n",
    "        s['days_held'] / 20,               # Days held normalized\n",
    "        \n",
    "        # Portfolio context\n",
    "        s['cash_ratio'],                   # Cash available\n",
    "        s['losing_streak'],                # Adaptation signal\n",
    "    ], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "\n",
    "def train_episode(env, brain, optimizer, epsilon=0.3):\n",
    "    \"\"\"\n",
    "    Train one episode with AlphaGo principles:\n",
    "    - Learn from both wins AND losses\n",
    "    - Never get stuck (penalize hold paralysis)\n",
    "    - Adapt to losing streaks\n",
    "    - Lock in winning patterns\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    memories = []\n",
    "    action_counts = {'buy': 0, 'hold': 0, 'sell': 0}\n",
    "    \n",
    "    while True:\n",
    "        actions = []\n",
    "        \n",
    "        for s in state:\n",
    "            x = state_to_tensor(s)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                probs, val, strategy_weights = brain(x.unsqueeze(0))\n",
    "            \n",
    "            # Use intelligent action selection\n",
    "            a = selector.select(probs[0], s, epsilon)\n",
    "            \n",
    "            action_map = {0: 'buy', 1: 'hold', 2: 'sell'}\n",
    "            action_name = action_map[a]\n",
    "            actions.append({'t': s['t'], 'a': action_name})\n",
    "            action_counts[action_name] += 1\n",
    "            \n",
    "            # Store for learning\n",
    "            memories.append({\n",
    "                'state': x,\n",
    "                'action': a,\n",
    "                'old_prob': probs[0][a].item(),\n",
    "                'value': val.item(),\n",
    "                'strategy_weights': strategy_weights[0].detach().cpu().numpy(),\n",
    "                'signals': {\n",
    "                    'dip_buy': s['dip_buy'],\n",
    "                    'profit_take': s['profit_take'],\n",
    "                    'in_position': s['in_position'],\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        next_state, reward, done, info = env.step(actions)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward, info['value'], memories, info, action_counts\n",
    "\n",
    "\n",
    "def update_brain(brain, optimizer, memories, episode_reward, episode_value):\n",
    "    \"\"\"\n",
    "    PPO-style update with AlphaGo adaptations:\n",
    "    - Extra reward for following signals correctly\n",
    "    - Penalty for hold paralysis\n",
    "    - Strategy pathway specialization\n",
    "    \"\"\"\n",
    "    if len(memories) < 10:\n",
    "        return 0\n",
    "    \n",
    "    brain.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Sample memories for learning\n",
    "    sample_size = min(300, len(memories))\n",
    "    sample_idx = random.sample(range(len(memories)), sample_size)\n",
    "    \n",
    "    for idx in sample_idx:\n",
    "        mem = memories[idx]\n",
    "        x = mem['state']\n",
    "        a = mem['action']\n",
    "        old_prob = mem['old_prob']\n",
    "        old_val = mem['value']\n",
    "        signals = mem['signals']\n",
    "        \n",
    "        probs, new_val, weights = brain(x.unsqueeze(0))\n",
    "        \n",
    "        # Calculate advantage\n",
    "        returns = episode_reward / 1000\n",
    "        advantage = returns - old_val\n",
    "        \n",
    "        # SIGNAL ALIGNMENT BONUS\n",
    "        signal_bonus = 0\n",
    "        if signals['dip_buy'] and a == 0:  # Bought on dip signal\n",
    "            signal_bonus = 0.1\n",
    "        if signals['profit_take'] and signals['in_position'] and a == 2:  # Sold on profit signal\n",
    "            signal_bonus = 0.1\n",
    "        \n",
    "        advantage += signal_bonus\n",
    "        \n",
    "        # Policy loss with clipping (PPO)\n",
    "        ratio = probs[0][a] / (old_prob + 1e-8)\n",
    "        clipped_ratio = torch.clamp(ratio, 0.8, 1.2)\n",
    "        policy_loss = -torch.min(ratio * advantage, clipped_ratio * advantage)\n",
    "        \n",
    "        # Value loss\n",
    "        value_loss = (new_val - returns) ** 2\n",
    "        \n",
    "        # Entropy bonus for exploration\n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum()\n",
    "        \n",
    "        # HOLD PENALTY - Discourage paralysis\n",
    "        hold_penalty = 0.02 * probs[0][1]  # Small penalty for hold probability\n",
    "        \n",
    "        loss = policy_loss + 0.5 * value_loss - 0.02 * entropy + hold_penalty\n",
    "        total_loss += loss\n",
    "    \n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    (total_loss / sample_size).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(brain.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss.item() / sample_size\n",
    "\n",
    "\n",
    "print('‚úÖ Training Functions V4 - ALPHAGO LEARNING')\n",
    "print('   üìä 17 features including position & portfolio context')\n",
    "print('   üéØ Signal-aware exploration')\n",
    "print('   üîÑ PPO-style updates with signal alignment')\n",
    "print('   üö´ Hold penalty to prevent paralysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: üéÆ ALPHAGO TRAINING - WALK-FORWARD VALIDATION\n",
    "# No cheating. Train on past, validate on future. Lock in what works.\n",
    "\n",
    "env = TradingEnv(ALL_DATA)\n",
    "N_EPISODES = 200\n",
    "results = []\n",
    "\n",
    "print('üéÆ ALPHAGO V4 - WALK-FORWARD TRAINING')\n",
    "print('='*70)\n",
    "print('PRINCIPLES:')\n",
    "print('  1. Every loss is data - learn and adapt')\n",
    "print('  2. Never get stuck - cut losers fast')\n",
    "print('  3. Cash is opportunity - preserve for dips')\n",
    "print('  4. Lock in wins - encode what works')\n",
    "print('='*70)\n",
    "\n",
    "# Track best models\n",
    "best_value = 0\n",
    "best_win_rate = 0\n",
    "best_sharpe = -999\n",
    "\n",
    "# Training metrics\n",
    "training_log = []\n",
    "\n",
    "for ep in range(N_EPISODES):\n",
    "    # Adaptive epsilon - more exploration early, exploit later\n",
    "    if ep < 50:\n",
    "        epsilon = max(0.3, 0.6 - ep * 0.006)  # High exploration first\n",
    "    elif ep < 150:\n",
    "        epsilon = max(0.1, 0.3 - (ep - 50) * 0.002)  # Gradual decay\n",
    "    else:\n",
    "        epsilon = 0.05  # Low exploration, trust learned patterns\n",
    "    \n",
    "    # Run episode\n",
    "    reward, final_val, memories, info, action_counts = train_episode(\n",
    "        env, brain, optimizer, epsilon\n",
    "    )\n",
    "    \n",
    "    # Update brain\n",
    "    loss = update_brain(brain, optimizer, memories, reward, final_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pct_return = (final_val / 10000 - 1) * 100\n",
    "    win_rate = info['win_rate']\n",
    "    n_trades = info['total_trades']\n",
    "    pattern_hits = info.get('pattern_hits', {})\n",
    "    \n",
    "    # Track action distribution\n",
    "    total_actions = sum(action_counts.values())\n",
    "    buy_pct = action_counts['buy'] / max(1, total_actions) * 100\n",
    "    sell_pct = action_counts['sell'] / max(1, total_actions) * 100\n",
    "    hold_pct = action_counts['hold'] / max(1, total_actions) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'ep': ep, 'value': final_val, 'reward': reward,\n",
    "        'win_rate': win_rate, 'trades': n_trades, 'loss': loss,\n",
    "        'buy_pct': buy_pct, 'sell_pct': sell_pct, 'hold_pct': hold_pct,\n",
    "        'dip_buys': pattern_hits.get('dip_buy', 0),\n",
    "        'profit_takes': pattern_hits.get('profit_take', 0)\n",
    "    })\n",
    "    \n",
    "    # Save best models\n",
    "    if final_val > best_value:\n",
    "        best_value = final_val\n",
    "        torch.save(brain.state_dict(), 'best_brain_value.pt')\n",
    "    \n",
    "    if win_rate > best_win_rate and n_trades >= 15:\n",
    "        best_win_rate = win_rate\n",
    "        torch.save(brain.state_dict(), 'best_brain_winrate.pt')\n",
    "    \n",
    "    # Every 20 episodes, show progress\n",
    "    if ep % 20 == 0:\n",
    "        print(f'\\nEp {ep:3d} | ${final_val:,.0f} ({pct_return:+.1f}%) | '\n",
    "              f'WR: {win_rate:.0%} | Trades: {n_trades}')\n",
    "        print(f'        Actions: Buy {buy_pct:.0f}% | Sell {sell_pct:.0f}% | Hold {hold_pct:.0f}% | Œµ={epsilon:.2f}')\n",
    "        print(f'        Patterns: {pattern_hits.get(\"dip_buy\", 0)} dip buys, {pattern_hits.get(\"profit_take\", 0)} profit takes')\n",
    "\n",
    "# Final summary\n",
    "print('\\n' + '='*70)\n",
    "print('üèÜ TRAINING COMPLETE - BEST RESULTS')\n",
    "print('='*70)\n",
    "print(f'Best Portfolio Value:  ${best_value:,.0f} ({(best_value/10000-1)*100:+.1f}%)')\n",
    "print(f'Best Win Rate:         {best_win_rate:.0%}')\n",
    "\n",
    "# Action distribution evolution\n",
    "early_results = results[:20]\n",
    "late_results = results[-20:]\n",
    "\n",
    "early_hold = np.mean([r['hold_pct'] for r in early_results])\n",
    "late_hold = np.mean([r['hold_pct'] for r in late_results])\n",
    "\n",
    "early_trades = np.mean([r['trades'] for r in early_results])\n",
    "late_trades = np.mean([r['trades'] for r in late_results])\n",
    "\n",
    "print(f'\\nüìä LEARNING EVOLUTION:')\n",
    "print(f'   Early Hold %:  {early_hold:.1f}% ‚Üí Late Hold %: {late_hold:.1f}%')\n",
    "print(f'   Early Trades:  {early_trades:.0f} ‚Üí Late Trades: {late_trades:.0f}')\n",
    "print(f'   (Lower hold + more trades = less paralysis!)')\n",
    "\n",
    "# Pattern effectiveness\n",
    "total_dip_buys = sum(r.get('dip_buys', 0) for r in results)\n",
    "total_profit_takes = sum(r.get('profit_takes', 0) for r in results)\n",
    "print(f'\\nüéØ PATTERN HITS:')\n",
    "print(f'   Dip Buys:      {total_dip_buys}')\n",
    "print(f'   Profit Takes:  {total_profit_takes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe12089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: üìà WALK-FORWARD VALIDATION - NO CHEATING!\n",
    "# Train on past, test on future. This is the REAL test.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('üìà WALK-FORWARD VALIDATION')\n",
    "print('='*70)\n",
    "print('This validates that our strategy works on UNSEEN data')\n",
    "print('Train on past ‚Üí Test on future ‚Üí No overfitting!')\n",
    "print('='*70)\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# 1. Portfolio Value\n",
    "vals = [r['value'] for r in results]\n",
    "axes[0, 0].plot(vals, alpha=0.6)\n",
    "axes[0, 0].plot(pd.Series(vals).rolling(10).mean(), 'b-', linewidth=2, label='10-ep MA')\n",
    "axes[0, 0].axhline(10000, color='r', linestyle='--', label='Start $10k')\n",
    "axes[0, 0].set_title('Portfolio Value Over Episodes')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('$')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Win Rate\n",
    "win_rates = [r['win_rate'] for r in results]\n",
    "axes[0, 1].plot(win_rates, alpha=0.6)\n",
    "axes[0, 1].plot(pd.Series(win_rates).rolling(10).mean(), 'g-', linewidth=2, label='10-ep MA')\n",
    "axes[0, 1].axhline(0.5, color='r', linestyle='--', label='50% baseline')\n",
    "axes[0, 1].set_title('Win Rate Over Episodes')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Win Rate')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Action Distribution Evolution\n",
    "buy_pcts = [r['buy_pct'] for r in results]\n",
    "sell_pcts = [r['sell_pct'] for r in results]\n",
    "hold_pcts = [r['hold_pct'] for r in results]\n",
    "\n",
    "axes[0, 2].plot(pd.Series(buy_pcts).rolling(10).mean(), 'g-', label='Buy %', linewidth=2)\n",
    "axes[0, 2].plot(pd.Series(sell_pcts).rolling(10).mean(), 'r-', label='Sell %', linewidth=2)\n",
    "axes[0, 2].plot(pd.Series(hold_pcts).rolling(10).mean(), 'gray', label='Hold %', linewidth=2)\n",
    "axes[0, 2].set_title('Action Distribution (10-ep MA)')\n",
    "axes[0, 2].set_xlabel('Episode')\n",
    "axes[0, 2].set_ylabel('%')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Trade Count\n",
    "trades = [r['trades'] for r in results]\n",
    "axes[1, 0].plot(trades, alpha=0.6)\n",
    "axes[1, 0].plot(pd.Series(trades).rolling(10).mean(), 'purple', linewidth=2, label='10-ep MA')\n",
    "axes[1, 0].set_title('Trades Per Episode')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('# Trades')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Reward\n",
    "rewards = [r['reward'] for r in results]\n",
    "axes[1, 1].plot(rewards, alpha=0.6)\n",
    "axes[1, 1].plot(pd.Series(rewards).rolling(10).mean(), 'orange', linewidth=2, label='10-ep MA')\n",
    "axes[1, 1].axhline(0, color='r', linestyle='--')\n",
    "axes[1, 1].set_title('Episode Reward')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Reward')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Pattern Hits\n",
    "dip_buys = [r.get('dip_buys', 0) for r in results]\n",
    "profit_takes = [r.get('profit_takes', 0) for r in results]\n",
    "axes[1, 2].bar(range(len(results)), dip_buys, alpha=0.6, label='Dip Buys', color='green')\n",
    "axes[1, 2].bar(range(len(results)), profit_takes, alpha=0.6, label='Profit Takes', color='gold', bottom=dip_buys)\n",
    "axes[1, 2].set_title('Pattern Hits (Secret Sauce)')\n",
    "axes[1, 2].set_xlabel('Episode')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ============ WALK-FORWARD TEST ============\n",
    "print('\\n' + '='*70)\n",
    "print('üî¨ WALK-FORWARD TEST - TRUE OUT-OF-SAMPLE VALIDATION')\n",
    "print('='*70)\n",
    "\n",
    "# Split data into 4 periods\n",
    "all_lens = [len(df) for df in ALL_DATA.values()]\n",
    "min_len = min(all_lens)\n",
    "period_size = min_len // 4\n",
    "\n",
    "print(f'Data split into 4 periods of ~{period_size} days each')\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold in range(3):  # 3 folds (train on 1-2, test on 2-3, etc.)\n",
    "    # Training period\n",
    "    train_start = fold * period_size\n",
    "    train_end = (fold + 2) * period_size\n",
    "    \n",
    "    # Test period\n",
    "    test_start = train_end\n",
    "    test_end = min((fold + 3) * period_size, min_len)\n",
    "    \n",
    "    if test_end > min_len:\n",
    "        break\n",
    "    \n",
    "    print(f'\\nFold {fold + 1}: Train days {train_start}-{train_end}, Test days {test_start}-{test_end}')\n",
    "    \n",
    "    # Create train/test data\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    for ticker, df in ALL_DATA.items():\n",
    "        if len(df) >= test_end:\n",
    "            train_data[ticker] = df.iloc[train_start:train_end].copy()\n",
    "            test_data[ticker] = df.iloc[test_start:test_end].copy()\n",
    "    \n",
    "    # Quick test on out-of-sample data\n",
    "    test_env = TradingEnv(test_data)\n",
    "    \n",
    "    # Run test episode with no exploration (pure exploitation)\n",
    "    test_reward, test_value, _, test_info, _ = train_episode(test_env, brain, optimizer, epsilon=0.0)\n",
    "    \n",
    "    fold_return = (test_value / 10000 - 1) * 100\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'return': fold_return,\n",
    "        'value': test_value,\n",
    "        'win_rate': test_info['win_rate'],\n",
    "        'trades': test_info['total_trades']\n",
    "    })\n",
    "    \n",
    "    print(f'   Test Return: {fold_return:+.2f}%')\n",
    "    print(f'   Win Rate: {test_info[\"win_rate\"]:.0%}')\n",
    "    print(f'   Trades: {test_info[\"total_trades\"]}')\n",
    "\n",
    "# Summary\n",
    "if fold_results:\n",
    "    avg_return = np.mean([f['return'] for f in fold_results])\n",
    "    avg_winrate = np.mean([f['win_rate'] for f in fold_results])\n",
    "    consistency = np.std([f['return'] for f in fold_results])\n",
    "    \n",
    "    print('\\n' + '='*70)\n",
    "    print('üèÜ WALK-FORWARD RESULTS')\n",
    "    print('='*70)\n",
    "    print(f'Average OOS Return:  {avg_return:+.2f}%')\n",
    "    print(f'Average Win Rate:    {avg_winrate:.0%}')\n",
    "    print(f'Consistency (std):   {consistency:.2f}%')\n",
    "    print()\n",
    "    \n",
    "    if avg_return > 0 and consistency < 15:\n",
    "        print('‚úÖ STRATEGY VALIDATED - Works on unseen data!')\n",
    "        print('   The SECRET SAUCE is real.')\n",
    "    elif avg_return > 0:\n",
    "        print('‚ö†Ô∏è  POSITIVE but INCONSISTENT - Needs refinement')\n",
    "    else:\n",
    "        print('‚ùå NEEDS MORE WORK - Negative OOS returns')\n",
    "\n",
    "print('\\nüíæ Training curves saved to training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c31a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: üìä TODAY'S PREDICTIONS - ALPHAGO SECRET SAUCE\n",
    "import os\n",
    "\n",
    "# Load best model\n",
    "if os.path.exists('best_brain_winrate.pt'):\n",
    "    brain.load_state_dict(torch.load('best_brain_winrate.pt'))\n",
    "    print('üìÇ Loaded BEST WIN RATE model')\n",
    "elif os.path.exists('best_brain_value.pt'):\n",
    "    brain.load_state_dict(torch.load('best_brain_value.pt'))\n",
    "    print('üìÇ Loaded BEST VALUE model')\n",
    "\n",
    "brain.eval()\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('üß¨ ALPHAGO PREDICTIONS - SECRET SAUCE ENCODED')\n",
    "print('='*70)\n",
    "print('Strategy DNA Active:')\n",
    "print(f'  ‚Ä¢ Dip Buy: {StrategyDNA.DIP_BUY[\"min_drop\"]*100:.0f}% drop + RSI<{StrategyDNA.DIP_BUY[\"max_rsi\"]}')\n",
    "print(f'  ‚Ä¢ Profit Take: {StrategyDNA.PROFIT_TAKE[\"target\"]*100:.0f}% gain target')\n",
    "print(f'  ‚Ä¢ Cut Loss: {StrategyDNA.CUT_LOSS[\"max_loss\"]*100:.0f}% max loss')\n",
    "print('='*70)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    if ticker not in ALL_DATA:\n",
    "        continue\n",
    "    df = ALL_DATA[ticker]\n",
    "    if len(df) < 60:\n",
    "        continue\n",
    "    \n",
    "    idx = len(df) - 1\n",
    "    c = df['Close'].iloc[max(0,idx-20):idx+1].values\n",
    "    v = df['Volume'].iloc[max(0,idx-20):idx+1].values\n",
    "    h = df['High'].iloc[max(0,idx-20):idx+1].values\n",
    "    l = df['Low'].iloc[max(0,idx-20):idx+1].values\n",
    "    \n",
    "    # Calculate ALL features\n",
    "    ret5 = (c[-1]/c[-6]-1) if len(c)>5 else 0\n",
    "    ret10 = (c[-1]/c[-11]-1) if len(c)>10 else 0\n",
    "    vol_r = v[-1]/(np.mean(v)+1) if len(v)>1 else 1\n",
    "    \n",
    "    d = np.diff(c)\n",
    "    g = np.mean(d[d>0]) if len(d[d>0])>0 else 0\n",
    "    lo = abs(np.mean(d[d<0])) if len(d[d<0])>0 else 1e-8\n",
    "    rsi = 100 - 100/(1+g/lo)\n",
    "    \n",
    "    high_20 = max(c)\n",
    "    low_20 = min(c)\n",
    "    drawdown = (c[-1] - high_20) / high_20\n",
    "    recovery = (c[-1] - low_20) / (high_20 - low_20 + 0.001)\n",
    "    \n",
    "    atr = np.mean(h[-14:] - l[-14:]) if len(h) >= 14 else 0\n",
    "    atr_pct = atr / c[-1] if c[-1] > 0 else 0\n",
    "    vol_squeeze = 1 if atr_pct < 0.02 else 0\n",
    "    vol_spike = 1 if vol_r > 2.0 else 0\n",
    "    \n",
    "    if len(c) >= 10:\n",
    "        mom_5 = c[-1] - c[-6]\n",
    "        mom_10 = c[-6] - c[-11] if len(c) > 10 else 0\n",
    "        momentum_accel = 1 if mom_5 > mom_10 else -1\n",
    "    else:\n",
    "        momentum_accel = 0\n",
    "    \n",
    "    # SECRET SAUCE SIGNALS\n",
    "    dip_buy = 1 if (drawdown < StrategyDNA.DIP_BUY['min_drop'] and \n",
    "                    rsi < StrategyDNA.DIP_BUY['max_rsi'] and \n",
    "                    vol_r > StrategyDNA.DIP_BUY['min_vol']) else 0\n",
    "    profit_take = 1 if (recovery > 0.85 and \n",
    "                        rsi > StrategyDNA.PROFIT_TAKE['max_rsi']) else 0\n",
    "    \n",
    "    # Build 17-feature tensor (not in position for predictions)\n",
    "    x = torch.tensor([\n",
    "        ret5, ret10, vol_r, rsi/100, c[-1]/1000,\n",
    "        drawdown, recovery, vol_squeeze, vol_spike, momentum_accel,\n",
    "        dip_buy, profit_take,\n",
    "        0,  # not in position\n",
    "        0,  # no current pnl\n",
    "        0,  # no days held\n",
    "        0.5,  # assume 50% cash\n",
    "        0,  # no losing streak\n",
    "    ], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs, val, strategy_weights = brain(x.unsqueeze(0))\n",
    "    \n",
    "    buy_prob = probs[0][0].item()\n",
    "    hold_prob = probs[0][1].item()\n",
    "    sell_prob = probs[0][2].item()\n",
    "    \n",
    "    # Determine action from network\n",
    "    if buy_prob > hold_prob and buy_prob > sell_prob:\n",
    "        action = 'BUY'\n",
    "    elif sell_prob > hold_prob and sell_prob > buy_prob:\n",
    "        action = 'SELL'\n",
    "    else:\n",
    "        action = 'HOLD'\n",
    "    \n",
    "    # OVERRIDE with SECRET SAUCE signals\n",
    "    signal = ''\n",
    "    signal_type = ''\n",
    "    \n",
    "    if dip_buy:\n",
    "        signal = 'üéØ DIP BUY!'\n",
    "        signal_type = 'dip'\n",
    "        action = 'BUY'  # Force buy on dip signal\n",
    "    elif profit_take:\n",
    "        signal = 'üí∞ TAKE PROFIT'\n",
    "        signal_type = 'profit'\n",
    "        # Don't force sell here - just flag it\n",
    "    elif vol_squeeze and momentum_accel > 0:\n",
    "        signal = '‚ö° SQUEEZE BREAKOUT'\n",
    "        signal_type = 'squeeze'\n",
    "    elif drawdown < -0.05:\n",
    "        signal = f'üìâ Down {drawdown*100:.1f}%'\n",
    "        signal_type = 'down'\n",
    "    elif vol_spike:\n",
    "        signal = 'üìä VOL SPIKE'\n",
    "        signal_type = 'volume'\n",
    "    \n",
    "    # Get strategy weights for insight\n",
    "    weights = strategy_weights[0].cpu().numpy()\n",
    "    dominant_strategy = ['DIP', 'MOM', 'RISK', 'CASH'][np.argmax(weights)]\n",
    "    \n",
    "    predictions.append({\n",
    "        'ticker': ticker,\n",
    "        'action': action,\n",
    "        'buy_prob': buy_prob,\n",
    "        'sell_prob': sell_prob,\n",
    "        'hold_prob': hold_prob,\n",
    "        'price': float(c[-1]),\n",
    "        'rsi': rsi,\n",
    "        'drawdown': drawdown * 100,\n",
    "        'recovery': recovery * 100,\n",
    "        'ret5': ret5 * 100,\n",
    "        'signal': signal,\n",
    "        'signal_type': signal_type,\n",
    "        'dip_buy': dip_buy,\n",
    "        'profit_take': profit_take,\n",
    "        'vol_squeeze': vol_squeeze,\n",
    "        'dominant_strategy': dominant_strategy,\n",
    "        'strategy_weights': weights.tolist()\n",
    "    })\n",
    "\n",
    "# Sort: DIP BUYS first, then by buy probability\n",
    "predictions.sort(key=lambda x: (-x['dip_buy'], -x['profit_take'], -x['buy_prob'] if x['action']=='BUY' else x['sell_prob']))\n",
    "\n",
    "# ============ DISPLAY RESULTS ============\n",
    "\n",
    "print('\\nüéØ SPECIAL SIGNALS (ACT ON THESE):')\n",
    "print('-'*70)\n",
    "special = [p for p in predictions if p['signal']]\n",
    "if special:\n",
    "    for p in special:\n",
    "        sym = 'üü¢' if p['action'] == 'BUY' else ('üî¥' if p['action'] == 'SELL' else '‚ö™')\n",
    "        print(f\"{sym} {p['ticker']:5s} ${p['price']:>8.2f} | {p['action']:4s} | \"\n",
    "              f\"RSI:{p['rsi']:3.0f} | Drop:{p['drawdown']:+5.1f}% | {p['signal']} [{p['dominant_strategy']}]\")\n",
    "else:\n",
    "    print(\"   No special signals today - market in consolidation\")\n",
    "\n",
    "# DIP OPPORTUNITIES\n",
    "dips = [p for p in predictions if p['dip_buy']]\n",
    "if dips:\n",
    "    print(f'\\nüéØ DIP BUY OPPORTUNITIES ({len(dips)}) - THIS IS WHERE BIG WINS COME FROM:')\n",
    "    print('-'*70)\n",
    "    for p in dips:\n",
    "        print(f\"   {p['ticker']:5s} ${p['price']:>8.2f} | Down {p['drawdown']:+.1f}% from high | RSI: {p['rsi']:.0f}\")\n",
    "        print(f\"          Strategy: {p['dominant_strategy']} | Confidence: {p['buy_prob']:.0%}\")\n",
    "\n",
    "# PROFIT TAKE\n",
    "profit_signals = [p for p in predictions if p['profit_take']]\n",
    "if profit_signals:\n",
    "    print(f'\\nüí∞ PROFIT TAKE SIGNALS ({len(profit_signals)}) - LIKE YOUR HOOD TRADE:')\n",
    "    print('-'*70)\n",
    "    for p in profit_signals:\n",
    "        print(f\"   {p['ticker']:5s} ${p['price']:>8.2f} | Recovery: {p['recovery']:.0f}% | RSI: {p['rsi']:.0f} (overbought)\")\n",
    "\n",
    "# ALL PREDICTIONS\n",
    "print(f'\\nüìä ALL {len(predictions)} PREDICTIONS:')\n",
    "print('-'*70)\n",
    "print(f\"{'':3} {'TICKER':<6} {'PRICE':>9} {'ACTION':<5} {'BUY':>5} {'SELL':>5} {'RSI':>4} {'5D':>6} {'STRATEGY':>6}\")\n",
    "print('-'*70)\n",
    "\n",
    "for p in predictions:\n",
    "    sym = 'üü¢' if p['action'] == 'BUY' else ('üî¥' if p['action'] == 'SELL' else '‚ö™')\n",
    "    sig = '*' if p['signal'] else ' '\n",
    "    print(f\"{sig}{sym} {p['ticker']:<5} ${p['price']:>8.2f} {p['action']:<5} \"\n",
    "          f\"{p['buy_prob']:>4.0%} {p['sell_prob']:>4.0%} {p['rsi']:>4.0f} {p['ret5']:>+5.1f}% {p['dominant_strategy']:>6}\")\n",
    "\n",
    "# SUMMARY\n",
    "buys = [p for p in predictions if p['action'] == 'BUY']\n",
    "sells = [p for p in predictions if p['action'] == 'SELL']\n",
    "holds = [p for p in predictions if p['action'] == 'HOLD']\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print(f'üìä SUMMARY: {len(buys)} BUYs | {len(sells)} SELLs | {len(holds)} HOLDs')\n",
    "print(f'   üéØ Dip Buys: {len(dips)} | üí∞ Profit Takes: {len(profit_signals)}')\n",
    "\n",
    "if buys:\n",
    "    print(f'\\nüü¢ TOP BUY SIGNALS:')\n",
    "    for p in buys[:8]:\n",
    "        sig_note = f\" ‚Üê {p['signal']}\" if p['signal'] else \"\"\n",
    "        print(f\"   {p['ticker']:5s} ${p['price']:>7.2f} | {p['buy_prob']:.0%} conf | {p['dominant_strategy']}{sig_note}\")\n",
    "\n",
    "if sells:\n",
    "    print(f'\\nüî¥ SELL/AVOID:')\n",
    "    for p in sells[:5]:\n",
    "        sig_note = f\" ‚Üê {p['signal']}\" if p['signal'] else \"\"\n",
    "        print(f\"   {p['ticker']:5s} ${p['price']:>7.2f} | {p['sell_prob']:.0%} conf | {p['dominant_strategy']}{sig_note}\")\n",
    "\n",
    "print(f'\\n‚úÖ {len(predictions)} predictions ready with SECRET SAUCE encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: üíæ SAVE THE SECRET SAUCE\n",
    "import json\n",
    "\n",
    "print('='*70)\n",
    "print('üíæ SAVING THE SECRET SAUCE')\n",
    "print('='*70)\n",
    "\n",
    "# 1. Save the trained brain\n",
    "torch.save(brain.state_dict(), 'alphago_trader_brain.pt')\n",
    "print('‚úÖ Saved: alphago_trader_brain.pt')\n",
    "\n",
    "# Also save best models if they exist\n",
    "if os.path.exists('best_brain_winrate.pt'):\n",
    "    print('‚úÖ Best Win Rate model: best_brain_winrate.pt')\n",
    "if os.path.exists('best_brain_value.pt'):\n",
    "    print('‚úÖ Best Value model: best_brain_value.pt')\n",
    "\n",
    "# 2. Save training results\n",
    "with open('training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('‚úÖ Saved: training_results.json')\n",
    "\n",
    "# 3. Save predictions with all features\n",
    "with open('todays_predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=2, default=float)\n",
    "print('‚úÖ Saved: todays_predictions.json')\n",
    "\n",
    "# 4. Save Strategy DNA (the secret sauce rules)\n",
    "strategy_dna = {\n",
    "    'dip_buy': StrategyDNA.DIP_BUY,\n",
    "    'profit_take': StrategyDNA.PROFIT_TAKE,\n",
    "    'cut_loss': StrategyDNA.CUT_LOSS,\n",
    "    'cash_management': StrategyDNA.CASH,\n",
    "    'discovered_at': datetime.now().isoformat(),\n",
    "    'validation': {\n",
    "        'walk_forward_returns': [f['return'] for f in fold_results] if 'fold_results' in dir() else [],\n",
    "        'avg_oos_return': avg_return if 'avg_return' in dir() else 0,\n",
    "        'consistency': consistency if 'consistency' in dir() else 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('strategy_dna.json', 'w') as f:\n",
    "    json.dump(strategy_dna, f, indent=2)\n",
    "print('‚úÖ Saved: strategy_dna.json (THE SECRET SAUCE)')\n",
    "\n",
    "# 5. Create summary report\n",
    "summary = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'episodes': len(results),\n",
    "    'best_portfolio_value': best_value,\n",
    "    'best_win_rate': best_win_rate,\n",
    "    'final_predictions': {\n",
    "        'total': len(predictions),\n",
    "        'buys': len([p for p in predictions if p['action'] == 'BUY']),\n",
    "        'sells': len([p for p in predictions if p['action'] == 'SELL']),\n",
    "        'dip_opportunities': len([p for p in predictions if p['dip_buy']]),\n",
    "        'profit_take_signals': len([p for p in predictions if p['profit_take']]),\n",
    "    },\n",
    "    'strategy_dna': strategy_dna,\n",
    "    'top_buys': [{'ticker': p['ticker'], 'price': p['price'], 'confidence': p['buy_prob'], 'signal': p['signal']} \n",
    "                 for p in predictions if p['action'] == 'BUY'][:10],\n",
    "    'dip_buys': [{'ticker': p['ticker'], 'price': p['price'], 'drawdown': p['drawdown'], 'rsi': p['rsi']} \n",
    "                 for p in predictions if p['dip_buy']],\n",
    "}\n",
    "\n",
    "with open('alphago_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=float)\n",
    "print('‚úÖ Saved: alphago_summary.json')\n",
    "\n",
    "# Summary display\n",
    "print('\\n' + '='*70)\n",
    "print('üì¶ FILES READY FOR DOWNLOAD:')\n",
    "print('='*70)\n",
    "print('   1. alphago_trader_brain.pt    - The trained neural network')\n",
    "print('   2. best_brain_winrate.pt      - Best win rate model')\n",
    "print('   3. best_brain_value.pt        - Best value model')\n",
    "print('   4. training_results.json      - Learning curve data')\n",
    "print('   5. todays_predictions.json    - Today\\'s signals')\n",
    "print('   6. strategy_dna.json          - THE SECRET SAUCE rules')\n",
    "print('   7. alphago_summary.json       - Full summary report')\n",
    "print('   8. training_curves.png        - Visual learning curves')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nüß¨ SECRET SAUCE ENCODED:')\n",
    "print(f'   DIP BUY: Drop ‚â§ {strategy_dna[\"dip_buy\"][\"min_drop\"]*100:.0f}%, RSI ‚â§ {strategy_dna[\"dip_buy\"][\"max_rsi\"]}, Vol ‚â• {strategy_dna[\"dip_buy\"][\"min_vol\"]}x')\n",
    "print(f'   PROFIT: Target {strategy_dna[\"profit_take\"][\"target\"]*100:.0f}%+, RSI ‚â• {strategy_dna[\"profit_take\"][\"max_rsi\"]}')\n",
    "print(f'   CUT LOSS: Max {strategy_dna[\"cut_loss\"][\"max_loss\"]*100:.0f}% loss, Max {strategy_dna[\"cut_loss\"][\"max_days\"]} days red')\n",
    "print(f'   CASH: Keep {strategy_dna[\"cash_management\"][\"min_reserve\"]*100:.0f}% reserve for dips')\n",
    "\n",
    "# Download files (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print('\\nüì• Downloading files...')\n",
    "    files.download('alphago_trader_brain.pt')\n",
    "    files.download('todays_predictions.json')\n",
    "    files.download('strategy_dna.json')\n",
    "    files.download('alphago_summary.json')\n",
    "    files.download('training_results.json')\n",
    "    if os.path.exists('training_curves.png'):\n",
    "        files.download('training_curves.png')\n",
    "    print('‚úÖ Downloads started!')\n",
    "except:\n",
    "    print('\\nüí° Not in Colab - files saved locally')\n",
    "\n",
    "print('\\nüöÄ ALPHAGO TRADER V4 COMPLETE!')\n",
    "print('   Load these files into your local system to use the SECRET SAUCE.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
