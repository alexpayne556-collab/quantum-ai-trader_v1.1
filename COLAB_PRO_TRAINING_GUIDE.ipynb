{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dada34e",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ef7b6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create directories\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p /content/drive/MyDrive/quantum_trader/models\n",
    "!mkdir -p /content/drive/MyDrive/quantum_trader/data\n",
    "!mkdir -p /content/drive/MyDrive/quantum_trader/logs\n",
    "\n",
    "print(\"âœ… Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178c353",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0041c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install TA-Lib yfinance pandas numpy scikit-learn joblib matplotlib seaborn plotly\n",
    "\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4863",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65271ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "TICKERS = ['MU', 'IONQ', 'APLD', 'ANNX', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'GOOGL', 'AMZN', \n",
    "           'META', 'AMD', 'AVGO', 'MRVL', 'CRM', 'DDOG', 'PLTR', 'COIN', 'JPM', 'XOM']\n",
    "TRAINING_PERIOD = '3y'  # 3 years for robust models\n",
    "FORWARD_DAYS = 7  # Predict 7-day returns\n",
    "DRIVE_PATH = '/content/drive/MyDrive/quantum_trader'\n",
    "\n",
    "print(f\"Training {len(TICKERS)} tickers with {TRAINING_PERIOD} lookback\")\n",
    "print(f\"Forward prediction window: {FORWARD_DAYS} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f04a6",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c97b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Create 50+ ML features from OHLCV data.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        feat = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Price arrays\n",
    "        close = df['Close'].values\n",
    "        high = df['High'].values\n",
    "        low = df['Low'].values\n",
    "        volume = df['Volume'].values\n",
    "        open_price = df['Open'].values\n",
    "        \n",
    "        # Momentum\n",
    "        feat['rsi_9'] = talib.RSI(close, timeperiod=9)\n",
    "        feat['rsi_14'] = talib.RSI(close, timeperiod=14)\n",
    "        feat['rsi_21'] = talib.RSI(close, timeperiod=21)\n",
    "        \n",
    "        # MACD\n",
    "        macd, macd_signal, macd_hist = talib.MACD(close, fastperiod=5, slowperiod=13, signalperiod=1)\n",
    "        feat['macd'] = macd\n",
    "        feat['macd_signal'] = macd_signal\n",
    "        feat['macd_hist'] = macd_hist\n",
    "        \n",
    "        # Stochastics\n",
    "        slowk, slowd = talib.STOCH(high, low, close, fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "        feat['stoch_k'] = slowk\n",
    "        feat['stoch_d'] = slowd\n",
    "        \n",
    "        # Trend\n",
    "        feat['adx'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "        feat['cci'] = talib.CCI(high, low, close, timeperiod=14)\n",
    "        feat['aroon_up'], feat['aroon_down'] = talib.AROON(high, low, timeperiod=14)\n",
    "        \n",
    "        # Volatility\n",
    "        feat['atr'] = talib.ATR(high, low, close, timeperiod=14)\n",
    "        feat['natr'] = talib.NATR(high, low, close, timeperiod=14)\n",
    "        feat['bb_upper'], feat['bb_middle'], feat['bb_lower'] = talib.BBANDS(close, timeperiod=20)\n",
    "        feat['bb_pct'] = (close - feat['bb_lower']) / (feat['bb_upper'] - feat['bb_lower'])\n",
    "        \n",
    "        # Moving averages\n",
    "        for period in [5, 8, 13, 21, 34, 55, 89]:\n",
    "            feat[f'ema_{period}'] = talib.EMA(close, timeperiod=period)\n",
    "            feat[f'sma_{period}'] = talib.SMA(close, timeperiod=period)\n",
    "        \n",
    "        # Price vs EMAs\n",
    "        feat['price_vs_ema5'] = (close - feat['ema_5']) / close\n",
    "        feat['price_vs_ema13'] = (close - feat['ema_13']) / close\n",
    "        feat['price_vs_ema21'] = (close - feat['ema_21']) / close\n",
    "        \n",
    "        # Volume\n",
    "        feat['obv'] = talib.OBV(close, volume)\n",
    "        feat['ad'] = talib.AD(high, low, close, volume)\n",
    "        feat['adosc'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)\n",
    "        feat['vol_sma_20'] = talib.SMA(volume, timeperiod=20)\n",
    "        feat['vol_ratio'] = volume / feat['vol_sma_20']\n",
    "        \n",
    "        # Patterns\n",
    "        feat['doji'] = talib.CDLDOJI(open_price, high, low, close)\n",
    "        feat['engulfing'] = talib.CDLENGULFING(open_price, high, low, close)\n",
    "        feat['hammer'] = talib.CDLHAMMER(open_price, high, low, close)\n",
    "        feat['shooting_star'] = talib.CDLSHOOTINGSTAR(open_price, high, low, close)\n",
    "        \n",
    "        # Returns\n",
    "        feat['returns_1d'] = close / np.roll(close, 1) - 1\n",
    "        feat['returns_3d'] = close / np.roll(close, 3) - 1\n",
    "        feat['returns_7d'] = close / np.roll(close, 7) - 1\n",
    "        \n",
    "        return feat.dropna()\n",
    "\n",
    "print(\"âœ… FeatureEngineer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30bea8",
   "metadata": {},
   "source": [
    "## Step 5: Label Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0987492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df: pd.DataFrame, forward_days: int = 7) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Labels: 0=BEARISH (< -2%), 1=NEUTRAL (-2% to +2%), 2=BULLISH (> +2%)\n",
    "    \"\"\"\n",
    "    close = df['Close'].values\n",
    "    future_returns = (np.roll(close, -forward_days) / close - 1) * 100\n",
    "    \n",
    "    labels = np.where(future_returns < -2, 0, np.where(future_returns > 2, 2, 1))\n",
    "    return pd.Series(labels, index=df.index)\n",
    "\n",
    "print(\"âœ… Label function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07660e82",
   "metadata": {},
   "source": [
    "## Step 6: Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ticker_model(ticker: str, period: str = '3y'):\n",
    "    \"\"\"\n",
    "    Full training pipeline:\n",
    "    1. Fetch data from yfinance\n",
    "    2. Engineer features\n",
    "    3. Walk-forward cross-validation\n",
    "    4. Train final calibrated model\n",
    "    5. Save to Google Drive\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {ticker}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Fetch data\n",
    "    df = yf.download(ticker, period=period, interval='1d', progress=False, auto_adjust=True)\n",
    "    \n",
    "    if len(df) < 300:\n",
    "        print(f\"âš ï¸  Insufficient data for {ticker} ({len(df)} rows)\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Data: {len(df)} rows from {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    fe = FeatureEngineer()\n",
    "    X = fe.engineer(df)\n",
    "    y = create_labels(df, forward_days=FORWARD_DAYS)\n",
    "    \n",
    "    # Align\n",
    "    common_idx = X.index.intersection(y.index)\n",
    "    X = X.loc[common_idx]\n",
    "    y = y.loc[common_idx]\n",
    "    \n",
    "    print(f\"Features: {X.shape[1]}, Samples: {len(X)}\")\n",
    "    print(f\"Label distribution: {y.value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    # Walk-forward validation (5 folds)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train base model\n",
    "        base_model = HistGradientBoostingClassifier(\n",
    "            max_iter=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=20,\n",
    "            l2_regularization=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Calibrate probabilities\n",
    "        calibrated = CalibratedClassifierCV(base_model, method='sigmoid', cv=3)\n",
    "        calibrated.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = calibrated.predict(X_test_scaled)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        fold_scores.append(acc)\n",
    "        \n",
    "        print(f\"  Fold {fold}: Accuracy = {acc:.3f} (Train: {len(X_train)}, Test: {len(X_test)})\")\n",
    "    \n",
    "    avg_acc = np.mean(fold_scores)\n",
    "    std_acc = np.std(fold_scores)\n",
    "    print(f\"\\nâœ… Walk-forward validation: {avg_acc:.3f} Â± {std_acc:.3f}\")\n",
    "    \n",
    "    # Train final model on ALL data\n",
    "    scaler_final = StandardScaler()\n",
    "    X_scaled = scaler_final.fit_transform(X)\n",
    "    \n",
    "    final_base = HistGradientBoostingClassifier(\n",
    "        max_iter=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    final_model = CalibratedClassifierCV(final_base, method='sigmoid', cv=3)\n",
    "    final_model.fit(X_scaled, y)\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(final_model.estimators_[0], 'feature_importances_'):\n",
    "        importances = final_model.estimators_[0].feature_importances_\n",
    "        top_features = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print(\"\\nTop 10 features:\")\n",
    "        for feat, imp in top_features:\n",
    "            print(f\"  {feat:<20}: {imp:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_data = {\n",
    "        'model': final_model,\n",
    "        'scaler': scaler_final,\n",
    "        'feature_columns': X.columns.tolist(),\n",
    "        'accuracy_mean': avg_acc,\n",
    "        'accuracy_std': std_acc,\n",
    "        'ticker': ticker,\n",
    "        'training_samples': len(X),\n",
    "        'training_period': period,\n",
    "        'forward_days': FORWARD_DAYS,\n",
    "        'trained_datetime': datetime.now().isoformat(),\n",
    "        'label_distribution': y.value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    model_path = f\"{DRIVE_PATH}/models/{ticker}_tuned.pkl\"\n",
    "    joblib.dump(model_data, model_path)\n",
    "    print(f\"ðŸ’¾ Model saved: {model_path}\")\n",
    "    \n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'accuracy': avg_acc,\n",
    "        'std': std_acc,\n",
    "        'samples': len(X),\n",
    "        'features': X.shape[1]\n",
    "    }\n",
    "\n",
    "print(\"âœ… Training pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce91e3",
   "metadata": {},
   "source": [
    "## Step 7: Train All Models (RUN THIS CELL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2240dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all tickers\n",
    "results = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        result = train_ticker_model(ticker, period=TRAINING_PERIOD)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error training {ticker}: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage accuracy: {results_df['accuracy'].mean():.3f}\")\n",
    "print(f\"Best model: {results_df.iloc[0]['ticker']} ({results_df.iloc[0]['accuracy']:.3f})\")\n",
    "print(f\"\\nðŸ’¾ All models saved to: {DRIVE_PATH}/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419761c1",
   "metadata": {},
   "source": [
    "## Step 8: Download Models to Local System\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. After training completes, navigate to Google Drive:\n",
    "   - Go to `My Drive â†’ quantum_trader â†’ models/`\n",
    "   - You'll see all `.pkl` files (e.g., `MU_tuned.pkl`, `IONQ_tuned.pkl`, etc.)\n",
    "\n",
    "2. Download all `.pkl` files to your local system:\n",
    "   - Select all `.pkl` files\n",
    "   - Right-click â†’ Download\n",
    "   - Save to: `E:\\quantum-ai-trader-v1.1\\models\\`\n",
    "\n",
    "3. Verify local models:\n",
    "   ```powershell\n",
    "   ls E:\\quantum-ai-trader-v1.1\\models\\*.pkl\n",
    "   ```\n",
    "\n",
    "4. Start local orchestrator:\n",
    "   ```powershell\n",
    "   cd E:\\quantum-ai-trader-v1.1\n",
    "   python trading_orchestrator.py\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79368f12",
   "metadata": {},
   "source": [
    "## Step 9: Backtest Performance (Optional)\n",
    "\n",
    "Test models on recent unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7005b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_model(ticker: str):\n",
    "    \"\"\"Test model on last 60 days (unseen data).\"\"\"\n",
    "    model_path = f\"{DRIVE_PATH}/models/{ticker}_tuned.pkl\"\n",
    "    model_data = joblib.load(model_path)\n",
    "    \n",
    "    model = model_data['model']\n",
    "    scaler = model_data['scaler']\n",
    "    \n",
    "    # Fetch recent data\n",
    "    df = yf.download(ticker, period='90d', interval='1d', progress=False, auto_adjust=True)\n",
    "    \n",
    "    fe = FeatureEngineer()\n",
    "    X = fe.engineer(df)\n",
    "    y = create_labels(df, forward_days=FORWARD_DAYS)\n",
    "    \n",
    "    common_idx = X.index.intersection(y.index)\n",
    "    X = X.loc[common_idx]\n",
    "    y = y.loc[common_idx]\n",
    "    \n",
    "    # Use last 30 days only\n",
    "    X_test = X.tail(30)\n",
    "    y_test = y.tail(30)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{ticker}: {acc:.3f} accuracy on last 30 days\")\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# Backtest all\n",
    "print(\"Backtesting on recent unseen data...\\n\")\n",
    "backtest_accs = []\n",
    "for ticker in TICKERS[:5]:  # Test first 5\n",
    "    try:\n",
    "        acc = backtest_model(ticker)\n",
    "        backtest_accs.append(acc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\nAverage backtest accuracy: {np.mean(backtest_accs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45216c56",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ DEPLOYMENT CHECKLIST\n",
    "\n",
    "After training completes:\n",
    "\n",
    "- [ ] Download all `.pkl` files from Google Drive\n",
    "- [ ] Copy to `E:\\quantum-ai-trader-v1.1\\models\\`\n",
    "- [ ] Verify file count matches number of tickers\n",
    "- [ ] Run `python trading_orchestrator.py` locally\n",
    "- [ ] Check logs for model loading confirmation\n",
    "- [ ] Monitor first 5-minute cycle for signal generation\n",
    "- [ ] Review production logs in `data/production.db`\n",
    "\n",
    "**Weekly Retraining:**\n",
    "- Re-run this notebook every Sunday\n",
    "- Download updated models\n",
    "- Replace old `.pkl` files locally\n",
    "- Restart orchestrator\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Training pipeline ready for 24/7 operation!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
