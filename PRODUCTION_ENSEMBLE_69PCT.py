"""
üéØ PRODUCTION ENSEMBLE MODEL - 69.42% Validated
Optimized weights and hyperparameters from Colab training run

PERFORMANCE:
- Test Accuracy: 69.42%
- Validation Accuracy: 70.57%
- Improvement: +7.72% over baseline (61.7%)

USAGE:
    ensemble = ProductionEnsemble()
    ensemble.fit(X_train, y_train)
    predictions = ensemble.predict(X_test)
    probabilities = ensemble.predict_proba(X_test)
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import HistGradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb
from imblearn.over_sampling import SMOTE
import pickle
import warnings
warnings.filterwarnings('ignore')


class ProductionEnsemble:
    """Production-ready ensemble with optimized weights from Colab training"""
    
    def __init__(self):
        # Optimal weights from optimization
        self.weights = {
            'xgboost': 0.35764889084740487,
            'lightgbm': 0.2700799160738165,
            'histgb': 0.3722711930787786
        }
        
        # Optimized hyperparameters
        self.xgb_params = {
            'max_depth': 9,
            'learning_rate': 0.22975529672912376,
            'n_estimators': 308,
            'subsample': 0.6818680891178277,
            'colsample_bytree': 0.9755172622676036,
            'min_child_weight': 5,
            'gamma': 0.1741229332454554,
            'reg_alpha': 2.6256661239908117,
            'reg_lambda': 5.601071337321665,
            'tree_method': 'hist',
            'random_state': 42,
            'n_jobs': -1
        }
        
        self.lgb_params = {
            'num_leaves': 187,
            'max_depth': 12,
            'learning_rate': 0.13636384853167902,
            'n_estimators': 300,
            'subsample': 0.7414206358162381,
            'colsample_bytree': 0.8881981645023311,
            'min_child_samples': 21,
            'reg_alpha': 1.3595268415034327,
            'reg_lambda': 0.004122799441053829,
            'random_state': 42,
            'verbose': -1,
            'n_jobs': -1
        }
        
        self.histgb_params = {
            'max_iter': 492,
            'max_depth': 9,
            'learning_rate': 0.2747825638707255,
            'min_samples_leaf': 13,
            'l2_regularization': 2.008590502593976,
            'random_state': 42
        }
        
        # Initialize models
        self.models = {}
        self.scaler = StandardScaler()
        self.use_smote = True
        
    def fit(self, X, y, use_smote=True):
        """
        Fit ensemble on training data
        
        Args:
            X: Training features (numpy array or DataFrame)
            y: Training labels (0=BUY, 1=HOLD, 2=SELL)
            use_smote: Whether to apply SMOTE for class balancing
        """
        print("üîß Training Production Ensemble (69.42% validated)...")
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        # Apply SMOTE if enabled
        if use_smote:
            print("   ‚öñÔ∏è  Applying SMOTE for class balancing...")
            smote = SMOTE(random_state=42)
            X_scaled, y = smote.fit_resample(X_scaled, y)
            print(f"   ‚úÖ Resampled to {len(X_scaled)} samples")
        
        # Train XGBoost
        print("   üî¨ Training XGBoost...")
        self.models['xgboost'] = xgb.XGBClassifier(**self.xgb_params)
        self.models['xgboost'].fit(X_scaled, y)
        
        # Train LightGBM
        print("   üî¨ Training LightGBM...")
        self.models['lightgbm'] = lgb.LGBMClassifier(**self.lgb_params)
        self.models['lightgbm'].fit(X_scaled, y)
        
        # Train HistGradientBoosting
        print("   üî¨ Training HistGradientBoosting...")
        self.models['histgb'] = HistGradientBoostingClassifier(**self.histgb_params)
        self.models['histgb'].fit(X_scaled, y)
        
        print("‚úÖ Training complete!")
        return self
    
    def predict_proba(self, X):
        """
        Get probability predictions from weighted ensemble
        
        Args:
            X: Features to predict on
            
        Returns:
            Probabilities for each class [BUY, HOLD, SELL]
        """
        X_scaled = self.scaler.transform(X)
        
        # Get predictions from each model
        ensemble_probs = np.zeros((len(X_scaled), 3))
        
        for model_name, weight in self.weights.items():
            probs = self.models[model_name].predict_proba(X_scaled)
            ensemble_probs += probs * weight
        
        return ensemble_probs
    
    def predict(self, X):
        """
        Get class predictions from ensemble
        
        Args:
            X: Features to predict on
            
        Returns:
            Class predictions (0=BUY, 1=HOLD, 2=SELL)
        """
        probs = self.predict_proba(X)
        return np.argmax(probs, axis=1)
    
    def predict_with_confidence(self, X, threshold=0.6):
        """
        Get predictions only when confidence exceeds threshold
        
        Args:
            X: Features to predict on
            threshold: Minimum confidence to make prediction (default 0.6)
            
        Returns:
            predictions: Class predictions (defaults to HOLD if confidence low)
            confidences: Confidence scores for each prediction
        """
        probs = self.predict_proba(X)
        predictions = np.argmax(probs, axis=1)
        confidences = np.max(probs, axis=1)
        
        # Set low-confidence predictions to HOLD
        predictions[confidences < threshold] = 1  # HOLD
        
        return predictions, confidences
    
    def get_feature_importance(self):
        """Get feature importance from each model"""
        importance = {}
        
        # XGBoost feature importance
        if hasattr(self.models['xgboost'], 'feature_importances_'):
            importance['xgboost'] = self.models['xgboost'].feature_importances_
        
        # LightGBM feature importance
        if hasattr(self.models['lightgbm'], 'feature_importances_'):
            importance['lightgbm'] = self.models['lightgbm'].feature_importances_
        
        # Weighted average importance
        if importance:
            avg_importance = np.zeros_like(list(importance.values())[0])
            for model_name, imp in importance.items():
                avg_importance += imp * self.weights[model_name]
            importance['ensemble'] = avg_importance
        
        return importance
    
    def save(self, filepath='production_ensemble_69pct.pkl'):
        """Save trained ensemble to disk"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'models': self.models,
                'scaler': self.scaler,
                'weights': self.weights,
                'params': {
                    'xgb': self.xgb_params,
                    'lgb': self.lgb_params,
                    'histgb': self.histgb_params
                }
            }, f)
        print(f"‚úÖ Ensemble saved to {filepath}")
    
    @classmethod
    def load(cls, filepath='production_ensemble_69pct.pkl'):
        """Load trained ensemble from disk"""
        ensemble = cls()
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            ensemble.models = data['models']
            ensemble.scaler = data['scaler']
            ensemble.weights = data['weights']
        print(f"‚úÖ Ensemble loaded from {filepath}")
        return ensemble


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    
    print("="*80)
    print("PRODUCTION ENSEMBLE MODEL - 69.42% Validated")
    print("="*80)
    
    # Example: Generate sample data (replace with your actual data)
    np.random.seed(42)
    n_samples = 5000
    n_features = 47  # From optimization results
    
    X = np.random.randn(n_samples, n_features)
    y = np.random.choice([0, 1, 2], size=n_samples, p=[0.21, 0.63, 0.16])
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Train ensemble
    ensemble = ProductionEnsemble()
    ensemble.fit(X_train, y_train, use_smote=True)
    
    # Make predictions
    print("\nüìä Evaluating on test set...")
    predictions = ensemble.predict(X_test)
    probabilities = ensemble.predict_proba(X_test)
    
    # Evaluate
    accuracy = accuracy_score(y_test, predictions)
    print(f"\n‚úÖ Test Accuracy: {accuracy:.4f} ({100*accuracy:.2f}%)")
    print("\n" + "="*80)
    print("Classification Report:")
    print("="*80)
    print(classification_report(y_test, predictions, 
                                target_names=['BUY', 'HOLD', 'SELL'],
                                digits=4))
    
    # Test confidence-based predictions
    print("\n" + "="*80)
    print("Confidence-Based Predictions:")
    print("="*80)
    for threshold in [0.5, 0.6, 0.7]:
        conf_preds, conf_scores = ensemble.predict_with_confidence(X_test, threshold)
        conf_acc = accuracy_score(y_test, conf_preds)
        coverage = np.mean(conf_scores >= threshold)
        print(f"Threshold {threshold:.1f}: {conf_acc:.4f} accuracy | {100*coverage:.1f}% coverage")
    
    # Save ensemble
    print("\nüíæ Saving ensemble...")
    ensemble.save('production_ensemble_69pct.pkl')
    
    print("\n‚úÖ Production ensemble ready for deployment!")
