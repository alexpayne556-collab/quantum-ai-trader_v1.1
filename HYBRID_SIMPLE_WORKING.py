"""
ðŸŽ¯ HYBRID SIMPLE WORKING - Back to Basics That Actually Work
After over-engineering failed (27% accuracy), this uses PROVEN techniques:

1. Single-scale GASF (5 channels, 30 days)
2. Balanced dataset (oversample minority classes)
3. Simpler model (1.5M params instead of 5.4M)
4. Strong regularization
5. Conservative learning

Target: 60-65% accuracy (realistic baseline)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import numpy as np
import pandas as pd
import yfinance as yf
from pyts.image import GramianAngularField
from sklearn.model_selection import train_test_split
import json
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

#===============================================================================
# SIMPLE CONFIGURATION - PROVEN TO WORK
#===============================================================================

CONFIG = {
    'tickers': [
        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META', 'BRK-B',
        'JPM', 'V', 'JNJ', 'WMT', 'PG', 'MA', 'HD', 'DIS',
        'NFLX', 'PYPL', 'ADBE', 'CRM', 'INTC', 'AMD', 'QCOM', 'COST',
        'PEP', 'KO', 'NKE', 'MCD', 'BA', 'GE', 'F', 'GM',
        'C', 'BAC', 'WFC', 'GS', 'XOM', 'CVX', 'SLB', 'COP',
        'UNH', 'PFE', 'ABBV', 'TMO', 'LLY', 'MRK', 'ABT', 'DHR'
    ],
    'window_size': 30,         # Back to 30 days (simpler)
    'horizon': 5,
    
    # Labels
    'buy_threshold': 0.03,     # Back to 3% (less noisy)
    'sell_threshold': -0.03,
    
    # Data
    'augment_multiplier': 2,   # 2x augmentation (was 3x)
    'balance_classes': True,   # NEW: Oversample minority classes
    
    # Model - SIMPLER
    'visual_channels': [5, 32, 64, 128],  # Simpler: 5â†’32â†’64â†’128 (was 15â†’64â†’128â†’256â†’512)
    'visual_dim': 128,                     # Smaller (was 512)
    'numerical_dim': 15,                   # Back to 15 (was 25)
    'fusion_dim': 128,                     # Smaller (was 256)
    
    # Training - CONSERVATIVE
    'batch_size': 32,
    'lr': 0.001,
    'weight_decay': 1e-4,
    'epochs': 30,
    'patience': 7,
    
    # Loss
    'class_weights': [2.0, 0.5, 2.0],  # STRONGER: Really penalize HOLD bias
    
    # Regularization
    'dropout': 0.3,
    'spatial_dropout': 0.2,
}

CHECKPOINT_DIR = Path('hybrid_simple_checkpoints')
CHECKPOINT_DIR.mkdir(exist_ok=True)

#===============================================================================
# SIMPLE FEATURE GENERATION
#===============================================================================

def generate_gasf_simple(df, window_size=30, augment=False):
    """Simple single-scale GASF (5 channels, 30x30)"""
    gasf = GramianAngularField(image_size=window_size, method='summation')
    
    def normalize(series):
        return 2 * (series - series.min()) / (series.max() - series.min() + 1e-8) - 1
    
    channels = []
    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        series = df[col].values[-window_size:]
        series_norm = normalize(series)
        gasf_img = gasf.fit_transform(series_norm.reshape(1, -1))[0]
        channels.append(gasf_img)
    
    gasf_stack = np.stack(channels, axis=0)  # [5, 30, 30]
    
    if augment:
        # Simple augmentation
        noise = np.random.normal(0, 0.02, gasf_stack.shape)
        gasf_stack = gasf_stack + noise
        
        if np.random.random() > 0.5:
            gasf_stack = np.flip(gasf_stack, axis=2).copy()
    
    return gasf_stack

def generate_numerical_features_simple(df, window_size=30):
    """Simple 15-D numerical features"""
    features = []
    
    close_prices = df['Close'].values[-window_size:].flatten()
    volumes = df['Volume'].values[-window_size:].flatten()
    high_prices = df['High'].values[-window_size:].flatten()
    low_prices = df['Low'].values[-window_size:].flatten()
    
    # Price statistics (5)
    features.append(float(np.mean(close_prices)))
    features.append(float(np.std(close_prices)))
    features.append(float((close_prices[-1] - close_prices[0]) / (close_prices[0] + 1e-8)))
    features.append(float(np.max(close_prices) / (np.min(close_prices) + 1e-8) - 1))
    features.append(float((high_prices[-1] - low_prices[-1]) / (close_prices[-1] + 1e-8)))
    
    # Volume statistics (3)
    features.append(float(np.mean(volumes)))
    features.append(float(np.std(volumes)))
    features.append(float(volumes[-1] / (np.mean(volumes) + 1e-8) - 1))
    
    # Technical indicators (4)
    ma_5 = np.mean(close_prices[-5:])
    ma_10 = np.mean(close_prices[-10:])
    ma_20 = np.mean(close_prices[-20:])
    features.append(float(close_prices[-1] / (ma_5 + 1e-8) - 1))
    features.append(float(close_prices[-1] / (ma_10 + 1e-8) - 1))
    features.append(float(close_prices[-1] / (ma_20 + 1e-8) - 1))
    features.append(float(ma_5 / (ma_20 + 1e-8) - 1))
    
    # Momentum (3)
    features.append(float((close_prices[-1] - close_prices[-5]) / (close_prices[-5] + 1e-8)))
    features.append(float((close_prices[-1] - close_prices[-10]) / (close_prices[-10] + 1e-8)))
    features.append(float(np.std(np.diff(close_prices) / (close_prices[:-1] + 1e-8))))
    
    return np.array(features[:15], dtype=np.float32)

def create_dataset_simple(data):
    """Generate balanced dataset"""
    print("ðŸ–¼ï¸ Generating dataset...")
    print(f"   Processing {len(data)} tickers...")
    dataset = []
    
    for ticker_idx, (ticker, df) in enumerate(data.items(), 1):
        print(f"   [{ticker_idx}/{len(data)}] {ticker}...", end='\r')
        df = df.copy()
        df['Return'] = df['Close'].pct_change(CONFIG['horizon']).shift(-CONFIG['horizon'])
        
        for i in range(CONFIG['window_size'], len(df) - CONFIG['horizon'], 2):
            window = df.iloc[i-CONFIG['window_size']:i]
            future_return = df['Return'].iloc[i]
            
            if pd.isna(future_return):
                continue
            
            # Labels
            if future_return > CONFIG['buy_threshold']:
                label = 0  # BUY
            elif future_return < CONFIG['sell_threshold']:
                label = 2  # SELL
            else:
                label = 1  # HOLD
            
            # Generate features
            gasf_img = generate_gasf_simple(window, CONFIG['window_size'], augment=False)
            numerical = generate_numerical_features_simple(window, CONFIG['window_size'])
            
            dataset.append({
                'image': gasf_img,
                'numerical': numerical,
                'label': label,
                'return': future_return,
                'ticker': ticker
            })
            
            # Augment
            if CONFIG['augment_multiplier'] > 1:
                gasf_img_aug = generate_gasf_simple(window, CONFIG['window_size'], augment=True)
                numerical_aug = numerical + np.random.normal(0, 0.01, numerical.shape).astype(np.float32)
                
                dataset.append({
                    'image': gasf_img_aug,
                    'numerical': numerical_aug,
                    'label': label,
                    'return': future_return,
                    'ticker': ticker
                })
    
    print(f"\nâœ… Generated {len(dataset)} samples")
    labels = [d['label'] for d in dataset]
    print(f"   BUY: {labels.count(0)} ({100*labels.count(0)/len(labels):.1f}%)")
    print(f"   HOLD: {labels.count(1)} ({100*labels.count(1)/len(labels):.1f}%)")
    print(f"   SELL: {labels.count(2)} ({100*labels.count(2)/len(labels):.1f}%)\n")
    
    return dataset

#===============================================================================
# SIMPLE ARCHITECTURE
#===============================================================================

class SimpleVisualCNN(nn.Module):
    """Simpler visual CNN"""
    
    def __init__(self):
        super().__init__()
        channels = CONFIG['visual_channels']
        
        self.features = nn.Sequential(
            nn.Conv2d(channels[0], channels[1], 3, padding=1),
            nn.BatchNorm2d(channels[1]),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout2d(CONFIG['spatial_dropout']),
            
            nn.Conv2d(channels[1], channels[2], 3, padding=1),
            nn.BatchNorm2d(channels[2]),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout2d(CONFIG['spatial_dropout']),
            
            nn.Conv2d(channels[2], channels[3], 3, padding=1),
            nn.BatchNorm2d(channels[3]),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)
        )
    
    def forward(self, x):
        x = self.features(x)
        return x.view(x.size(0), -1)

class SimpleNumericalMLP(nn.Module):
    """Simple numerical MLP"""
    
    def __init__(self):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(CONFIG['numerical_dim'], 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(CONFIG['dropout']),
            
            nn.Linear(64, CONFIG['fusion_dim']),
            nn.BatchNorm1d(CONFIG['fusion_dim']),
            nn.ReLU()
        )
    
    def forward(self, x):
        return self.network(x)

class SimpleHybridModel(nn.Module):
    """Simple hybrid model"""
    
    def __init__(self):
        super().__init__()
        
        self.visual = SimpleVisualCNN()
        self.numerical = SimpleNumericalMLP()
        
        # Simple fusion
        self.fusion = nn.Sequential(
            nn.Linear(CONFIG['visual_dim'] + CONFIG['fusion_dim'], 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(CONFIG['dropout']),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(CONFIG['dropout']),
            
            nn.Linear(64, 3)
        )
    
    def forward(self, image, numerical):
        visual_feat = self.visual(image)
        numerical_feat = self.numerical(numerical)
        
        fused = torch.cat([visual_feat, numerical_feat], dim=1)
        return self.fusion(fused)

#===============================================================================
# TRAINING
#===============================================================================

class HybridDataset(Dataset):
    def __init__(self, data_list):
        self.data = data_list
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        return (
            torch.FloatTensor(sample['image']),
            torch.FloatTensor(sample['numerical']),
            torch.LongTensor([sample['label']]),
            torch.FloatTensor([sample['return']])
        )

def train_model(model, train_loader, val_loader, device):
    print("ðŸš€ Starting training...\n")
    
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)
    
    class_weights = torch.FloatTensor(CONFIG['class_weights']).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    best_acc = 0
    best_state = None
    patience_counter = 0
    
    for epoch in range(CONFIG['epochs']):
        # Train
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, numerical, labels, returns in train_loader:
            images = images.to(device)
            numerical = numerical.to(device)
            labels = labels.to(device).squeeze()
            
            optimizer.zero_grad()
            outputs = model(images, numerical)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            train_loss += loss.item()
            predicted = outputs.argmax(dim=1)
            train_correct += (predicted == labels).sum().item()
            train_total += labels.size(0)
        
        # Validate
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, numerical, labels, returns in val_loader:
                images = images.to(device)
                numerical = numerical.to(device)
                labels = labels.to(device).squeeze()
                
                outputs = model(images, numerical)
                predicted = outputs.argmax(dim=1)
                val_correct += (predicted == labels).sum().item()
                val_total += labels.size(0)
        
        train_acc = train_correct / train_total
        val_acc = val_correct / val_total
        avg_loss = train_loss / len(train_loader)
        
        scheduler.step(val_acc)
        
        if val_acc > best_acc:
            best_acc = val_acc
            best_state = model.state_dict().copy()
            patience_counter = 0
            
            torch.save({
                'epoch': epoch,
                'model_state': best_state,
                'best_acc': best_acc,
                'config': CONFIG
            }, CHECKPOINT_DIR / 'best_model.pth')
        else:
            patience_counter += 1
        
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:3d}/{CONFIG['epochs']} | "
                  f"Loss: {avg_loss:.4f} | "
                  f"Train: {train_acc:.4f} | "
                  f"Val: {val_acc:.4f} | "
                  f"Best: {best_acc:.4f} | "
                  f"Patience: {patience_counter}/{CONFIG['patience']}")
        
        if patience_counter >= CONFIG['patience']:
            print(f"\nðŸ›‘ Early stopping at epoch {epoch+1}")
            break
    
    if best_state is not None:
        model.load_state_dict(best_state)
    
    print(f"\nâœ… Training complete! Best val accuracy: {best_acc:.4f}\n")
    
    return model, best_acc

#===============================================================================
# MAIN
#===============================================================================

def main():
    print("\n" + "="*80)
    print("ðŸŽ¯ HYBRID SIMPLE WORKING - Back to Basics")
    print("="*80)
    print("Strategy: Simple proven techniques that actually work")
    print("Target: 60-65% accuracy (realistic baseline)")
    print("="*80 + "\n")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ðŸ–¥ï¸ Device: {device}\n")
    
    # Download data
    print("ðŸ“¥ Downloading data...")
    data = {}
    for ticker in CONFIG['tickers']:
        try:
            df = yf.download(ticker, period='3y', interval='1d', progress=False)
            if len(df) > 100:
                data[ticker] = df
        except:
            pass
    print(f"âœ… Downloaded {len(data)} tickers\n")
    
    # Generate dataset
    dataset = create_dataset_simple(data)
    
    # Split
    train_data, temp_data = train_test_split(dataset, test_size=0.3, random_state=42, stratify=[d['label'] for d in dataset])
    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=[d['label'] for d in temp_data])
    
    print(f"ðŸ“Š Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\n")
    
    # Balanced sampling for training
    train_labels = [d['label'] for d in train_data]
    class_counts = [train_labels.count(i) for i in range(3)]
    class_weights_sampling = [1.0 / count for count in class_counts]
    sample_weights = [class_weights_sampling[label] for label in train_labels]
    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
    
    # Create dataloaders
    train_loader = DataLoader(HybridDataset(train_data), batch_size=CONFIG['batch_size'], sampler=sampler)
    val_loader = DataLoader(HybridDataset(val_data), batch_size=CONFIG['batch_size'], shuffle=False)
    test_loader = DataLoader(HybridDataset(test_data), batch_size=CONFIG['batch_size'], shuffle=False)
    
    # Create model
    print("ðŸ—ï¸ Building model...")
    model = SimpleHybridModel().to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {total_params:,}\n")
    
    # Train
    model, best_acc = train_model(model, train_loader, val_loader, device)
    
    # Test
    print("ðŸ§ª Evaluating on test set...")
    model.eval()
    test_correct = 0
    test_total = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for images, numerical, labels, returns in test_loader:
            images = images.to(device)
            numerical = numerical.to(device)
            labels = labels.to(device).squeeze()
            
            outputs = model(images, numerical)
            predicted = outputs.argmax(dim=1)
            
            test_correct += (predicted == labels).sum().item()
            test_total += labels.size(0)
            
            predictions.extend(predicted.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    test_acc = test_correct / test_total
    
    print(f"\n{'='*80}")
    print(f"ðŸ“Š FINAL RESULTS")
    print(f"{'='*80}")
    print(f"Validation: {best_acc:.4f} ({100*best_acc:.2f}%)")
    print(f"Test:       {test_acc:.4f} ({100*test_acc:.2f}%)")
    
    from sklearn.metrics import classification_report
    print(f"\nPer-Class Performance:")
    print(classification_report(true_labels, predictions, target_names=['BUY', 'HOLD', 'SELL'], digits=4))
    
    print(f"{'='*80}\n")
    
    if test_acc >= 0.60:
        print("ðŸŽ‰ SUCCESS! Achieved 60%+ baseline!")
    elif test_acc >= 0.55:
        print("âœ… Good progress! Above 55%.")
    else:
        print("ðŸ“ˆ Needs more tuning.")
    
    print("\n")

if __name__ == "__main__":
    main()
