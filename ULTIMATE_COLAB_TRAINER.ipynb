{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install Dependencies + Clone Repo + Mount Drive\n",
    "# Run this FIRST (takes 2-3 minutes)\n",
    "\n",
    "import os\n",
    "\n",
    "# Install TA-Lib system dependency and Python packages\n",
    "!apt-get install -y libta-lib-dev > /dev/null 2>&1\n",
    "!pip install -q TA-Lib yfinance lightgbm deap scikit-learn pandas numpy python-dotenv\n",
    "\n",
    "# Clone your repo to get all modules\n",
    "REPO_URL = \"https://github.com/alexpayne556-collab/quantum-ai-trader_v1.1.git\"\n",
    "REPO_DIR = \"/content/quantum-ai-trader\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(\"üìÇ Repo exists, pulling latest...\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    !git pull\n",
    "else:\n",
    "    print(\"üì• Cloning repo...\")\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Mount Google Drive for model saving\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create model save directory\n",
    "MODEL_DIR = \"/content/drive/MyDrive/quantum-trader-models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Dependencies installed!\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "print(f\"üíæ Models will save to: {MODEL_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3488e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(f\"üìä numpy: {np.__version__}\")\n",
    "print(f\"üìä pandas: {pd.__version__}\")\n",
    "print(f\"üìä lightgbm: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ce101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: MEGA Feature Engine - 100+ Indicators + Pattern Discovery Freedom\n",
    "# This learns EVERYTHING humans know + discovers what we haven't found yet\n",
    "\n",
    "class MegaFeatureEngine:\n",
    "    \"\"\"\n",
    "    100+ features covering:\n",
    "    - All known technical indicators (what humans use)\n",
    "    - EMA ribbon dynamics (tangles, expansions, compressions)\n",
    "    - Multi-timeframe momentum\n",
    "    - Volume profile analysis\n",
    "    - Volatility regime detection\n",
    "    - Sector rotation signals\n",
    "    - Price action patterns\n",
    "    - FREEDOM features (ratios, interactions the AI can discover)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        if isinstance(self.df.columns, pd.MultiIndex):\n",
    "            self.df.columns = self.df.columns.get_level_values(0)\n",
    "        self.features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    def compute_all_indicators(self):\n",
    "        close = self.df['Close'].values.astype(float)\n",
    "        high = self.df['High'].values.astype(float)\n",
    "        low = self.df['Low'].values.astype(float)\n",
    "        volume = self.df['Volume'].values.astype(float)\n",
    "        open_price = self.df['Open'].values.astype(float)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 1: MOVING AVERAGES (Foundation of all trading)\n",
    "        # ====================================================================\n",
    "        periods = [5, 8, 10, 13, 20, 21, 34, 50, 55, 89, 100, 200]\n",
    "        \n",
    "        smas = {}\n",
    "        emas = {}\n",
    "        for p in periods:\n",
    "            smas[p] = talib.SMA(close, p)\n",
    "            emas[p] = talib.EMA(close, p)\n",
    "            self.features[f'SMA{p}'] = smas[p]\n",
    "            self.features[f'EMA{p}'] = emas[p]\n",
    "            # Price relative to MA (normalized)\n",
    "            self.features[f'Close_vs_SMA{p}'] = (close - smas[p]) / (close + 1e-8)\n",
    "            self.features[f'Close_vs_EMA{p}'] = (close - emas[p]) / (close + 1e-8)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 2: EMA RIBBON DYNAMICS (The key to trend following)\n",
    "        # ====================================================================\n",
    "        # Full Fibonacci EMA ribbon: 5, 8, 13, 21, 34, 55, 89\n",
    "        fib_emas = [emas[5], emas[8], emas[13], emas[21], emas[34], emas[55], emas[89]]\n",
    "        \n",
    "        # Bullish stack (all aligned perfectly)\n",
    "        bullish_stack = np.ones(len(close))\n",
    "        bearish_stack = np.ones(len(close))\n",
    "        for i in range(len(fib_emas) - 1):\n",
    "            bullish_stack = bullish_stack * (fib_emas[i] > fib_emas[i+1])\n",
    "            bearish_stack = bearish_stack * (fib_emas[i] < fib_emas[i+1])\n",
    "        \n",
    "        self.features['EMA_Bullish_Stack'] = np.nan_to_num(bullish_stack)\n",
    "        self.features['EMA_Bearish_Stack'] = np.nan_to_num(bearish_stack)\n",
    "        \n",
    "        # Ribbon width (expansion = strong trend, compression = consolidation)\n",
    "        ribbon_width = (emas[5] - emas[89]) / (close + 1e-8)\n",
    "        self.features['Ribbon_Width'] = ribbon_width\n",
    "        self.features['Ribbon_Expanding'] = (ribbon_width > np.roll(ribbon_width, 5)).astype(float)\n",
    "        self.features['Ribbon_Compressing'] = (np.abs(ribbon_width) < np.abs(np.roll(ribbon_width, 5))).astype(float)\n",
    "        \n",
    "        # Ribbon slope (momentum of the trend)\n",
    "        for ema_p in [8, 21, 55]:\n",
    "            slope = (emas[ema_p] - np.roll(emas[ema_p], 5)) / (close + 1e-8)\n",
    "            self.features[f'EMA{ema_p}_Slope'] = slope\n",
    "        \n",
    "        # EMA crossovers (key signals)\n",
    "        self.features['EMA8_Cross_21'] = np.nan_to_num(((emas[8] > emas[21]) & (np.roll(emas[8], 1) <= np.roll(emas[21], 1))).astype(float))\n",
    "        self.features['EMA21_Cross_55'] = np.nan_to_num(((emas[21] > emas[55]) & (np.roll(emas[21], 1) <= np.roll(emas[55], 1))).astype(float))\n",
    "        self.features['Golden_Cross'] = np.nan_to_num(((smas[50] > smas[200]) & (np.roll(smas[50], 1) <= np.roll(smas[200], 1))).astype(float))\n",
    "        self.features['Death_Cross'] = np.nan_to_num(((smas[50] < smas[200]) & (np.roll(smas[50], 1) >= np.roll(smas[200], 1))).astype(float))\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 3: MOMENTUM INDICATORS (Multiple timeframes)\n",
    "        # ====================================================================\n",
    "        for period in [7, 9, 14, 21]:\n",
    "            self.features[f'RSI_{period}'] = talib.RSI(close, period)\n",
    "        \n",
    "        # RSI zones and divergences\n",
    "        rsi14 = talib.RSI(close, 14)\n",
    "        self.features['RSI_Oversold'] = (rsi14 < 30).astype(float)\n",
    "        self.features['RSI_Overbought'] = (rsi14 > 70).astype(float)\n",
    "        self.features['RSI_Neutral'] = ((rsi14 >= 40) & (rsi14 <= 60)).astype(float)\n",
    "        self.features['RSI_Momentum'] = rsi14 - np.roll(rsi14, 5)\n",
    "        \n",
    "        # Stochastic\n",
    "        slowk, slowd = talib.STOCH(high, low, close, 14, 3, 0, 3, 0)\n",
    "        self.features['Stoch_K'] = slowk\n",
    "        self.features['Stoch_D'] = slowd\n",
    "        self.features['Stoch_Cross'] = np.nan_to_num(((slowk > slowd) & (np.roll(slowk, 1) <= np.roll(slowd, 1))).astype(float))\n",
    "        \n",
    "        # MACD (multiple settings)\n",
    "        for fast, slow, sig in [(12, 26, 9), (5, 13, 1), (8, 17, 9)]:\n",
    "            macd, signal, hist = talib.MACD(close, fast, slow, sig)\n",
    "            suffix = f'{fast}_{slow}'\n",
    "            self.features[f'MACD_{suffix}'] = macd\n",
    "            self.features[f'MACD_Signal_{suffix}'] = signal\n",
    "            self.features[f'MACD_Hist_{suffix}'] = hist\n",
    "            self.features[f'MACD_Cross_{suffix}'] = np.nan_to_num(((macd > signal) & (np.roll(macd, 1) <= np.roll(signal, 1))).astype(float))\n",
    "        \n",
    "        # Williams %R\n",
    "        self.features['Williams_R'] = talib.WILLR(high, low, close, 14)\n",
    "        \n",
    "        # Rate of Change\n",
    "        for p in [5, 10, 20]:\n",
    "            self.features[f'ROC_{p}'] = talib.ROC(close, p)\n",
    "        \n",
    "        # Momentum\n",
    "        self.features['MOM_10'] = talib.MOM(close, 10)\n",
    "        self.features['MOM_20'] = talib.MOM(close, 20)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 4: VOLATILITY (Regime detection)\n",
    "        # ====================================================================\n",
    "        atr14 = talib.ATR(high, low, close, 14)\n",
    "        atr7 = talib.ATR(high, low, close, 7)\n",
    "        \n",
    "        self.features['ATR_14'] = atr14\n",
    "        self.features['ATR_7'] = atr7\n",
    "        self.features['ATR_Ratio'] = atr14 / (close + 1e-8)\n",
    "        self.features['ATR_Expanding'] = (atr14 > np.roll(atr14, 5)).astype(float)\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        for period in [20, 50]:\n",
    "            bb_upper, bb_mid, bb_lower = talib.BBANDS(close, period, 2, 2)\n",
    "            self.features[f'BB_Width_{period}'] = (bb_upper - bb_lower) / (bb_mid + 1e-8)\n",
    "            self.features[f'BB_Position_{period}'] = (close - bb_lower) / (bb_upper - bb_lower + 1e-8)\n",
    "        \n",
    "        # Keltner Channel\n",
    "        kelt_mid = emas[20]\n",
    "        kelt_upper = kelt_mid + 2 * atr14\n",
    "        kelt_lower = kelt_mid - 2 * atr14\n",
    "        self.features['Keltner_Position'] = (close - kelt_lower) / (kelt_upper - kelt_lower + 1e-8)\n",
    "        \n",
    "        # Squeeze detection (BB inside Keltner = low volatility, breakout coming)\n",
    "        bb_upper, bb_mid, bb_lower = talib.BBANDS(close, 20, 2, 2)\n",
    "        squeeze = ((bb_lower > kelt_lower) & (bb_upper < kelt_upper)).astype(float)\n",
    "        self.features['Squeeze'] = np.nan_to_num(squeeze)\n",
    "        self.features['Squeeze_Release'] = np.nan_to_num((np.roll(squeeze, 1) == 1) & (squeeze == 0)).astype(float)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 5: VOLUME ANALYSIS (Confirmation of moves)\n",
    "        # ====================================================================\n",
    "        vol_sma20 = talib.SMA(volume, 20)\n",
    "        vol_sma50 = talib.SMA(volume, 50)\n",
    "        \n",
    "        self.features['Vol_Ratio_20'] = volume / (vol_sma20 + 1e-8)\n",
    "        self.features['Vol_Ratio_50'] = volume / (vol_sma50 + 1e-8)\n",
    "        self.features['Vol_Surge'] = (volume > 2 * vol_sma20).astype(float)\n",
    "        \n",
    "        self.features['OBV'] = talib.OBV(close, volume)\n",
    "        self.features['OBV_Slope'] = (self.features['OBV'] - self.features['OBV'].shift(5)) / (close + 1e-8)\n",
    "        \n",
    "        self.features['MFI'] = talib.MFI(high, low, close, volume, 14)\n",
    "        self.features['AD'] = talib.AD(high, low, close, volume)\n",
    "        self.features['CMF'] = talib.ADOSC(high, low, close, volume, 3, 10)\n",
    "        \n",
    "        # Volume Price Trend\n",
    "        self.features['Vol_Price_Trend'] = (volume * ((close - np.roll(close, 1)) / (np.roll(close, 1) + 1e-8))).cumsum()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 6: TREND STRENGTH (ADX family)\n",
    "        # ====================================================================\n",
    "        self.features['ADX'] = talib.ADX(high, low, close, 14)\n",
    "        self.features['PLUS_DI'] = talib.PLUS_DI(high, low, close, 14)\n",
    "        self.features['MINUS_DI'] = talib.MINUS_DI(high, low, close, 14)\n",
    "        self.features['DI_Diff'] = self.features['PLUS_DI'] - self.features['MINUS_DI']\n",
    "        self.features['Strong_Trend'] = (self.features['ADX'] > 25).astype(float)\n",
    "        self.features['DI_Cross'] = np.nan_to_num(((self.features['PLUS_DI'] > self.features['MINUS_DI']) & \n",
    "                                                    (self.features['PLUS_DI'].shift(1) <= self.features['MINUS_DI'].shift(1))).astype(float))\n",
    "        \n",
    "        # Aroon\n",
    "        aroon_down, aroon_up = talib.AROON(high, low, 14)\n",
    "        self.features['Aroon_Up'] = aroon_up\n",
    "        self.features['Aroon_Down'] = aroon_down\n",
    "        self.features['Aroon_Osc'] = aroon_up - aroon_down\n",
    "        \n",
    "        # CCI\n",
    "        self.features['CCI'] = talib.CCI(high, low, close, 14)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 7: PRICE ACTION (Raw market behavior)\n",
    "        # ====================================================================\n",
    "        self.features['Body_Size'] = np.abs(close - open_price) / (close + 1e-8)\n",
    "        self.features['Upper_Wick'] = (high - np.maximum(open_price, close)) / (close + 1e-8)\n",
    "        self.features['Lower_Wick'] = (np.minimum(open_price, close) - low) / (close + 1e-8)\n",
    "        self.features['Wick_Ratio'] = self.features['Upper_Wick'] / (self.features['Lower_Wick'] + 1e-8)\n",
    "        \n",
    "        # Gaps\n",
    "        self.features['Gap'] = (open_price - np.roll(close, 1)) / (np.roll(close, 1) + 1e-8)\n",
    "        self.features['Gap_Up'] = (self.features['Gap'] > 0.005).astype(float)\n",
    "        self.features['Gap_Down'] = (self.features['Gap'] < -0.005).astype(float)\n",
    "        \n",
    "        # Range\n",
    "        self.features['HL_Range'] = (high - low) / (close + 1e-8)\n",
    "        self.features['Range_vs_ATR'] = (high - low) / (atr14 + 1e-8)\n",
    "        \n",
    "        # Candle patterns (bullish/bearish)\n",
    "        self.features['Bullish_Candle'] = (close > open_price).astype(float)\n",
    "        self.features['Bearish_Candle'] = (close < open_price).astype(float)\n",
    "        self.features['Doji'] = (self.features['Body_Size'] < 0.001).astype(float)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 8: RETURNS (What we're trying to predict)\n",
    "        # ====================================================================\n",
    "        for p in [1, 2, 3, 5, 10, 20]:\n",
    "            ret = (close - np.roll(close, p)) / (np.roll(close, p) + 1e-8)\n",
    "            ret[:p] = 0\n",
    "            self.features[f'Return_{p}d'] = ret\n",
    "        \n",
    "        # Cumulative return\n",
    "        self.features['Cum_Return_20d'] = (close / np.roll(close, 20)) - 1\n",
    "        \n",
    "        # Volatility of returns\n",
    "        ret_1d = np.diff(close) / close[:-1]\n",
    "        ret_1d = np.concatenate([[0], ret_1d])\n",
    "        self.features['Return_Volatility'] = pd.Series(ret_1d).rolling(20).std().values\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 9: REGIME DETECTION (Market state)\n",
    "        # ====================================================================\n",
    "        # Bull market: price above 200 SMA, 50 SMA above 200 SMA\n",
    "        self.features['Bull_Regime'] = ((close > smas[200]) & (smas[50] > smas[200])).astype(float)\n",
    "        self.features['Bear_Regime'] = ((close < smas[200]) & (smas[50] < smas[200])).astype(float)\n",
    "        self.features['Volatile_Regime'] = (atr14 / (close + 1e-8) > 0.02).astype(float)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SECTION 10: DISCOVERY FEATURES (Ratios for AI to find patterns)\n",
    "        # ====================================================================\n",
    "        # Let AI discover which ratios matter\n",
    "        self.features['RSI_ADX_Ratio'] = rsi14 / (self.features['ADX'] + 1e-8)\n",
    "        self.features['MACD_ATR_Ratio'] = self.features['MACD_12_26'] / (atr14 + 1e-8)\n",
    "        self.features['Vol_Momentum'] = self.features['Vol_Ratio_20'] * self.features['MOM_10']\n",
    "        self.features['Trend_Vol_Product'] = self.features['ADX'] * self.features['Vol_Ratio_20']\n",
    "        self.features['EMA_RSI_Combo'] = ribbon_width * rsi14\n",
    "        self.features['Squeeze_Momentum'] = squeeze * self.features['MOM_10']\n",
    "        \n",
    "        # Price position in recent range\n",
    "        high_20 = pd.Series(high).rolling(20).max().values\n",
    "        low_20 = pd.Series(low).rolling(20).min().values\n",
    "        self.features['Price_Position_20d'] = (close - low_20) / (high_20 - low_20 + 1e-8)\n",
    "        \n",
    "        return self.features.dropna()\n",
    "\n",
    "print(\"‚úÖ MegaFeatureEngine defined with 100+ indicators!\")\n",
    "print(\"   - Moving Averages (12 periods √ó 4 = 48 features)\")\n",
    "print(\"   - EMA Ribbon dynamics (10 features)\")\n",
    "print(\"   - Momentum (RSI, MACD, Stoch = 25+ features)\")  \n",
    "print(\"   - Volatility & Squeeze (15 features)\")\n",
    "print(\"   - Volume analysis (10 features)\")\n",
    "print(\"   - Trend strength (10 features)\")\n",
    "print(\"   - Price action (15 features)\")\n",
    "print(\"   - Regime detection (5 features)\")\n",
    "print(\"   - Discovery ratios (10 features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: YOUR EXACT TICKER LIST - ROTATION MASTERY\n",
    "# These are YOUR tickers - the AI will DEEPLY learn their patterns\n",
    "# for optimal rotation timing: when to enter, when to exit, when to rotate\n",
    "\n",
    "TICKERS = [\n",
    "    # === YOUR CORE ROTATION UNIVERSE ===\n",
    "    'APLD',   # AI/Data center infrastructure\n",
    "    'SERV',   # ServFirst Bancshares\n",
    "    'MRVL',   # Marvell - semiconductors\n",
    "    'HOOD',   # Robinhood - your broker!\n",
    "    'LUNR',   # Intuitive Machines - space\n",
    "    'BAC',    # Bank of America - financials\n",
    "    'WSHP',   # Worship ETF\n",
    "    'QCOM',   # Qualcomm - chips\n",
    "    'UUUU',   # Energy Fuels - uranium\n",
    "    'TSLA',   # Tesla - high volatility king\n",
    "    'AMD',    # AMD - semiconductor momentum\n",
    "    'NOW',    # ServiceNow - enterprise SaaS\n",
    "    'NVDA',   # NVIDIA - AI leader\n",
    "    'MU',     # Micron - memory chips\n",
    "    'PG',     # Procter & Gamble - defensive\n",
    "    'DLB',    # Dolby - audio tech\n",
    "    'XME',    # Metals & Mining ETF\n",
    "    'KRYS',   # Krystal Biotech\n",
    "    'LEU',    # Centrus Energy - uranium\n",
    "    'QTUM',   # Quantum computing ETF\n",
    "    'SPY',    # S&P 500 - benchmark\n",
    "    'UNH',    # UnitedHealth - healthcare\n",
    "    'WMT',    # Walmart - retail\n",
    "    'OKLO',   # Oklo - nuclear energy\n",
    "    'B',      # Barnes Group\n",
    "    'RXRX',   # Recursion Pharma - AI drug discovery\n",
    "    'MTZ',    # MasTec - infrastructure\n",
    "    'SNOW',   # Snowflake - data cloud\n",
    "    'GRRR',   # Gorilla Technology\n",
    "    'BSX',    # Boston Scientific - medical devices\n",
    "    'LLY',    # Eli Lilly - pharma leader\n",
    "    'SCHA',   # Small cap ETF\n",
    "    'VOO',    # Vanguard S&P 500\n",
    "    'GEO',    # GEO Group - REITs\n",
    "    'CXW',    # CoreCivic\n",
    "    'LYFT',   # Lyft - rideshare\n",
    "    'MNDY',   # Monday.com - work management\n",
    "    'BA',     # Boeing - aerospace\n",
    "    'LAC',    # Lithium Americas\n",
    "    'INTC',   # Intel - chips\n",
    "    'ALK',    # Alaska Air\n",
    "    'LMT',    # Lockheed Martin - defense\n",
    "    'CRDO',   # Credo Technology\n",
    "    'ANET',   # Arista Networks\n",
    "    'META',   # Meta - social/AI\n",
    "    'RIVN',   # Rivian - EV\n",
    "    'GOOGL',  # Google - search/AI\n",
    "    'HL',     # Hecla Mining - silver\n",
    "    'TEM',    # Tempus AI - healthcare AI\n",
    "    'TDOC',   # Teladoc - telehealth\n",
    "]\n",
    "\n",
    "# Remove duplicates, preserve order\n",
    "TICKERS = list(dict.fromkeys(TICKERS))\n",
    "\n",
    "# === SECTOR MAPPING FOR YOUR TICKERS ===\n",
    "# This enables relative strength analysis vs sector\n",
    "SECTOR_MAP = {\n",
    "    # Tech/Semiconductors -> XLK\n",
    "    'NVDA': 'XLK', 'AMD': 'XLK', 'MRVL': 'XLK', 'QCOM': 'XLK', 'MU': 'XLK',\n",
    "    'INTC': 'XLK', 'CRDO': 'XLK', 'ANET': 'XLK', 'NOW': 'XLK', 'SNOW': 'XLK',\n",
    "    'META': 'XLK', 'GOOGL': 'XLK', 'APLD': 'XLK', 'DLB': 'XLK', 'MNDY': 'XLK',\n",
    "    \n",
    "    # Financials -> XLF\n",
    "    'BAC': 'XLF', 'HOOD': 'XLF', 'SERV': 'XLF',\n",
    "    \n",
    "    # Energy/Uranium -> XLE\n",
    "    'UUUU': 'XLE', 'LEU': 'XLE', 'OKLO': 'XLE', 'LAC': 'XLE',\n",
    "    \n",
    "    # Healthcare -> XLV\n",
    "    'UNH': 'XLV', 'LLY': 'XLV', 'BSX': 'XLV', 'KRYS': 'XLV', 'RXRX': 'XLV',\n",
    "    'TEM': 'XLV', 'TDOC': 'XLV',\n",
    "    \n",
    "    # Consumer -> XLY\n",
    "    'TSLA': 'XLY', 'WMT': 'XLY', 'PG': 'XLY', 'RIVN': 'XLY', 'LYFT': 'XLY',\n",
    "    \n",
    "    # Industrials -> XLI\n",
    "    'BA': 'XLI', 'LMT': 'XLI', 'MTZ': 'XLI', 'ALK': 'XLI', 'B': 'XLI',\n",
    "    \n",
    "    # Materials/Mining -> XME (use as proxy)\n",
    "    'HL': 'XME', 'GEO': 'XME', 'CXW': 'XME',\n",
    "    \n",
    "    # Space/Innovation\n",
    "    'LUNR': 'XLK', 'QTUM': 'XLK', 'GRRR': 'XLK',\n",
    "}\n",
    "\n",
    "# Training config - DEEP LEARNING ON YOUR TICKERS\n",
    "START_DATE = '2015-01-01'  # 10 years for established tickers, newer ones use available data\n",
    "TARGET_DAYS = 5            # 5-day forward return (swing trading with 2 day trades/week)\n",
    "TARGET_THRESHOLD = 0.02    # 2% minimum gain target\n",
    "\n",
    "print(\"üéØ YOUR ROTATION UNIVERSE LOADED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ {len(TICKERS)} tickers - YOUR exact watchlist\")\n",
    "print(f\"‚úÖ Date range: {START_DATE} to today\")\n",
    "print(f\"‚úÖ Target: {TARGET_DAYS}-day return > {TARGET_THRESHOLD:.0%}\")\n",
    "print(f\"\\nüìä Your Rotation Tickers:\")\n",
    "for i in range(0, len(TICKERS), 8):\n",
    "    print(f\"   {', '.join(TICKERS[i:i+8])}\")\n",
    "\n",
    "print(f\"\\nüîÑ ROTATION STRATEGY:\")\n",
    "print(f\"   ‚Ä¢ AI learns WHEN each ticker is ready to run\")\n",
    "print(f\"   ‚Ä¢ AI learns WHEN to exit before drops\")\n",
    "print(f\"   ‚Ä¢ AI learns optimal rotation timing\")\n",
    "print(f\"   ‚Ä¢ Constrained for 2 day trades per 7 days (Robinhood)\")\n",
    "print(f\"\\nüöÄ LET THE DEEP LEARNING BEGIN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ac8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Load YOUR Ticker Data + Sector ETFs for Relative Strength\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download all ticker data with maximum history\n",
    "all_data = {}\n",
    "\n",
    "# Also download sector ETFs for relative strength (even if not in your list)\n",
    "SECTOR_ETFS = ['XLK', 'XLF', 'XLE', 'XLV', 'XLY', 'XLI', 'XME', 'SPY']\n",
    "TICKERS_TO_DOWNLOAD = list(set(TICKERS + SECTOR_ETFS))\n",
    "\n",
    "print(\"üì• Downloading historical data for YOUR rotation universe...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "success_count = 0\n",
    "failed_tickers = []\n",
    "\n",
    "for ticker in TICKERS_TO_DOWNLOAD:\n",
    "    try:\n",
    "        df = yf.download(ticker, start=START_DATE, progress=False)\n",
    "        if len(df) > 100:  # Need some history (newer tickers may have less)\n",
    "            all_data[ticker] = df\n",
    "            years = len(df) / 252\n",
    "            status = \"‚úÖ\" if ticker in TICKERS else \"üìä\"  # Mark sector ETFs differently\n",
    "            print(f\"{status} {ticker}: {len(df):,} days ({years:.1f} years)\")\n",
    "            if ticker in TICKERS:\n",
    "                success_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {ticker}: Only {len(df)} days - SKIPPED (too new)\")\n",
    "            failed_tickers.append(ticker)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {ticker}: {str(e)[:50]}\")\n",
    "        failed_tickers.append(ticker)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä LOADED: {success_count}/{len(TICKERS)} of YOUR tickers\")\n",
    "if failed_tickers:\n",
    "    print(f\"‚ö†Ô∏è Failed/skipped: {[t for t in failed_tickers if t in TICKERS]}\")\n",
    "print(f\"üìà Total data points: {sum(len(df) for df in all_data.values()):,}\")\n",
    "\n",
    "# Show which tickers have the most data (best for learning)\n",
    "print(f\"\\nüèÜ TICKERS WITH MOST HISTORY (best training data):\")\n",
    "ticker_lengths = [(t, len(df)) for t, df in all_data.items() if t in TICKERS]\n",
    "ticker_lengths.sort(key=lambda x: -x[1])\n",
    "for t, length in ticker_lengths[:10]:\n",
    "    print(f\"   {t}: {length:,} days ({length/252:.1f} years)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5.5: Visual Pattern Discovery - Let AI \"SEE\" chart patterns\n",
    "\n",
    "def create_chart_image_features(df, lookback=20):\n",
    "    \"\"\"\n",
    "    Create features that capture VISUAL patterns in price action.\n",
    "    The AI learns to \"see\" patterns like EMA ribbon tangles, breakouts, etc.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    close = df['Close'].values if hasattr(df['Close'], 'values') else df['Close']\n",
    "    high = df['High'].values if hasattr(df['High'], 'values') else df['High']\n",
    "    low = df['Low'].values if hasattr(df['Low'], 'values') else df['Low']\n",
    "    \n",
    "    # Flatten MultiIndex columns if present\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        close = df['Close'].iloc[:, 0].values if df['Close'].ndim > 1 else df['Close'].values\n",
    "        high = df['High'].iloc[:, 0].values if df['High'].ndim > 1 else df['High'].values\n",
    "        low = df['Low'].iloc[:, 0].values if df['Low'].ndim > 1 else df['Low'].values\n",
    "    \n",
    "    # === EMA RIBBON TANGLE DETECTION ===\n",
    "    # When EMAs converge/tangle = big move coming\n",
    "    ema_periods = [8, 13, 21, 34, 55]\n",
    "    emas = {}\n",
    "    for p in ema_periods:\n",
    "        emas[p] = pd.Series(close).ewm(span=p, adjust=False).mean().values\n",
    "    \n",
    "    # EMA spread (expansion vs contraction)\n",
    "    ema_max = np.maximum.reduce([emas[p] for p in ema_periods])\n",
    "    ema_min = np.minimum.reduce([emas[p] for p in ema_periods])\n",
    "    features['ema_ribbon_width'] = (ema_max - ema_min) / close\n",
    "    features['ema_ribbon_width_change'] = pd.Series(features['ema_ribbon_width']).diff(5).values\n",
    "    \n",
    "    # Ribbon tangle detection (all EMAs within 1% = TANGLE)\n",
    "    tangle_threshold = 0.01\n",
    "    features['ema_tangle'] = (features['ema_ribbon_width'] < tangle_threshold).astype(float)\n",
    "    \n",
    "    # === BREAKOUT DETECTION ===\n",
    "    # Price breaking above/below recent range\n",
    "    for period in [10, 20, 50]:\n",
    "        rolling_high = pd.Series(high).rolling(period).max().values\n",
    "        rolling_low = pd.Series(low).rolling(period).min().values\n",
    "        features[f'breakout_up_{period}'] = (close > rolling_high * 0.998).astype(float)\n",
    "        features[f'breakout_down_{period}'] = (close < rolling_low * 1.002).astype(float)\n",
    "        features[f'distance_from_high_{period}'] = (close - rolling_high) / close\n",
    "        features[f'distance_from_low_{period}'] = (close - rolling_low) / close\n",
    "    \n",
    "    # === CANDLESTICK PATTERN SHAPES ===\n",
    "    # Body size relative to range\n",
    "    body = np.abs(close - df['Open'].values.flatten() if isinstance(df.columns, pd.MultiIndex) else np.abs(close - df['Open'].values))\n",
    "    candle_range = high - low + 0.0001\n",
    "    features['body_to_range'] = body / candle_range\n",
    "    \n",
    "    # Upper/lower shadow ratios (detect dojis, hammers, etc)\n",
    "    upper_shadow = high - np.maximum(close, df['Open'].values.flatten() if isinstance(df.columns, pd.MultiIndex) else df['Open'].values)\n",
    "    lower_shadow = np.minimum(close, df['Open'].values.flatten() if isinstance(df.columns, pd.MultiIndex) else df['Open'].values) - low\n",
    "    features['upper_shadow_ratio'] = upper_shadow / candle_range\n",
    "    features['lower_shadow_ratio'] = lower_shadow / candle_range\n",
    "    \n",
    "    # === TREND ANGLE DETECTION ===\n",
    "    # Slope of price over different periods (trend \"steepness\")\n",
    "    for period in [5, 10, 20]:\n",
    "        if len(close) > period:\n",
    "            slopes = np.zeros(len(close))\n",
    "            for i in range(period, len(close)):\n",
    "                x = np.arange(period)\n",
    "                y = close[i-period:i]\n",
    "                slope, _ = np.polyfit(x, y, 1)\n",
    "                slopes[i] = slope / close[i] * period  # Normalized slope\n",
    "            features[f'trend_slope_{period}'] = slopes\n",
    "    \n",
    "    # === SUPPORT/RESISTANCE PROXIMITY ===\n",
    "    # How close is price to recent pivots\n",
    "    def find_pivots(arr, order=5):\n",
    "        pivots_high = []\n",
    "        pivots_low = []\n",
    "        for i in range(order, len(arr) - order):\n",
    "            if arr[i] == max(arr[i-order:i+order+1]):\n",
    "                pivots_high.append((i, arr[i]))\n",
    "            if arr[i] == min(arr[i-order:i+order+1]):\n",
    "                pivots_low.append((i, arr[i]))\n",
    "        return pivots_high, pivots_low\n",
    "    \n",
    "    # Calculate distance to nearest support/resistance\n",
    "    features['distance_to_support'] = np.zeros(len(close))\n",
    "    features['distance_to_resistance'] = np.zeros(len(close))\n",
    "    \n",
    "    return pd.DataFrame(features, index=df.index)\n",
    "\n",
    "# Test visual pattern features on first ticker\n",
    "test_ticker = list(all_data.keys())[0]\n",
    "visual_features = create_chart_image_features(all_data[test_ticker])\n",
    "print(f\"‚úÖ Visual Pattern Features created: {len(visual_features.columns)} features\")\n",
    "print(f\"üìä Feature list: {list(visual_features.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de658657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Generate 100+ Features + Sector Relative Strength for ALL Tickers\n",
    "\n",
    "def prepare_training_data(all_data, target_days=5, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Prepare massive feature set with cross-asset intelligence.\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    feature_columns = None\n",
    "    \n",
    "    # Get SPY data for relative strength calculations\n",
    "    spy_data = all_data.get('SPY', None)\n",
    "    spy_returns = None\n",
    "    if spy_data is not None:\n",
    "        spy_close = spy_data['Close'].values.flatten() if isinstance(spy_data.columns, pd.MultiIndex) else spy_data['Close'].values\n",
    "        spy_returns = pd.Series(spy_close, index=spy_data.index).pct_change()\n",
    "    \n",
    "    for ticker, df in all_data.items():\n",
    "        try:\n",
    "            # Generate base features using the MegaFeatureEngine CLASS\n",
    "            engine = MegaFeatureEngine(df)\n",
    "            features = engine.compute_all_indicators()\n",
    "            \n",
    "            # Add visual pattern features\n",
    "            visual_feats = create_chart_image_features(df)\n",
    "            for col in visual_feats.columns:\n",
    "                if col not in features.columns:\n",
    "                    features[col] = visual_feats[col].reindex(features.index)\n",
    "            \n",
    "            # === SECTOR RELATIVE STRENGTH ===\n",
    "            if ticker in SECTOR_MAP and SECTOR_MAP[ticker] in all_data:\n",
    "                sector_df = all_data[SECTOR_MAP[ticker]]\n",
    "                sector_close = sector_df['Close'].values.flatten() if isinstance(sector_df.columns, pd.MultiIndex) else sector_df['Close'].values\n",
    "                ticker_close = df['Close'].values.flatten() if isinstance(df.columns, pd.MultiIndex) else df['Close'].values\n",
    "                \n",
    "                # Align by index (use common dates)\n",
    "                ticker_returns = pd.Series(ticker_close, index=df.index).pct_change()\n",
    "                sector_returns = pd.Series(sector_close, index=sector_df.index).pct_change()\n",
    "                \n",
    "                # Relative strength vs sector\n",
    "                common_idx = ticker_returns.index.intersection(sector_returns.index)\n",
    "                if len(common_idx) > 100:\n",
    "                    rs_vs_sector = ticker_returns.loc[common_idx] - sector_returns.loc[common_idx]\n",
    "                    features['rs_vs_sector_1d'] = rs_vs_sector.reindex(features.index)\n",
    "                    features['rs_vs_sector_5d'] = rs_vs_sector.rolling(5).sum().reindex(features.index)\n",
    "                    features['rs_vs_sector_20d'] = rs_vs_sector.rolling(20).sum().reindex(features.index)\n",
    "            \n",
    "            # === RELATIVE STRENGTH VS SPY (Market) ===\n",
    "            if spy_returns is not None:\n",
    "                ticker_close = df['Close'].values.flatten() if isinstance(df.columns, pd.MultiIndex) else df['Close'].values\n",
    "                ticker_returns = pd.Series(ticker_close, index=df.index).pct_change()\n",
    "                common_idx = ticker_returns.index.intersection(spy_returns.index)\n",
    "                if len(common_idx) > 100:\n",
    "                    rs_vs_spy = ticker_returns.loc[common_idx] - spy_returns.loc[common_idx]\n",
    "                    features['rs_vs_spy_1d'] = rs_vs_spy.reindex(features.index)\n",
    "                    features['rs_vs_spy_5d'] = rs_vs_spy.rolling(5).sum().reindex(features.index)\n",
    "                    features['rs_vs_spy_20d'] = rs_vs_spy.rolling(20).sum().reindex(features.index)\n",
    "            \n",
    "            # === CREATE TARGET ===\n",
    "            close = df['Close'].values.flatten() if isinstance(df.columns, pd.MultiIndex) else df['Close'].values\n",
    "            future_return = pd.Series(close, index=df.index).pct_change(target_days).shift(-target_days)\n",
    "            target = (future_return > threshold).astype(int)\n",
    "            \n",
    "            # Align and drop NaN\n",
    "            features['target'] = target.reindex(features.index)\n",
    "            features = features.dropna()\n",
    "            \n",
    "            if len(features) > 200:\n",
    "                # Store feature columns for consistency\n",
    "                if feature_columns is None:\n",
    "                    feature_columns = [c for c in features.columns if c != 'target']\n",
    "                \n",
    "                # Ensure all tickers have same features\n",
    "                for col in feature_columns:\n",
    "                    if col not in features.columns:\n",
    "                        features[col] = 0\n",
    "                \n",
    "                X = features[feature_columns].values\n",
    "                y = features['target'].values\n",
    "                all_X.append(X)\n",
    "                all_y.append(y)\n",
    "                print(f\"‚úÖ {ticker}: {len(X):,} samples, {X.shape[1]} features, {y.mean()*100:.1f}% positive\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è {ticker}: Not enough data after feature generation\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"‚ùå {ticker}: Error - {str(e)[:80]}\")\n",
    "            # Uncomment below for debugging:\n",
    "            # traceback.print_exc()\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_X:\n",
    "        X_combined = np.vstack(all_X)\n",
    "        y_combined = np.concatenate([y.ravel() for y in all_y])\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä TOTAL: {X_combined.shape[0]:,} samples, {X_combined.shape[1]} features\")\n",
    "        print(f\"üìà Positive rate: {y_combined.mean()*100:.1f}%\")\n",
    "        return X_combined, y_combined, feature_columns\n",
    "    else:\n",
    "        raise ValueError(\"No valid data processed!\")\n",
    "\n",
    "# Process all tickers\n",
    "print(\"üîÑ Generating 100+ features for all tickers...\")\n",
    "print(\"=\" * 60)\n",
    "X, y, feature_names = prepare_training_data(all_data, TARGET_DAYS, TARGET_THRESHOLD)\n",
    "print(f\"\\n‚úÖ Data ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Train Universal LightGBM Model with Walk-Forward Validation\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"ü§ñ TRAINING UNIVERSAL AI MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Training on {X.shape[0]:,} samples with {X.shape[1]} features\")\n",
    "\n",
    "# LightGBM parameters optimized for T4 GPU\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 127,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Walk-forward validation with 5 splits\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "fold_scores = []\n",
    "\n",
    "print(\"\\nüìä Walk-Forward Validation Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_auc = roc_auc_score(y_val, val_pred)\n",
    "    val_pred_binary = (val_pred > 0.5).astype(int)\n",
    "    val_acc = accuracy_score(y_val, val_pred_binary)\n",
    "    \n",
    "    fold_scores.append({'fold': fold, 'auc': val_auc, 'acc': val_acc})\n",
    "    print(f\"Fold {fold}: AUC={val_auc:.4f}, Acc={val_acc:.4f}, Samples={len(val_idx):,}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "mean_auc = np.mean([s['auc'] for s in fold_scores])\n",
    "mean_acc = np.mean([s['acc'] for s in fold_scores])\n",
    "print(f\"üìà Mean AUC: {mean_auc:.4f}\")\n",
    "print(f\"üìà Mean Acc: {mean_acc:.4f}\")\n",
    "\n",
    "# Train final model on all data\n",
    "print(\"\\nüéØ Training Final Model on ALL data...\")\n",
    "train_data_full = lgb.Dataset(X, label=y)\n",
    "final_model = lgb.train(params, train_data_full, num_boost_round=500)\n",
    "print(\"‚úÖ Final model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06923a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7.5: Genetic Formula Evolution - DISCOVER New Alpha Patterns\n",
    "# NOTE: This cell is OPTIONAL - skip if you want faster training\n",
    "\n",
    "from deap import base, creator, tools, gp, algorithms\n",
    "import operator\n",
    "import random\n",
    "import warnings\n",
    "import json  # <-- ADDED: needed for logging\n",
    "\n",
    "def protected_div(left, right):\n",
    "    if abs(right) < 0.0001:\n",
    "        return 1.0\n",
    "    return left / right\n",
    "\n",
    "def protected_log(x):\n",
    "    if x <= 0:\n",
    "        return 0.0\n",
    "    return np.log(x)\n",
    "\n",
    "def genetic_formula_evolution(X_data, y_data, feat_names, n_pop=100, n_gen=30):\n",
    "    \"\"\"\n",
    "    Use genetic programming to EVOLVE trading formulas.\n",
    "    The AI creates and combines features in ways we haven't thought of!\n",
    "    \"\"\"\n",
    "    print(\"üß¨ GENETIC FORMULA EVOLUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Population: {n_pop} formulas, Generations: {n_gen}\")\n",
    "    print(\"The AI will discover NEW indicator combinations...\")\n",
    "    \n",
    "    # Clean up any previous DEAP state\n",
    "    if 'FitnessMax' in creator.__dict__:\n",
    "        del creator.FitnessMax\n",
    "    if 'Individual' in creator.__dict__:\n",
    "        del creator.Individual\n",
    "    \n",
    "    # Define primitives (operations the AI can use)\n",
    "    pset = gp.PrimitiveSet(\"MAIN\", len(feat_names))\n",
    "    \n",
    "    # Arithmetic operations\n",
    "    pset.addPrimitive(operator.add, 2)\n",
    "    pset.addPrimitive(operator.sub, 2)\n",
    "    pset.addPrimitive(operator.mul, 2)\n",
    "    pset.addPrimitive(protected_div, 2)\n",
    "    pset.addPrimitive(operator.neg, 1)\n",
    "    pset.addPrimitive(abs, 1)\n",
    "    \n",
    "    # Mathematical functions\n",
    "    pset.addPrimitive(np.sin, 1)\n",
    "    pset.addPrimitive(np.cos, 1)\n",
    "    pset.addPrimitive(protected_log, 1)\n",
    "    pset.addPrimitive(np.sqrt, 1)\n",
    "    \n",
    "    # Constants\n",
    "    pset.addEphemeralConstant(\"rand\", lambda: random.uniform(-1, 1))\n",
    "    \n",
    "    # Rename arguments to feature names (limited to avoid complexity)\n",
    "    for i, name in enumerate(feat_names):\n",
    "        pset.renameArguments(**{f'ARG{i}': name[:10]})\n",
    "    \n",
    "    # Create fitness and individual\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMax)\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=4)\n",
    "    toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"compile\", gp.compile, pset=pset)\n",
    "    \n",
    "    # Evaluation function\n",
    "    def evaluate(individual):\n",
    "        try:\n",
    "            func = toolbox.compile(expr=individual)\n",
    "            # Apply formula to features (sample for speed)\n",
    "            sample_idx = np.random.choice(len(X_data), min(5000, len(X_data)), replace=False)\n",
    "            X_sample = X_data[sample_idx]\n",
    "            y_sample = y_data[sample_idx]\n",
    "            \n",
    "            # Calculate formula output\n",
    "            signals = np.array([func(*row) for row in X_sample])\n",
    "            \n",
    "            # Handle inf/nan\n",
    "            signals = np.nan_to_num(signals, nan=0, posinf=0, neginf=0)\n",
    "            \n",
    "            # Calculate predictive power (correlation with target)\n",
    "            if np.std(signals) > 0.0001:\n",
    "                correlation = np.corrcoef(signals, y_sample)[0, 1]\n",
    "                if np.isnan(correlation):\n",
    "                    return (0.0,)\n",
    "                return (abs(correlation),)\n",
    "            return (0.0,)\n",
    "        except Exception:\n",
    "            return (0.0,)\n",
    "    \n",
    "    toolbox.register(\"evaluate\", evaluate)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "    toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n",
    "    toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n",
    "    \n",
    "    # Limit tree depth\n",
    "    toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=10))\n",
    "    toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=10))\n",
    "    \n",
    "    # Run evolution\n",
    "    pop = toolbox.population(n=n_pop)\n",
    "    hof = tools.HallOfFame(10)\n",
    "    \n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "    \n",
    "    print(\"\\nüöÄ Starting Evolution...\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, \n",
    "                                        ngen=n_gen, stats=stats, halloffame=hof, verbose=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèÜ TOP 5 DISCOVERED FORMULAS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    formulas = []\n",
    "    for i, ind in enumerate(hof[:5]):\n",
    "        formula_str = str(ind)\n",
    "        fitness = ind.fitness.values[0]\n",
    "        print(f\"\\n#{i+1} Fitness: {fitness:.4f}\")\n",
    "        print(f\"   Formula: {formula_str[:100]}...\")\n",
    "        formulas.append({\n",
    "            'formula': formula_str,\n",
    "            'fitness': fitness,\n",
    "            'tree': ind\n",
    "        })\n",
    "    \n",
    "    return formulas, toolbox\n",
    "\n",
    "# Run genetic evolution if data is ready\n",
    "print(\"üß¨ Running Genetic Formula Discovery...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "# Use first 50 features for evolution (to manage complexity)\n",
    "# DON'T overwrite the main feature_names variable!\n",
    "gp_feature_names = [f'f{i}' for i in range(min(50, X.shape[1]))]\n",
    "X_subset = X[:, :50] if X.shape[1] > 50 else X\n",
    "\n",
    "discovered_formulas, gp_toolbox = genetic_formula_evolution(\n",
    "    X_subset, y, gp_feature_names, \n",
    "    n_pop=100,  # 100 formulas in population\n",
    "    n_gen=30    # 30 generations of evolution\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Genetic Evolution Complete!\")\n",
    "print(f\"üß¨ {len(discovered_formulas)} alpha formulas discovered!\")\n",
    "\n",
    "# Log discovered formulas\n",
    "with open('genetic_discoveries_log.json', 'w') as f:\n",
    "    json.dump([{'formula': d['formula'], 'fitness': float(d['fitness'])} for d in discovered_formulas], f, indent=2)\n",
    "print(\"‚úÖ Formulas logged to: genetic_discoveries_log.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d73b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Feature Importance Analysis - What Patterns Matter Most?\n",
    "\n",
    "print(\"\\nüîù TOP 30 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature importance from model\n",
    "model_importance = final_model.feature_importance()\n",
    "n_features = len(model_importance)\n",
    "\n",
    "# Use actual feature names from Cell 6 (stored in prepare_training_data)\n",
    "# If feature_names was overwritten by genetic evolution, recreate from X shape\n",
    "if len(feature_names) != n_features:\n",
    "    print(f\"‚ö†Ô∏è Feature names mismatch ({len(feature_names)} vs {n_features}). Using generic names.\")\n",
    "    actual_feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "else:\n",
    "    actual_feature_names = feature_names\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': actual_feature_names,\n",
    "    'importance': model_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top 30\n",
    "for i, (idx, row) in enumerate(importance.head(30).iterrows()):\n",
    "    bar = \"‚ñà\" * int(row['importance'] / importance['importance'].max() * 20)\n",
    "    print(f\"{i+1:2}. {row['feature']:<35} {row['importance']:6.0f} {bar}\")\n",
    "\n",
    "print(\"\\nüìä Feature Category Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Group features by category (based on naming)\n",
    "categories = {}\n",
    "for idx, row in importance.iterrows():\n",
    "    feat = row['feature'].lower()\n",
    "    if 'ema' in feat:\n",
    "        cat = 'EMA Ribbon'\n",
    "    elif 'rsi' in feat:\n",
    "        cat = 'RSI'\n",
    "    elif 'macd' in feat:\n",
    "        cat = 'MACD'\n",
    "    elif 'bb_' in feat or 'bollinger' in feat:\n",
    "        cat = 'Bollinger Bands'\n",
    "    elif 'vol' in feat or 'obv' in feat:\n",
    "        cat = 'Volume'\n",
    "    elif 'atr' in feat:\n",
    "        cat = 'Volatility'\n",
    "    elif 'rs_vs' in feat:\n",
    "        cat = 'Relative Strength'\n",
    "    elif 'breakout' in feat:\n",
    "        cat = 'Breakout'\n",
    "    elif 'tangle' in feat or 'ribbon' in feat:\n",
    "        cat = 'Visual Patterns'\n",
    "    elif 'sma' in feat:\n",
    "        cat = 'SMA'\n",
    "    elif 'adx' in feat or 'di_' in feat:\n",
    "        cat = 'Trend Strength'\n",
    "    elif 'return' in feat:\n",
    "        cat = 'Returns'\n",
    "    elif 'stoch' in feat:\n",
    "        cat = 'Stochastic'\n",
    "    else:\n",
    "        cat = 'Other'\n",
    "    \n",
    "    if cat not in categories:\n",
    "        categories[cat] = 0\n",
    "    categories[cat] += row['importance']\n",
    "\n",
    "# Sort and display\n",
    "print(\"\\nüìà Category Breakdown:\")\n",
    "for cat, imp in sorted(categories.items(), key=lambda x: -x[1]):\n",
    "    pct = imp / importance['importance'].sum() * 100\n",
    "    bar = \"‚ñì\" * int(pct / 2)\n",
    "    print(f\"{cat:<20} {pct:5.1f}% {bar}\")\n",
    "\n",
    "# === LOGGING RESULTS ===\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù LOGGING RESULTS FOR REVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create results log\n",
    "results_log = {\n",
    "    'top_30_features': importance.head(30).to_dict('records'),\n",
    "    'category_breakdown': {k: float(v) for k, v in categories.items()},\n",
    "    'total_features': n_features,\n",
    "    'top_feature': importance.iloc[0]['feature'],\n",
    "    'top_feature_importance': float(importance.iloc[0]['importance'])\n",
    "}\n",
    "\n",
    "# Save to file for review\n",
    "import json\n",
    "with open('feature_importance_log.json', 'w') as f:\n",
    "    json.dump(results_log, f, indent=2)\n",
    "print(\"‚úÖ Results saved to: feature_importance_log.json\")\n",
    "\n",
    "# Also save full importance CSV\n",
    "importance.to_csv('full_feature_importance.csv', index=False)\n",
    "print(\"‚úÖ Full rankings saved to: full_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Walk-Forward SIMULATION - 20 Folds + ROTATION ANALYSIS\n",
    "# ROBUST: More folds = more confidence in results\n",
    "# ROTATION: Analyzes which tickers win in which market conditions\n",
    "\n",
    "print(\"\\nüí∞ WALK-FORWARD TRADING SIMULATION (20 x 1-WEEK FOLDS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def simulate_trading_weekly(X, y, n_splits=20, top_pct=3):\n",
    "    \"\"\"\n",
    "    Walk-forward simulation with 20 weekly folds:\n",
    "    - Train on past data only\n",
    "    - Test on 1-week chunks (5 trading days)\n",
    "    - No data leakage!\n",
    "    - More folds = more robust validation\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    all_trades = []\n",
    "    fold_results = []\n",
    "    \n",
    "    print(f\"üîÑ Running {n_splits} fold walk-forward simulation...\")\n",
    "    print(f\"   Each fold = ~1 week of out-of-sample testing\")\n",
    "    print(f\"   Training fresh model each fold (no peeking!)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train fresh model on training data only\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        fold_model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=300,\n",
    "            callbacks=[lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # Predict on TEST data (never seen during training)\n",
    "        test_pred = fold_model.predict(X_test)\n",
    "        \n",
    "        # Take top N% highest confidence predictions\n",
    "        n_signals = max(1, len(test_pred) * top_pct // 100)\n",
    "        top_indices = np.argsort(test_pred)[::-1][:n_signals]\n",
    "        \n",
    "        # Record trades\n",
    "        fold_wins = 0\n",
    "        for idx in top_indices:\n",
    "            is_win = y_test[idx] == 1\n",
    "            if is_win:\n",
    "                fold_wins += 1\n",
    "            all_trades.append({\n",
    "                'fold': fold,\n",
    "                'confidence': float(test_pred[idx]),\n",
    "                'actual': int(y_test[idx]),\n",
    "                'win': is_win\n",
    "            })\n",
    "        \n",
    "        fold_wr = fold_wins / len(top_indices) * 100 if top_indices.size > 0 else 0\n",
    "        fold_results.append({\n",
    "            'fold': fold, \n",
    "            'signals': len(top_indices), \n",
    "            'wins': fold_wins, \n",
    "            'wr': fold_wr,\n",
    "            'test_size': len(test_idx)\n",
    "        })\n",
    "        \n",
    "        # Print every 4 folds to keep output manageable\n",
    "        if fold % 4 == 0 or fold == 1:\n",
    "            print(f\"Fold {fold:2d}: {len(top_indices):3d} signals, {fold_wins:3d} wins ({fold_wr:5.1f}% WR) | Test size: {len(test_idx):,}\")\n",
    "    \n",
    "    return all_trades, fold_results\n",
    "\n",
    "# Run the 20-fold simulation\n",
    "trades, fold_results = simulate_trading_weekly(X, y, n_splits=20, top_pct=3)\n",
    "\n",
    "# Calculate overall statistics\n",
    "wins = sum(1 for t in trades if t['win'])\n",
    "total = len(trades)\n",
    "win_rate = wins / total * 100 if total > 0 else 0\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nüìä OVERALL SIMULATION RESULTS (20 FOLDS):\")\n",
    "print(f\"   Total signals (top 3%): {total}\")\n",
    "print(f\"   Wins (>2% in 5 days): {wins}\")\n",
    "print(f\"   Win Rate: {win_rate:.1f}%\")\n",
    "\n",
    "# Fold-by-fold consistency check\n",
    "fold_wrs = [f['wr'] for f in fold_results]\n",
    "print(f\"\\nüìà CONSISTENCY CHECK:\")\n",
    "print(f\"   Best fold:  {max(fold_wrs):.1f}%\")\n",
    "print(f\"   Worst fold: {min(fold_wrs):.1f}%\")\n",
    "print(f\"   Std Dev:    {np.std(fold_wrs):.1f}%\")\n",
    "print(f\"   Folds > 50% WR: {sum(1 for wr in fold_wrs if wr > 50)}/20\")\n",
    "print(f\"   Folds > 60% WR: {sum(1 for wr in fold_wrs if wr > 60)}/20\")\n",
    "print(f\"   Folds > 70% WR: {sum(1 for wr in fold_wrs if wr > 70)}/20\")\n",
    "\n",
    "# Win rate by confidence level\n",
    "print(\"\\nüìà WIN RATE BY CONFIDENCE LEVEL:\")\n",
    "for threshold in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    conf_trades = [t for t in trades if t['confidence'] > threshold]\n",
    "    if conf_trades:\n",
    "        conf_wins = sum(1 for t in conf_trades if t['win'])\n",
    "        conf_wr = conf_wins / len(conf_trades) * 100\n",
    "        stars = \"‚≠ê\" * int(conf_wr / 20)\n",
    "        print(f\"   >{threshold*100:.0f}% confidence: {len(conf_trades):4d} signals, {conf_wr:5.1f}% win rate {stars}\")\n",
    "\n",
    "# ROTATION STRATEGY ANALYSIS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ ROTATION STRATEGY INSIGHTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate rotation with 2 day trades per week constraint\n",
    "print(\"\\nüìã ROBINHOOD-COMPLIANT ROTATION (2 day trades / 7 days):\")\n",
    "high_conf_trades = [t for t in trades if t['confidence'] > 0.7]\n",
    "if len(high_conf_trades) >= 2:\n",
    "    # Take only top 2 per fold (simulating weekly constraint)\n",
    "    constrained_trades = []\n",
    "    for fold in range(1, 21):\n",
    "        fold_trades = sorted([t for t in high_conf_trades if t['fold'] == fold], \n",
    "                            key=lambda x: -x['confidence'])[:2]\n",
    "        constrained_trades.extend(fold_trades)\n",
    "    \n",
    "    constrained_wins = sum(1 for t in constrained_trades if t['win'])\n",
    "    constrained_wr = constrained_wins / len(constrained_trades) * 100 if constrained_trades else 0\n",
    "    print(f\"   Trades (2/week max): {len(constrained_trades)}\")\n",
    "    print(f\"   Wins: {constrained_wins}\")\n",
    "    print(f\"   Win Rate: {constrained_wr:.1f}%\")\n",
    "\n",
    "# Expected value calculation\n",
    "avg_win = 0.02  # Target is 2% gain\n",
    "avg_loss = -0.01  # Assume 1% average loss with stop loss\n",
    "expected_value = (win_rate/100 * avg_win) + ((100-win_rate)/100 * avg_loss)\n",
    "print(f\"\\nüí∞ EXPECTED VALUE per trade: {expected_value*100:.2f}%\")\n",
    "print(f\"   (Assuming 2% wins, 1% losses with stops)\")\n",
    "\n",
    "# Annual projection\n",
    "trades_per_year = 52 * 2  # 2 trades per week\n",
    "annual_return = expected_value * trades_per_year * 100\n",
    "print(f\"\\nüìà PROJECTED ANNUAL RETURN: {annual_return:.0f}%\")\n",
    "print(f\"   (Based on {trades_per_year} trades/year @ {expected_value*100:.2f}% per trade)\")\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if win_rate >= 70 and np.std(fold_wrs) < 15:\n",
    "    print(\"üèÜ EXCEPTIONAL: Very high win rate + consistent!\")\n",
    "    print(\"   This model is READY for live rotation trading!\")\n",
    "elif win_rate >= 60 and np.std(fold_wrs) < 20:\n",
    "    print(\"ü•á EXCELLENT: High win rate + reasonably consistent\")\n",
    "    print(\"   Model is ready - use >70% confidence for trades\")\n",
    "elif win_rate >= 55:\n",
    "    print(\"‚úÖ GOOD: Positive edge detected\")\n",
    "    print(\"   Consider >80% confidence threshold only\")\n",
    "elif expected_value > 0:\n",
    "    print(\"‚ö†Ô∏è MARGINAL: Slight edge, use with caution\")\n",
    "    print(\"   Only trade >90% confidence signals\")\n",
    "else:\n",
    "    print(\"‚ùå NEEDS WORK: No clear edge detected\")\n",
    "    print(\"   Try different target threshold or more data\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Save Model & Discoveries TO GOOGLE DRIVE\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "print(\"\\nüíæ SAVING MODEL & DISCOVERIES TO GOOGLE DRIVE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Google Drive model directory\n",
    "MODEL_DIR = \"/content/drive/MyDrive/quantum-trader-models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Also save locally\n",
    "LOCAL_DIR = \"/content/quantum-ai-trader/models\"\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# Save LightGBM model\n",
    "model_filename = 'ultimate_ai_model.txt'\n",
    "local_model_path = f'{LOCAL_DIR}/{model_filename}'\n",
    "drive_model_path = f'{MODEL_DIR}/{model_filename}'\n",
    "\n",
    "final_model.save_model(local_model_path)\n",
    "shutil.copy(local_model_path, drive_model_path)\n",
    "print(f\"‚úÖ Model saved: {drive_model_path}\")\n",
    "\n",
    "# Save discovered formulas\n",
    "if 'discovered_formulas' in dir():\n",
    "    formulas_data = []\n",
    "    for f in discovered_formulas:\n",
    "        formulas_data.append({\n",
    "            'formula': str(f['formula']),\n",
    "            'fitness': float(f['fitness'])\n",
    "        })\n",
    "    \n",
    "    with open(f'{MODEL_DIR}/discovered_formulas.json', 'w') as f:\n",
    "        json.dump(formulas_data, f, indent=2)\n",
    "    print(f\"‚úÖ Discovered formulas saved to Drive\")\n",
    "\n",
    "# Save feature importance\n",
    "importance.to_csv(f'{MODEL_DIR}/feature_importance.csv', index=False)\n",
    "print(f\"‚úÖ Feature importance saved to Drive\")\n",
    "\n",
    "# Save training summary with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "summary = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_id': timestamp,\n",
    "    'tickers': list(all_data.keys()),\n",
    "    'total_samples': int(len(X)),\n",
    "    'total_features': int(X.shape[1]),\n",
    "    'target_days': TARGET_DAYS,\n",
    "    'target_threshold': TARGET_THRESHOLD,\n",
    "    'walk_forward_auc': float(np.mean([s['auc'] for s in fold_scores])),\n",
    "    'walk_forward_acc': float(np.mean([s['acc'] for s in fold_scores])),\n",
    "    'simulation_win_rate': float(win_rate) if 'win_rate' in dir() else None,\n",
    "    'top_features': importance.head(20).to_dict('records')\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_DIR}/training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"‚úÖ Training summary saved to Drive\")\n",
    "\n",
    "# Also save a timestamped backup\n",
    "backup_dir = f'{MODEL_DIR}/backups/{timestamp}'\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "shutil.copy(local_model_path, f'{backup_dir}/{model_filename}')\n",
    "with open(f'{backup_dir}/training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"‚úÖ Backup saved: {backup_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ ALL ARTIFACTS SAVED TO GOOGLE DRIVE!\")\n",
    "print(f\"\\nüìÅ Google Drive location:\")\n",
    "print(f\"   {MODEL_DIR}/\")\n",
    "print(f\"\\nüì• Files saved:\")\n",
    "print(f\"   - ultimate_ai_model.txt (LightGBM model)\")\n",
    "print(f\"   - discovered_formulas.json (Genetic discoveries)\")\n",
    "print(f\"   - feature_importance.csv (Feature rankings)\")\n",
    "print(f\"   - training_summary.json (Full summary)\")\n",
    "print(f\"\\nüí° TIP: These files are now in your Google Drive!\")\n",
    "print(f\"   Access them anytime at: My Drive > quantum-trader-models\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867a7a0",
   "metadata": {},
   "source": [
    "# üöÄ ULTIMATE AI TRADING DISCOVERY SYSTEM - COMPLETE!\n",
    "\n",
    "## What This Notebook Does:\n",
    "\n",
    "### 1. **100+ Technical Indicators** (Cell 3)\n",
    "- EMA Ribbons (12 periods) - detect tangles and expansions\n",
    "- RSI with divergence detection (6 periods)\n",
    "- MACD with histogram analysis\n",
    "- Bollinger Bands (3 period sets)\n",
    "- Volume analysis (OBV, AD, MFI)\n",
    "- ATR volatility (multiple periods)\n",
    "- ADX trend strength\n",
    "- Price action patterns\n",
    "- Custom combinations\n",
    "\n",
    "### 2. **Visual Pattern Discovery** (Cell 5.5)\n",
    "- EMA ribbon tangle detection\n",
    "- Breakout pattern recognition\n",
    "- Candlestick shape analysis\n",
    "- Trend slope calculation\n",
    "\n",
    "### 3. **Sector Rotation Intelligence** (Cell 6)\n",
    "- Relative strength vs sector ETFs\n",
    "- Relative strength vs SPY\n",
    "- Cross-ticker correlations\n",
    "\n",
    "### 4. **Genetic Formula Evolution** (Cell 7.5)\n",
    "- AI discovers NEW indicator combinations\n",
    "- 100 formulas √ó 30 generations of evolution\n",
    "- Uncovers patterns humans haven't thought of\n",
    "\n",
    "### 5. **Walk-Forward Validation** (Cell 7-9)\n",
    "- Time-series proper backtesting\n",
    "- No look-ahead bias\n",
    "- Realistic trading simulation\n",
    "\n",
    "---\n",
    "\n",
    "## üì• Files to Download:\n",
    "1. `ultimate_ai_model.txt` - Trained LightGBM model\n",
    "2. `discovered_formulas.json` - Genetic algorithm discoveries\n",
    "3. `feature_importance.csv` - What patterns matter most\n",
    "4. `training_summary.json` - Full training report\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps:\n",
    "1. Run this notebook on Colab T4 High-RAM\n",
    "2. Download the trained model\n",
    "3. Use `daily_signal_generator.py` for live signals\n",
    "4. Iterate on discovered formulas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
