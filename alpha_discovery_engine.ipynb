{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install Dependencies (Colab-Compatible)\n",
    "# TA-Lib requires system-level installation on Colab\n",
    "!wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
    "!tar -xzf ta-lib-0.4.0-src.tar.gz\n",
    "%cd ta-lib/\n",
    "!./configure --prefix=/usr > /dev/null 2>&1\n",
    "!make > /dev/null 2>&1\n",
    "!make install > /dev/null 2>&1\n",
    "%cd ..\n",
    "!rm -rf ta-lib ta-lib-0.4.0-src.tar.gz\n",
    "\n",
    "# Now install Python packages\n",
    "!pip install -q yfinance pyts deap gymnasium TA-Lib\n",
    "\n",
    "print(\"âœ“ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1db4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from pyts.image import GramianAngularField\n",
    "from deap import base, creator, tools, gp, algorithms\n",
    "import operator\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "print(\"âœ“ All libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Stage 1: ULTIMATE Data Preparation (50+ Indicators)\n",
    "# 1. Get data (10 years minimum for robust patterns)\n",
    "print(\"Loading data...\")\n",
    "ticker = \"SPY\"  # or your asset\n",
    "df = yf.download(ticker, start=\"2014-01-01\", end=\"2024-12-31\", progress=False)\n",
    "\n",
    "# Handle MultiIndex columns if present (yfinance update)\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(df)} days of data\")\n",
    "\n",
    "# 2. ULTIMATE Feature Engineering - 50+ Indicators\n",
    "if len(df) > 200:\n",
    "    close = df['Close'].values.astype(float)\n",
    "    high = df['High'].values.astype(float)\n",
    "    low = df['Low'].values.astype(float)\n",
    "    open_price = df['Open'].values.astype(float)\n",
    "    volume = df['Volume'].values.astype(float)\n",
    "    \n",
    "    # =============================================\n",
    "    # === EMA RIBBON (8 EMAs for trend clarity) ===\n",
    "    # =============================================\n",
    "    df['EMA8'] = talib.EMA(close, timeperiod=8)\n",
    "    df['EMA13'] = talib.EMA(close, timeperiod=13)\n",
    "    df['EMA21'] = talib.EMA(close, timeperiod=21)\n",
    "    df['EMA34'] = talib.EMA(close, timeperiod=34)\n",
    "    df['EMA55'] = talib.EMA(close, timeperiod=55)\n",
    "    df['EMA89'] = talib.EMA(close, timeperiod=89)\n",
    "    df['EMA144'] = talib.EMA(close, timeperiod=144)\n",
    "    df['EMA200'] = talib.EMA(close, timeperiod=200)\n",
    "    \n",
    "    # EMA Ribbon Spread (compression = breakout coming)\n",
    "    df['EMA_Ribbon_Width'] = (df['EMA8'] - df['EMA200']) / df['Close'] * 100\n",
    "    df['EMA_Fast_Slow_Ratio'] = df['EMA8'] / df['EMA55']\n",
    "    df['EMA_Trend_Strength'] = (df['EMA8'] > df['EMA21']).astype(int) + \\\n",
    "                               (df['EMA21'] > df['EMA34']).astype(int) + \\\n",
    "                               (df['EMA34'] > df['EMA55']).astype(int) + \\\n",
    "                               (df['EMA55'] > df['EMA89']).astype(int)\n",
    "    \n",
    "    # =============================================\n",
    "    # === MOVING AVERAGES ===\n",
    "    # =============================================\n",
    "    df['SMA5'] = talib.SMA(close, timeperiod=5)\n",
    "    df['SMA10'] = talib.SMA(close, timeperiod=10)\n",
    "    df['SMA20'] = talib.SMA(close, timeperiod=20)\n",
    "    df['SMA50'] = talib.SMA(close, timeperiod=50)\n",
    "    df['SMA100'] = talib.SMA(close, timeperiod=100)\n",
    "    df['SMA200'] = talib.SMA(close, timeperiod=200)\n",
    "    \n",
    "    # Price vs MAs (where price sits relative to key levels)\n",
    "    df['Price_vs_SMA20'] = (df['Close'] - df['SMA20']) / df['SMA20'] * 100\n",
    "    df['Price_vs_SMA50'] = (df['Close'] - df['SMA50']) / df['SMA50'] * 100\n",
    "    df['Price_vs_SMA200'] = (df['Close'] - df['SMA200']) / df['SMA200'] * 100\n",
    "    \n",
    "    # =============================================\n",
    "    # === MOMENTUM INDICATORS ===\n",
    "    # =============================================\n",
    "    df['RSI'] = talib.RSI(close, timeperiod=14)\n",
    "    df['RSI_Fast'] = talib.RSI(close, timeperiod=7)\n",
    "    df['RSI_Slow'] = talib.RSI(close, timeperiod=21)\n",
    "    df['RSI_Divergence'] = df['RSI_Fast'] - df['RSI_Slow']\n",
    "    \n",
    "    # Stochastic\n",
    "    df['STOCH_K'], df['STOCH_D'] = talib.STOCH(high, low, close, \n",
    "                                               fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "    df['STOCH_Cross'] = df['STOCH_K'] - df['STOCH_D']\n",
    "    \n",
    "    # Williams %R\n",
    "    df['WILLR'] = talib.WILLR(high, low, close, timeperiod=14)\n",
    "    \n",
    "    # CCI - Commodity Channel Index\n",
    "    df['CCI'] = talib.CCI(high, low, close, timeperiod=20)\n",
    "    df['CCI_Fast'] = talib.CCI(high, low, close, timeperiod=10)\n",
    "    \n",
    "    # Ultimate Oscillator\n",
    "    df['ULTOSC'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "    \n",
    "    # ROC - Rate of Change\n",
    "    df['ROC'] = talib.ROC(close, timeperiod=10)\n",
    "    df['ROC_Fast'] = talib.ROC(close, timeperiod=5)\n",
    "    df['ROC_Slow'] = talib.ROC(close, timeperiod=20)\n",
    "    \n",
    "    # Momentum\n",
    "    df['MOM'] = talib.MOM(close, timeperiod=10)\n",
    "    \n",
    "    # =============================================\n",
    "    # === MACD (Multiple Timeframes) ===\n",
    "    # =============================================\n",
    "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = talib.MACD(close, \n",
    "                                                                 fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df['MACD_Fast'], df['MACD_Fast_Signal'], df['MACD_Fast_Hist'] = talib.MACD(close, \n",
    "                                                                                fastperiod=8, slowperiod=17, signalperiod=9)\n",
    "    \n",
    "    # =============================================\n",
    "    # === VOLATILITY INDICATORS ===\n",
    "    # =============================================\n",
    "    df['ATR'] = talib.ATR(high, low, close, timeperiod=14)\n",
    "    df['ATR_Fast'] = talib.ATR(high, low, close, timeperiod=7)\n",
    "    df['ATR_Percent'] = df['ATR'] / df['Close'] * 100\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = talib.BBANDS(close, timeperiod=20)\n",
    "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle'] * 100\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'] + 1e-8)\n",
    "    \n",
    "    # Keltner Channels (approximation)\n",
    "    df['KC_Middle'] = df['EMA21']\n",
    "    df['KC_Upper'] = df['EMA21'] + 2 * df['ATR']\n",
    "    df['KC_Lower'] = df['EMA21'] - 2 * df['ATR']\n",
    "    df['KC_Position'] = (df['Close'] - df['KC_Lower']) / (df['KC_Upper'] - df['KC_Lower'] + 1e-8)\n",
    "    \n",
    "    # Squeeze (BB inside KC = compression)\n",
    "    df['Squeeze'] = ((df['BB_Lower'] > df['KC_Lower']) & (df['BB_Upper'] < df['KC_Upper'])).astype(int)\n",
    "    \n",
    "    # Historical Volatility\n",
    "    df['HV_10'] = df['Close'].pct_change().rolling(10).std() * np.sqrt(252) * 100\n",
    "    df['HV_20'] = df['Close'].pct_change().rolling(20).std() * np.sqrt(252) * 100\n",
    "    df['HV_Ratio'] = df['HV_10'] / (df['HV_20'] + 1e-8)\n",
    "    \n",
    "    # =============================================\n",
    "    # === TREND INDICATORS ===\n",
    "    # =============================================\n",
    "    # ADX - Trend Strength\n",
    "    df['ADX'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "    df['PLUS_DI'] = talib.PLUS_DI(high, low, close, timeperiod=14)\n",
    "    df['MINUS_DI'] = talib.MINUS_DI(high, low, close, timeperiod=14)\n",
    "    df['DI_Spread'] = df['PLUS_DI'] - df['MINUS_DI']\n",
    "    \n",
    "    # Aroon\n",
    "    df['AROON_Up'], df['AROON_Down'] = talib.AROON(high, low, timeperiod=14)\n",
    "    df['AROON_Osc'] = df['AROON_Up'] - df['AROON_Down']\n",
    "    \n",
    "    # Parabolic SAR\n",
    "    df['SAR'] = talib.SAR(high, low, acceleration=0.02, maximum=0.2)\n",
    "    df['SAR_Signal'] = np.where(df['Close'] > df['SAR'], 1, -1)\n",
    "    \n",
    "    # =============================================\n",
    "    # === VOLUME INDICATORS ===\n",
    "    # =============================================\n",
    "    df['Volume_MA'] = df['Volume'].rolling(20).mean()\n",
    "    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']\n",
    "    \n",
    "    # OBV - On Balance Volume\n",
    "    df['OBV'] = talib.OBV(close, volume)\n",
    "    df['OBV_EMA'] = talib.EMA(df['OBV'].values, timeperiod=20)\n",
    "    df['OBV_Trend'] = df['OBV'] - df['OBV_EMA']\n",
    "    \n",
    "    # AD - Accumulation/Distribution\n",
    "    df['AD'] = talib.AD(high, low, close, volume)\n",
    "    df['AD_EMA'] = talib.EMA(df['AD'].values, timeperiod=20)\n",
    "    \n",
    "    # MFI - Money Flow Index\n",
    "    df['MFI'] = talib.MFI(high, low, close, volume, timeperiod=14)\n",
    "    \n",
    "    # VWAP approximation (rolling)\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).rolling(20).sum() / df['Volume'].rolling(20).sum()\n",
    "    df['Price_vs_VWAP'] = (df['Close'] - df['VWAP']) / df['VWAP'] * 100\n",
    "    \n",
    "    # =============================================\n",
    "    # === CANDLESTICK PATTERNS ===\n",
    "    # =============================================\n",
    "    df['CDL_DOJI'] = talib.CDLDOJI(open_price, high, low, close)\n",
    "    df['CDL_HAMMER'] = talib.CDLHAMMER(open_price, high, low, close)\n",
    "    df['CDL_ENGULFING'] = talib.CDLENGULFING(open_price, high, low, close)\n",
    "    df['CDL_MORNINGSTAR'] = talib.CDLMORNINGSTAR(open_price, high, low, close)\n",
    "    df['CDL_EVENINGSTAR'] = talib.CDLEVENINGSTAR(open_price, high, low, close)\n",
    "    df['CDL_3WHITESOLDIERS'] = talib.CDL3WHITESOLDIERS(open_price, high, low, close)\n",
    "    df['CDL_3BLACKCROWS'] = talib.CDL3BLACKCROWS(open_price, high, low, close)\n",
    "    df['CDL_MARUBOZU'] = talib.CDLMARUBOZU(open_price, high, low, close)\n",
    "    \n",
    "    # =============================================\n",
    "    # === PRICE ACTION ===\n",
    "    # =============================================\n",
    "    df['Body_Size'] = abs(df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    df['Upper_Shadow'] = (df['High'] - df[['Open', 'Close']].max(axis=1)) / df['Open'] * 100\n",
    "    df['Lower_Shadow'] = (df[['Open', 'Close']].min(axis=1) - df['Low']) / df['Open'] * 100\n",
    "    df['Range'] = (df['High'] - df['Low']) / df['Low'] * 100\n",
    "    \n",
    "    # Higher Highs / Lower Lows\n",
    "    df['HH'] = (df['High'] > df['High'].shift(1)).astype(int)\n",
    "    df['LL'] = (df['Low'] < df['Low'].shift(1)).astype(int)\n",
    "    df['HH_Count'] = df['HH'].rolling(5).sum()\n",
    "    df['LL_Count'] = df['LL'].rolling(5).sum()\n",
    "    \n",
    "    # Gap Analysis\n",
    "    df['Gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1) * 100\n",
    "    df['Gap_Up'] = (df['Gap'] > 0.5).astype(int)\n",
    "    df['Gap_Down'] = (df['Gap'] < -0.5).astype(int)\n",
    "    \n",
    "    # =============================================\n",
    "    # === MULTI-TIMEFRAME FEATURES ===\n",
    "    # =============================================\n",
    "    # Returns at different horizons\n",
    "    df['Return_1d'] = df['Close'].pct_change(1)\n",
    "    df['Return_3d'] = df['Close'].pct_change(3)\n",
    "    df['Return_5d'] = df['Close'].pct_change(5)\n",
    "    df['Return_10d'] = df['Close'].pct_change(10)\n",
    "    df['Return_20d'] = df['Close'].pct_change(20)\n",
    "    \n",
    "    # Rolling max/min (support/resistance)\n",
    "    df['High_20d'] = df['High'].rolling(20).max()\n",
    "    df['Low_20d'] = df['Low'].rolling(20).min()\n",
    "    df['Position_in_Range'] = (df['Close'] - df['Low_20d']) / (df['High_20d'] - df['Low_20d'] + 1e-8)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "print(f\"âœ“ ULTIMATE Indicators computed: {len([c for c in df.columns if c not in ['Open','High','Low','Close','Volume']])} features\")\n",
    "print(f\"  Remaining rows: {len(df)}\")\n",
    "\n",
    "# List all features\n",
    "feature_categories = {\n",
    "    'EMA Ribbon': ['EMA8', 'EMA13', 'EMA21', 'EMA34', 'EMA55', 'EMA89', 'EMA144', 'EMA200', \n",
    "                   'EMA_Ribbon_Width', 'EMA_Fast_Slow_Ratio', 'EMA_Trend_Strength'],\n",
    "    'Moving Averages': ['SMA5', 'SMA10', 'SMA20', 'SMA50', 'SMA100', 'SMA200', \n",
    "                        'Price_vs_SMA20', 'Price_vs_SMA50', 'Price_vs_SMA200'],\n",
    "    'Momentum': ['RSI', 'RSI_Fast', 'RSI_Slow', 'RSI_Divergence', 'STOCH_K', 'STOCH_D', 'STOCH_Cross',\n",
    "                 'WILLR', 'CCI', 'CCI_Fast', 'ULTOSC', 'ROC', 'ROC_Fast', 'ROC_Slow', 'MOM'],\n",
    "    'MACD': ['MACD', 'MACD_Signal', 'MACD_Hist', 'MACD_Fast', 'MACD_Fast_Signal', 'MACD_Fast_Hist'],\n",
    "    'Volatility': ['ATR', 'ATR_Fast', 'ATR_Percent', 'BB_Upper', 'BB_Middle', 'BB_Lower', \n",
    "                   'BB_Width', 'BB_Position', 'KC_Position', 'Squeeze', 'HV_10', 'HV_20', 'HV_Ratio'],\n",
    "    'Trend': ['ADX', 'PLUS_DI', 'MINUS_DI', 'DI_Spread', 'AROON_Up', 'AROON_Down', 'AROON_Osc', 'SAR_Signal'],\n",
    "    'Volume': ['Volume_MA', 'Volume_Ratio', 'OBV_Trend', 'MFI', 'Price_vs_VWAP'],\n",
    "    'Candlestick': ['CDL_DOJI', 'CDL_HAMMER', 'CDL_ENGULFING', 'CDL_MORNINGSTAR', 'CDL_EVENINGSTAR',\n",
    "                    'CDL_3WHITESOLDIERS', 'CDL_3BLACKCROWS', 'CDL_MARUBOZU'],\n",
    "    'Price Action': ['Body_Size', 'Upper_Shadow', 'Lower_Shadow', 'Range', 'HH_Count', 'LL_Count', 'Gap'],\n",
    "    'Multi-TF': ['Return_1d', 'Return_3d', 'Return_5d', 'Return_10d', 'Return_20d', 'Position_in_Range']\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š Feature Categories:\")\n",
    "for cat, feats in feature_categories.items():\n",
    "    available = [f for f in feats if f in df.columns]\n",
    "    print(f\"  {cat}: {len(available)} features\")\n",
    "\n",
    "# 3. Create GASF images (visual representation)\n",
    "print(\"\\nCreating GASF images...\")\n",
    "\n",
    "close_series = df['Close'].values.astype(float)\n",
    "returns = np.log(close_series[1:] / close_series[:-1])\n",
    "\n",
    "# IMPORTANT: image_size must be <= window_size\n",
    "window_size = 20\n",
    "image_size = 20  # Must be <= window_size\n",
    "\n",
    "gasf_images = []\n",
    "gasf_indices = []\n",
    "\n",
    "# pyts GASF - image_size must match or be smaller than input length\n",
    "gasf = GramianAngularField(image_size=image_size, method='summation', sample_range=(-1, 1))\n",
    "\n",
    "for i in range(len(returns) - window_size):\n",
    "    window = returns[i:i+window_size]\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    window_min = window.min()\n",
    "    window_max = window.max()\n",
    "    if window_max - window_min > 1e-8:\n",
    "        window_norm = 2 * (window - window_min) / (window_max - window_min) - 1\n",
    "    else:\n",
    "        window_norm = np.zeros_like(window)\n",
    "    \n",
    "    try:\n",
    "        gasf_img = gasf.fit_transform(window_norm.reshape(1, -1))\n",
    "        gasf_images.append(gasf_img[0])\n",
    "        gasf_indices.append(i + window_size)\n",
    "    except Exception as e:\n",
    "        if len(gasf_images) == 0 and i == 0:\n",
    "            print(f\"  Debug - First window error: {e}\")\n",
    "        continue\n",
    "\n",
    "gasf_images = np.array(gasf_images)\n",
    "print(f\"âœ“ Created {len(gasf_images)} GASF images (size: {image_size}x{image_size})\")\n",
    "\n",
    "# 4. Create labels (future returns)\n",
    "df = df.reset_index(drop=True)\n",
    "df['Returns'] = np.nan\n",
    "df.loc[1:len(returns), 'Returns'] = returns\n",
    "df['Future_5d_Return'] = df['Returns'].rolling(5).sum().shift(-5)\n",
    "df['Label'] = (df['Future_5d_Return'] > 0).astype(int)\n",
    "\n",
    "# Align data\n",
    "valid_indices = [idx for idx in gasf_indices if idx < len(df)]\n",
    "aligned_future_returns = df['Future_5d_Return'].iloc[valid_indices].values\n",
    "aligned_gasf_images = gasf_images[:len(valid_indices)]\n",
    "\n",
    "mask = ~np.isnan(aligned_future_returns)\n",
    "aligned_gasf_images = aligned_gasf_images[mask]\n",
    "aligned_future_returns = aligned_future_returns[mask]\n",
    "aligned_indices = np.array(valid_indices)[mask]\n",
    "\n",
    "print(f\"âœ“ Data ready for discovery. Samples: {len(aligned_gasf_images)}\")\n",
    "\n",
    "if len(aligned_gasf_images) == 0:\n",
    "    print(\"\\nâš ï¸ WARNING: No GASF images created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Stage 2: Unsupervised Visual Discovery\n",
    "# Check if data is ready\n",
    "if 'aligned_gasf_images' not in dir() or len(aligned_gasf_images) == 0:\n",
    "    raise RuntimeError(\"âš ï¸ Run Cell 3 (Data Preparation) first!\")\n",
    "\n",
    "class VisualPatternFinder:\n",
    "    \"\"\"Cluster GASF images - find recurring visual structures.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_patterns=10):\n",
    "        self.n_patterns = n_patterns\n",
    "        self.kmeans = None\n",
    "        self.patterns = {}\n",
    "    \n",
    "    def discover_visual_patterns(self, gasf_images, future_returns):\n",
    "        \"\"\"\n",
    "        Cluster images â†’ find which clusters are profitable.\n",
    "        \"\"\"\n",
    "        # Reshape for clustering (flatten images)\n",
    "        n_samples, h, w = gasf_images.shape\n",
    "        X = gasf_images.reshape(n_samples, h*w)\n",
    "        \n",
    "        # Cluster\n",
    "        self.kmeans = KMeans(n_clusters=self.n_patterns, random_state=42, n_init=10)\n",
    "        clusters = self.kmeans.fit_predict(X)\n",
    "        \n",
    "        # Analyze each cluster\n",
    "        print(\"\\nVISUAL PATTERN ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for cluster_id in range(self.n_patterns):\n",
    "            mask = clusters == cluster_id\n",
    "            cluster_returns = future_returns[mask]\n",
    "            \n",
    "            if len(cluster_returns) == 0:\n",
    "                continue\n",
    "\n",
    "            # Statistics\n",
    "            avg_return = np.mean(cluster_returns)\n",
    "            win_rate = (cluster_returns > 0).mean()\n",
    "            frequency = mask.sum() / len(clusters)\n",
    "            std_dev = np.std(cluster_returns) + 1e-8\n",
    "            sharpe = avg_return / std_dev * np.sqrt(252/5)  # Annualized roughly\n",
    "            \n",
    "            print(f\"\\nPattern {cluster_id}:\")\n",
    "            print(f\"  Frequency: {frequency*100:.1f}% of days\")\n",
    "            print(f\"  Next-5d return: {avg_return*100:.2f}%\")\n",
    "            print(f\"  Win rate: {win_rate*100:.1f}%\")\n",
    "            print(f\"  Sharpe: {sharpe:.2f}\")\n",
    "            \n",
    "            if avg_return > 0.005:  # Threshold for \"profitable\"\n",
    "                print(f\"  âœ“ PROFITABLE PATTERN FOUND\")\n",
    "            \n",
    "            self.patterns[cluster_id] = {\n",
    "                'avg_return': avg_return,\n",
    "                'win_rate': win_rate,\n",
    "                'frequency': frequency,\n",
    "                'sharpe': sharpe,\n",
    "                'centroid': self.kmeans.cluster_centers_[cluster_id].reshape(h, w)\n",
    "            }\n",
    "            \n",
    "    def plot_patterns(self):\n",
    "        \"\"\"Visualize the centroids of discovered patterns.\"\"\"\n",
    "        if not self.patterns:\n",
    "            print(\"No patterns discovered yet.\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, (self.n_patterns + 1) // 2, figsize=(15, 6))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (cluster_id, stats) in enumerate(self.patterns.items()):\n",
    "            if i < len(axes):\n",
    "                axes[i].imshow(stats['centroid'], cmap='rainbow', origin='lower')\n",
    "                axes[i].set_title(f\"P{cluster_id}: WR {stats['win_rate']:.2f}\")\n",
    "                axes[i].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Discover patterns\n",
    "finder = VisualPatternFinder(n_patterns=8)\n",
    "finder.discover_visual_patterns(aligned_gasf_images, aligned_future_returns)\n",
    "finder.plot_patterns()\n",
    "\n",
    "print(\"\\nâœ“ DISCOVERED: Which price shapes predict returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4304bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Stage 3: ULTIMATE Rare Structure Detection (Multi-Feature)\n",
    "class UltimateRareStateDetector:\n",
    "    \"\"\"Find anomalous market structures using ALL indicators.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pca = PCA(n_components=0.95)  # Keep 95% variance\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)\n",
    "        self.rare_profiles = {}\n",
    "    \n",
    "    def find_rare_states(self, df, feature_cols=None):\n",
    "        \"\"\"Analyze: What rare states precede big moves?\"\"\"\n",
    "        \n",
    "        # Use ALL numeric features if not specified\n",
    "        if feature_cols is None:\n",
    "            exclude_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Returns', \n",
    "                           'Future_5d_Return', 'Label', 'SAR', 'OBV', 'AD', 'VWAP',\n",
    "                           'High_20d', 'Low_20d', 'BB_Upper', 'BB_Lower', 'BB_Middle',\n",
    "                           'KC_Upper', 'KC_Lower', 'KC_Middle']\n",
    "            feature_cols = [c for c in df.columns if c not in exclude_cols and df[c].dtype in ['float64', 'int64']]\n",
    "        \n",
    "        print(\"\\nRARE STATE ANALYSIS (ULTIMATE)\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Analyzing {len(feature_cols)} features...\")\n",
    "        \n",
    "        data_subset = df[feature_cols].copy()\n",
    "        data_subset.dropna(inplace=True)\n",
    "        \n",
    "        features = data_subset.values\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "        \n",
    "        # Reduce dimensions\n",
    "        features_reduced = self.pca.fit_transform(features)\n",
    "        print(f\"PCA: {features.shape[1]} features â†’ {features_reduced.shape[1]} components (95% variance)\")\n",
    "        \n",
    "        # Find anomalies at different contamination levels\n",
    "        for contamination in [0.02, 0.05, 0.10]:\n",
    "            self.iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            anomalies = self.iso_forest.fit_predict(features_reduced)\n",
    "            anomaly_indices = np.where(anomalies == -1)[0]\n",
    "            \n",
    "            rare_returns = []\n",
    "            for idx in anomaly_indices:\n",
    "                df_idx = data_subset.index[idx]\n",
    "                if df_idx in df.index:\n",
    "                    ret = df.loc[df_idx, 'Future_5d_Return']\n",
    "                    if not np.isnan(ret):\n",
    "                        rare_returns.append(ret)\n",
    "            \n",
    "            rare_returns = np.array(rare_returns)\n",
    "            \n",
    "            if len(rare_returns) > 0:\n",
    "                avg_ret = np.mean(rare_returns)\n",
    "                win_rate = (rare_returns > 0).mean()\n",
    "                \n",
    "                print(f\"\\nðŸ“ Top {contamination*100:.0f}% Rarest States ({len(anomaly_indices)} days):\")\n",
    "                print(f\"   Avg Return (5d): {avg_ret*100:.2f}%\")\n",
    "                print(f\"   Win Rate: {win_rate*100:.1f}%\")\n",
    "                print(f\"   Best: +{np.max(rare_returns)*100:.2f}%  Worst: {np.min(rare_returns)*100:.2f}%\")\n",
    "                \n",
    "                if avg_ret > 0.01:\n",
    "                    print(f\"   âœ“ BULLISH SIGNAL\")\n",
    "                elif avg_ret < -0.01:\n",
    "                    print(f\"   âš ï¸ BEARISH SIGNAL\")\n",
    "                \n",
    "                self.rare_profiles[contamination] = {\n",
    "                    'count': len(anomaly_indices),\n",
    "                    'avg_return': avg_ret,\n",
    "                    'win_rate': win_rate,\n",
    "                    'indices': anomaly_indices\n",
    "                }\n",
    "        \n",
    "        # Identify WHAT makes states rare\n",
    "        print(\"\\nðŸ” Feature Contribution to Rare States:\")\n",
    "        best_contam = max(self.rare_profiles.keys(), key=lambda x: abs(self.rare_profiles[x]['avg_return']))\n",
    "        rare_idx = self.rare_profiles[best_contam]['indices']\n",
    "        \n",
    "        # Compare rare vs normal\n",
    "        rare_features = features[rare_idx]\n",
    "        normal_features = features[~np.isin(np.arange(len(features)), rare_idx)]\n",
    "        \n",
    "        # Find most different features\n",
    "        diffs = []\n",
    "        for i, col in enumerate(feature_cols):\n",
    "            if i < rare_features.shape[1]:\n",
    "                rare_mean = np.mean(rare_features[:, i])\n",
    "                normal_mean = np.mean(normal_features[:, i])\n",
    "                diff = abs(rare_mean - normal_mean)\n",
    "                diffs.append((col, diff, rare_mean, normal_mean))\n",
    "        \n",
    "        diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"   Top discriminating features (rare vs normal):\")\n",
    "        for col, diff, rare_m, norm_m in diffs[:10]:\n",
    "            direction = \"â†‘\" if rare_m > norm_m else \"â†“\"\n",
    "            print(f\"   {direction} {col}: rare={rare_m:.2f} vs normal={norm_m:.2f}\")\n",
    "        \n",
    "        return self.rare_profiles\n",
    "\n",
    "detector = UltimateRareStateDetector()\n",
    "rare_profiles = detector.find_rare_states(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Stage 4: ULTIMATE Symbolic Regression (25 Features!)\n",
    "class UltimateFormulaEvolver:\n",
    "    \"\"\"Genetic programming: evolve trading formulas with 25 key features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Check if creator classes already exist to avoid errors on re-run\n",
    "        if hasattr(creator, \"FitnessMax\"):\n",
    "            del creator.FitnessMax\n",
    "        if hasattr(creator, \"Individual\"):\n",
    "            del creator.Individual\n",
    "            \n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "        creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMax)\n",
    "        \n",
    "        self.toolbox = base.Toolbox()\n",
    "        \n",
    "        # === 25 KEY FEATURES for formula evolution ===\n",
    "        self.feature_names = [\n",
    "            # Momentum (5)\n",
    "            'RSI', 'RSI_Divergence', 'STOCH_K', 'CCI', 'MOM',\n",
    "            # Trend (5)\n",
    "            'ADX', 'DI_Spread', 'AROON_Osc', 'EMA_Trend_Strength', 'SAR_Signal',\n",
    "            # Volatility (5)\n",
    "            'ATR_Percent', 'BB_Position', 'BB_Width', 'Squeeze', 'HV_Ratio',\n",
    "            # Volume (3)\n",
    "            'Volume_Ratio', 'OBV_Trend', 'MFI',\n",
    "            # Price Position (4)\n",
    "            'Price_vs_SMA20', 'Price_vs_SMA200', 'Price_vs_VWAP', 'Position_in_Range',\n",
    "            # Multi-TF (3)\n",
    "            'Return_1d', 'Return_5d', 'Return_20d'\n",
    "        ]\n",
    "        \n",
    "        self.n_features = len(self.feature_names)\n",
    "        self.pset = gp.PrimitiveSet(\"MAIN\", self.n_features)\n",
    "        \n",
    "        # Rename arguments for clarity in output\n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            self.pset.renameArguments(**{f'ARG{i}': name})\n",
    "        \n",
    "        # === OPERATIONS (including advanced ones) ===\n",
    "        self.pset.addPrimitive(operator.add, 2, name='add')\n",
    "        self.pset.addPrimitive(operator.sub, 2, name='sub')\n",
    "        self.pset.addPrimitive(operator.mul, 2, name='mul')\n",
    "        \n",
    "        def protected_div(x, y):\n",
    "            return x / (y + 1e-8)\n",
    "        self.pset.addPrimitive(protected_div, 2, name='div')\n",
    "        \n",
    "        def protected_sqrt(x):\n",
    "            return np.sqrt(np.abs(x))\n",
    "        self.pset.addPrimitive(protected_sqrt, 1, name='sqrt')\n",
    "        \n",
    "        def protected_log(x):\n",
    "            return np.log(np.abs(x) + 1e-8)\n",
    "        self.pset.addPrimitive(protected_log, 1, name='log')\n",
    "        \n",
    "        def protected_exp(x):\n",
    "            return np.clip(np.exp(np.clip(x, -10, 10)), -1e10, 1e10)\n",
    "        self.pset.addPrimitive(protected_exp, 1, name='exp')\n",
    "        \n",
    "        self.pset.addPrimitive(np.sin, 1, name='sin')\n",
    "        self.pset.addPrimitive(np.cos, 1, name='cos')\n",
    "        self.pset.addPrimitive(np.tanh, 1, name='tanh')\n",
    "        self.pset.addPrimitive(np.abs, 1, name='abs')\n",
    "        self.pset.addPrimitive(operator.neg, 1, name='neg')\n",
    "        \n",
    "        # Max/Min for combining signals\n",
    "        def safe_max(x, y):\n",
    "            return np.maximum(x, y)\n",
    "        def safe_min(x, y):\n",
    "            return np.minimum(x, y)\n",
    "        self.pset.addPrimitive(safe_max, 2, name='max')\n",
    "        self.pset.addPrimitive(safe_min, 2, name='min')\n",
    "        \n",
    "        # Conditional (if-then-else approximation)\n",
    "        def if_positive(condition, if_true, if_false):\n",
    "            return np.where(condition > 0, if_true, if_false)\n",
    "        self.pset.addPrimitive(if_positive, 3, name='ifpos')\n",
    "        \n",
    "        # Terminal names (no dots allowed)\n",
    "        self.pset.addTerminal(0.0, name='zero')\n",
    "        self.pset.addTerminal(0.5, name='half')\n",
    "        self.pset.addTerminal(1.0, name='one')\n",
    "        self.pset.addTerminal(2.0, name='two')\n",
    "        self.pset.addTerminal(-1.0, name='neg1')\n",
    "        self.pset.addTerminal(30.0, name='overbought')  # RSI threshold\n",
    "        self.pset.addTerminal(70.0, name='oversold')     # RSI threshold\n",
    "        self.pset.addTerminal(25.0, name='adx_trend')    # ADX threshold\n",
    "        \n",
    "        self.toolbox.register(\"expr\", gp.genHalfAndHalf, pset=self.pset, min_=2, max_=5)\n",
    "        self.toolbox.register(\"individual\", tools.initIterate, creator.Individual, self.toolbox.expr)\n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n",
    "        \n",
    "        self.toolbox.register(\"evaluate\", self.eval_formula)\n",
    "        self.toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "        self.toolbox.register(\"mutate\", gp.mutUniform, expr=self.toolbox.expr, pset=self.pset)\n",
    "        self.toolbox.register(\"select\", tools.selTournament, tournsize=5)  # Increased selection pressure\n",
    "        \n",
    "        # Limit tree height to prevent bloat\n",
    "        self.toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=12))\n",
    "        self.toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=12))\n",
    "    \n",
    "    def eval_formula(self, individual, X, y):\n",
    "        \"\"\"Evaluate formula with multiple metrics.\"\"\"\n",
    "        try:\n",
    "            func = gp.compile(individual, self.pset)\n",
    "            # Unpack all 25 features\n",
    "            pred = func(*[X[:,i] for i in range(self.n_features)])\n",
    "            \n",
    "            if np.isnan(pred).any() or np.isinf(pred).any():\n",
    "                return (-100.0,)\n",
    "            \n",
    "            # Multiple fitness components\n",
    "            correlation = np.corrcoef(pred.flatten(), y.flatten())[0, 1]\n",
    "            if np.isnan(correlation):\n",
    "                return (-100.0,)\n",
    "            \n",
    "            # Sharpe-like metric\n",
    "            pred_sign = np.sign(pred)\n",
    "            strategy_returns = pred_sign * y\n",
    "            if np.std(strategy_returns) > 0:\n",
    "                sharpe = np.mean(strategy_returns) / np.std(strategy_returns)\n",
    "            else:\n",
    "                sharpe = 0\n",
    "            \n",
    "            # Combined fitness: correlation + sharpe contribution\n",
    "            fitness = correlation * 0.6 + np.tanh(sharpe) * 0.4\n",
    "            \n",
    "            return (fitness,)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return (-100.0,)\n",
    "    \n",
    "    def evolve(self, X, y, pop_size=200, generations=30):\n",
    "        \"\"\"Evolve formulas with larger population and more generations.\"\"\"\n",
    "        pop = self.toolbox.population(n=pop_size)\n",
    "        hof = tools.HallOfFame(10)  # Keep top 10\n",
    "        \n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"avg\", np.mean)\n",
    "        stats.register(\"max\", np.max)\n",
    "        \n",
    "        self.toolbox.register(\"evaluate\", self.eval_formula, X=X, y=y)\n",
    "        \n",
    "        pop, log = algorithms.eaSimple(\n",
    "            pop, self.toolbox,\n",
    "            cxpb=0.7, mutpb=0.3,  # Higher mutation for exploration\n",
    "            ngen=generations,\n",
    "            stats=stats,\n",
    "            halloffame=hof,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return hof, log\n",
    "    \n",
    "    def backtest_formula(self, formula, X, y, verbose=True):\n",
    "        \"\"\"Backtest a discovered formula.\"\"\"\n",
    "        func = gp.compile(formula, self.pset)\n",
    "        pred = func(*[X[:,i] for i in range(self.n_features)])\n",
    "        \n",
    "        # Generate signals\n",
    "        signals = np.sign(pred)\n",
    "        returns = signals * y\n",
    "        \n",
    "        # Metrics\n",
    "        total_return = (1 + returns).prod() - 1\n",
    "        win_rate = (returns > 0).sum() / (returns != 0).sum() if (returns != 0).sum() > 0 else 0\n",
    "        sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252/5)\n",
    "        max_dd = (np.maximum.accumulate(np.cumsum(returns)) - np.cumsum(returns)).max()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Total Return: {total_return*100:.2f}%\")\n",
    "            print(f\"  Win Rate: {win_rate*100:.1f}%\")\n",
    "            print(f\"  Sharpe Ratio: {sharpe:.2f}\")\n",
    "            print(f\"  Max Drawdown: {max_dd*100:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'win_rate': win_rate,\n",
    "            'sharpe': sharpe,\n",
    "            'max_dd': max_dd,\n",
    "            'signals': signals\n",
    "        }\n",
    "\n",
    "# === Prepare data for Evolution with 25 features ===\n",
    "print(\"\\nFORMULA EVOLUTION (ULTIMATE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Feature list must match evolver\n",
    "feature_cols = [\n",
    "    'RSI', 'RSI_Divergence', 'STOCH_K', 'CCI', 'MOM',\n",
    "    'ADX', 'DI_Spread', 'AROON_Osc', 'EMA_Trend_Strength', 'SAR_Signal',\n",
    "    'ATR_Percent', 'BB_Position', 'BB_Width', 'Squeeze', 'HV_Ratio',\n",
    "    'Volume_Ratio', 'OBV_Trend', 'MFI',\n",
    "    'Price_vs_SMA20', 'Price_vs_SMA200', 'Price_vs_VWAP', 'Position_in_Range',\n",
    "    'Return_1d', 'Return_5d', 'Return_20d'\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = [f for f in feature_cols if f in df.columns]\n",
    "missing_features = [f for f in feature_cols if f not in df.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"âš ï¸ Missing features: {missing_features}\")\n",
    "    print(\"Using available features only...\")\n",
    "    feature_cols = available_features\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features for formula evolution\")\n",
    "\n",
    "data_subset = df[feature_cols + ['Future_5d_Return']].dropna()\n",
    "\n",
    "X_features = data_subset[feature_cols].values\n",
    "y_returns = data_subset['Future_5d_Return'].values\n",
    "\n",
    "# Normalize features (important for GP)\n",
    "scaler = StandardScaler()\n",
    "X_features = scaler.fit_transform(X_features)\n",
    "\n",
    "# === EVOLVE with SERIOUS parameters ===\n",
    "print(f\"\\nðŸ§¬ Starting Evolution...\")\n",
    "print(f\"   Population: 200\")\n",
    "print(f\"   Generations: 30\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "print()\n",
    "\n",
    "evolver = UltimateFormulaEvolver()\n",
    "best_formulas, log = evolver.evolve(X_features, y_returns, pop_size=200, generations=30)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ† TOP DISCOVERED FORMULAS (with backtests):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, formula in enumerate(best_formulas[:5]):\n",
    "    fitness = formula.fitness.values[0]\n",
    "    print(f\"\\n{i+1}. Fitness: {fitness:.4f}\")\n",
    "    print(f\"   Formula: {str(formula)[:100]}...\")\n",
    "    \n",
    "    # Backtest\n",
    "    evolver.backtest_formula(formula, X_features, y_returns)\n",
    "\n",
    "# Store for later use\n",
    "best_formula = best_formulas[0] if best_formulas else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Stage 5: ULTIMATE RL Environment (20 Feature State)\n",
    "class UltimateTradingEnv(gym.Env):\n",
    "    \"\"\"RL environment with comprehensive market state.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, window=20):\n",
    "        super(UltimateTradingEnv, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.window = window\n",
    "        self.current_idx = window\n",
    "        self.max_idx = len(df) - 5\n",
    "        \n",
    "        # Actions: 0=SHORT, 1=HOLD, 2=LONG\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # === 20 KEY STATE FEATURES ===\n",
    "        self.state_features = [\n",
    "            'RSI', 'RSI_Divergence', 'STOCH_K', 'CCI',\n",
    "            'ADX', 'DI_Spread', 'EMA_Trend_Strength',\n",
    "            'ATR_Percent', 'BB_Position', 'Squeeze',\n",
    "            'Volume_Ratio', 'MFI',\n",
    "            'Price_vs_SMA20', 'Price_vs_SMA200', 'Position_in_Range',\n",
    "            'Return_1d', 'Return_5d', 'MACD_Hist',\n",
    "            'HH_Count', 'LL_Count'\n",
    "        ]\n",
    "        \n",
    "        # Verify features exist\n",
    "        self.state_features = [f for f in self.state_features if f in df.columns]\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(self.state_features),), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Normalize features using the whole dataset\n",
    "        self.scaler = StandardScaler()\n",
    "        valid_data = df[self.state_features].dropna()\n",
    "        self.scaler.fit(valid_data)\n",
    "        \n",
    "        # Track position\n",
    "        self.position = 0  # -1, 0, 1\n",
    "        self.entry_price = 0\n",
    "        self.pnl = 0\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(self.window, max(self.window + 1, self.max_idx - 200))\n",
    "        self.position = 0\n",
    "        self.entry_price = 0\n",
    "        self.pnl = 0\n",
    "        return self._get_state(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price = self.df['Close'].iloc[self.current_idx]\n",
    "        next_price = self.df['Close'].iloc[min(self.current_idx + 1, len(self.df) - 1)]\n",
    "        \n",
    "        # Position mapping: 0=SHORT, 1=HOLD, 2=LONG\n",
    "        new_position = action - 1\n",
    "        \n",
    "        # Calculate reward based on actual position\n",
    "        price_change = (next_price - current_price) / current_price\n",
    "        reward = float(new_position * price_change * 100)  # Percentage PnL\n",
    "        \n",
    "        # Transaction cost\n",
    "        if new_position != self.position:\n",
    "            reward -= 0.01  # 0.01% transaction cost\n",
    "        \n",
    "        self.position = new_position\n",
    "        self.current_idx += 1\n",
    "        \n",
    "        terminated = self.current_idx >= self.max_idx\n",
    "        truncated = False\n",
    "        \n",
    "        return self._get_state(), reward, terminated, truncated, {'pnl': reward}\n",
    "    \n",
    "    def _get_state(self):\n",
    "        idx = min(self.current_idx, len(self.df) - 1)\n",
    "        \n",
    "        state = []\n",
    "        for feat in self.state_features:\n",
    "            val = self.df[feat].iloc[idx]\n",
    "            if np.isnan(val):\n",
    "                val = 0.0\n",
    "            state.append(val)\n",
    "        \n",
    "        state = np.array(state, dtype=np.float32).reshape(1, -1)\n",
    "        state = self.scaler.transform(state).flatten()\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "\n",
    "# === Simple Q-Learning Agent (no deep learning needed) ===\n",
    "class SimpleQAgent:\n",
    "    \"\"\"Tabular Q-learning agent for market pattern discovery.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states=100, n_actions=3, lr=0.1, gamma=0.95, epsilon=0.3):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        self.state_bins = None\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to discrete bin.\"\"\"\n",
    "        if self.state_bins is None:\n",
    "            return 0\n",
    "        \n",
    "        # Use first 3 features for simple discretization\n",
    "        bins = []\n",
    "        for i in range(min(3, len(state))):\n",
    "            bin_idx = np.digitize(state[i], self.state_bins[i]) - 1\n",
    "            bin_idx = np.clip(bin_idx, 0, 4)  # 5 bins per feature\n",
    "            bins.append(bin_idx)\n",
    "        \n",
    "        # Combine into single state index\n",
    "        state_idx = bins[0] * 25 + bins[1] * 5 + bins[2] if len(bins) >= 3 else bins[0]\n",
    "        return min(state_idx, self.n_states - 1)\n",
    "    \n",
    "    def setup_bins(self, states):\n",
    "        \"\"\"Create discretization bins from data.\"\"\"\n",
    "        n_bins = 5\n",
    "        self.state_bins = []\n",
    "        for i in range(min(3, states.shape[1])):\n",
    "            percentiles = np.percentile(states[:, i], np.linspace(0, 100, n_bins + 1)[1:-1])\n",
    "            self.state_bins.append(percentiles)\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        state_idx = self.discretize_state(state)\n",
    "        return np.argmax(self.q_table[state_idx])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-learning update.\"\"\"\n",
    "        state_idx = self.discretize_state(state)\n",
    "        next_state_idx = self.discretize_state(next_state)\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state_idx])\n",
    "        \n",
    "        self.q_table[state_idx, action] += self.lr * (target - self.q_table[state_idx, action])\n",
    "\n",
    "# === Create and Train Agent ===\n",
    "print(\"\\nRL ENVIRONMENT & TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = UltimateTradingEnv(df)\n",
    "print(f\"Observation space: {env.observation_space} ({len(env.state_features)} features)\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"State features: {env.state_features[:5]}... +{len(env.state_features)-5} more\")\n",
    "\n",
    "# Collect sample states for discretization\n",
    "print(\"\\nCollecting state samples...\")\n",
    "sample_states = []\n",
    "for _ in range(100):\n",
    "    state, _ = env.reset()\n",
    "    sample_states.append(state)\n",
    "sample_states = np.array(sample_states)\n",
    "\n",
    "# Create and train agent\n",
    "agent = SimpleQAgent(n_states=125, n_actions=3, lr=0.1, gamma=0.95, epsilon=0.2)\n",
    "agent.setup_bins(sample_states)\n",
    "\n",
    "print(\"\\nðŸ¤– Training Q-Learning Agent (500 episodes)...\")\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(100):  # Max 100 steps per episode\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    # Decay epsilon\n",
    "    agent.epsilon = max(0.05, agent.epsilon * 0.995)\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        print(f\"   Episode {episode+1}: Avg Reward = {avg_reward:.2f}\")\n",
    "\n",
    "# Analyze learned policy\n",
    "print(\"\\nðŸ“Š Learned Policy Analysis:\")\n",
    "print(\"Q-Table Summary (action preferences by market state):\")\n",
    "print(\"   State Region â†’ [SHORT, HOLD, LONG]\")\n",
    "\n",
    "# Show some interesting states\n",
    "for i in range(0, 125, 25):\n",
    "    q_vals = agent.q_table[i]\n",
    "    best_action = ['SHORT', 'HOLD', 'LONG'][np.argmax(q_vals)]\n",
    "    print(f\"   State {i:3d}: {q_vals.round(2)} â†’ {best_action}\")\n",
    "\n",
    "print(\"\\nâœ“ Agent trained and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Stage 6: ULTIMATE Multimodal Fusion\n",
    "class UltimateEnsemble:\n",
    "    \"\"\"Combine all discovery methods with confidence weighting.\"\"\"\n",
    "    \n",
    "    def __init__(self, visual_finder, rare_detector, best_formula, evolver, rl_agent, env):\n",
    "        self.visual = visual_finder\n",
    "        self.rare = rare_detector\n",
    "        self.formula = best_formula\n",
    "        self.evolver = evolver\n",
    "        self.agent = rl_agent\n",
    "        self.env = env\n",
    "        \n",
    "        if self.formula is not None:\n",
    "            self.compiled_formula = gp.compile(self.formula, self.evolver.pset)\n",
    "        else:\n",
    "            self.compiled_formula = None\n",
    "        \n",
    "        # Track performance for confidence weighting\n",
    "        self.method_accuracy = {\n",
    "            'visual': 0.5,\n",
    "            'genetic': 0.5,\n",
    "            'rare': 0.5,\n",
    "            'rl': 0.5\n",
    "        }\n",
    "    \n",
    "    def get_ensemble_signal(self, row_dict, gasf_img=None):\n",
    "        \"\"\"Query all modalities and combine signals.\"\"\"\n",
    "        \n",
    "        votes = {}\n",
    "        confidences = {}\n",
    "        \n",
    "        # 1. Visual Pattern Vote\n",
    "        if gasf_img is not None and self.visual.kmeans is not None:\n",
    "            try:\n",
    "                cluster = self.visual.kmeans.predict(gasf_img.reshape(1, -1))[0]\n",
    "                pattern_stats = self.visual.patterns.get(cluster, {})\n",
    "                avg_ret = pattern_stats.get('avg_return', 0)\n",
    "                votes['visual'] = np.sign(avg_ret)\n",
    "                confidences['visual'] = min(abs(avg_ret) * 100, 1.0)  # Scale confidence\n",
    "            except:\n",
    "                votes['visual'] = 0\n",
    "                confidences['visual'] = 0\n",
    "        else:\n",
    "            votes['visual'] = 0\n",
    "            confidences['visual'] = 0\n",
    "            \n",
    "        # 2. Genetic Formula Vote\n",
    "        if self.compiled_formula is not None:\n",
    "            try:\n",
    "                # Build feature array from row_dict\n",
    "                features = []\n",
    "                for feat in self.evolver.feature_names:\n",
    "                    val = row_dict.get(feat, 0)\n",
    "                    if np.isnan(val):\n",
    "                        val = 0\n",
    "                    features.append(val)\n",
    "                \n",
    "                f_val = self.compiled_formula(*features)\n",
    "                votes['genetic'] = float(np.sign(f_val))\n",
    "                confidences['genetic'] = min(abs(f_val), 1.0)\n",
    "            except:\n",
    "                votes['genetic'] = 0\n",
    "                confidences['genetic'] = 0\n",
    "        else:\n",
    "            votes['genetic'] = 0\n",
    "            confidences['genetic'] = 0\n",
    "            \n",
    "        # 3. Rare State Vote\n",
    "        # Check if current state is rare (use ADX + BB_Width + Volume_Ratio)\n",
    "        try:\n",
    "            adx = row_dict.get('ADX', 25)\n",
    "            bb_width = row_dict.get('BB_Width', 10)\n",
    "            vol_ratio = row_dict.get('Volume_Ratio', 1)\n",
    "            \n",
    "            # Rare = low ADX + narrow bands + high volume (squeeze breakout)\n",
    "            is_squeeze = bb_width < 5 and vol_ratio > 1.5\n",
    "            if is_squeeze:\n",
    "                votes['rare'] = 1  # Squeeze = bullish bias\n",
    "                confidences['rare'] = 0.7\n",
    "            else:\n",
    "                votes['rare'] = 0\n",
    "                confidences['rare'] = 0.3\n",
    "        except:\n",
    "            votes['rare'] = 0\n",
    "            confidences['rare'] = 0\n",
    "            \n",
    "        # 4. RL Agent Vote\n",
    "        if self.agent is not None:\n",
    "            try:\n",
    "                state = []\n",
    "                for feat in self.env.state_features:\n",
    "                    val = row_dict.get(feat, 0)\n",
    "                    if np.isnan(val):\n",
    "                        val = 0\n",
    "                    state.append(val)\n",
    "                state = np.array(state, dtype=np.float32)\n",
    "                \n",
    "                action = self.agent.act(state)\n",
    "                votes['rl'] = action - 1  # Convert to -1, 0, 1\n",
    "                \n",
    "                # Confidence from Q-value spread\n",
    "                state_idx = self.agent.discretize_state(state)\n",
    "                q_vals = self.agent.q_table[state_idx]\n",
    "                q_spread = np.max(q_vals) - np.mean(q_vals)\n",
    "                confidences['rl'] = min(q_spread / 10, 1.0)\n",
    "            except:\n",
    "                votes['rl'] = 0\n",
    "                confidences['rl'] = 0\n",
    "        else:\n",
    "            votes['rl'] = 0\n",
    "            confidences['rl'] = 0\n",
    "        \n",
    "        # Weighted consensus\n",
    "        total_weight = sum(confidences.values())\n",
    "        if total_weight > 0:\n",
    "            weighted_signal = sum(v * confidences[k] for k, v in votes.items()) / total_weight\n",
    "        else:\n",
    "            weighted_signal = 0\n",
    "        \n",
    "        return {\n",
    "            'signal': weighted_signal,\n",
    "            'direction': 'LONG' if weighted_signal > 0.2 else 'SHORT' if weighted_signal < -0.2 else 'NEUTRAL',\n",
    "            'votes': votes,\n",
    "            'confidences': confidences,\n",
    "            'consensus_strength': abs(weighted_signal)\n",
    "        }\n",
    "    \n",
    "    def backtest_ensemble(self, df, gasf_images=None, aligned_indices=None):\n",
    "        \"\"\"Backtest the full ensemble on historical data.\"\"\"\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ ENSEMBLE BACKTEST\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        signals = []\n",
    "        returns = []\n",
    "        \n",
    "        # Sample positions to avoid overfitting\n",
    "        test_indices = range(200, min(len(df)-5, len(df)), 5)  # Every 5th day\n",
    "        \n",
    "        for idx in test_indices:\n",
    "            if idx >= len(df) - 5:\n",
    "                continue\n",
    "                \n",
    "            row = df.iloc[idx].to_dict()\n",
    "            \n",
    "            # Get GASF if available\n",
    "            gasf_img = None\n",
    "            if gasf_images is not None and aligned_indices is not None:\n",
    "                matching = np.where(aligned_indices == idx)[0]\n",
    "                if len(matching) > 0:\n",
    "                    gasf_img = gasf_images[matching[0]]\n",
    "            \n",
    "            result = self.get_ensemble_signal(row, gasf_img)\n",
    "            signal = result['signal']\n",
    "            \n",
    "            # Get actual future return\n",
    "            future_ret = df['Future_5d_Return'].iloc[idx]\n",
    "            if np.isnan(future_ret):\n",
    "                continue\n",
    "            \n",
    "            signals.append(signal)\n",
    "            returns.append(future_ret)\n",
    "        \n",
    "        signals = np.array(signals)\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        # Calculate strategy returns\n",
    "        positions = np.sign(signals)\n",
    "        strategy_returns = positions * returns\n",
    "        \n",
    "        # Metrics\n",
    "        total_return = (1 + strategy_returns).prod() - 1\n",
    "        buy_hold_return = (1 + returns).prod() - 1\n",
    "        win_rate = (strategy_returns > 0).sum() / (strategy_returns != 0).sum() if (strategy_returns != 0).sum() > 0 else 0\n",
    "        \n",
    "        avg_win = strategy_returns[strategy_returns > 0].mean() if (strategy_returns > 0).sum() > 0 else 0\n",
    "        avg_loss = abs(strategy_returns[strategy_returns < 0].mean()) if (strategy_returns < 0).sum() > 0 else 0\n",
    "        profit_factor = avg_win / avg_loss if avg_loss > 0 else 999\n",
    "        \n",
    "        sharpe = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-8) * np.sqrt(252/5)\n",
    "        \n",
    "        print(f\"Strategy Return: {total_return*100:.2f}%\")\n",
    "        print(f\"Buy & Hold Return: {buy_hold_return*100:.2f}%\")\n",
    "        print(f\"Alpha Generated: {(total_return - buy_hold_return)*100:.2f}%\")\n",
    "        print(f\"Win Rate: {win_rate*100:.1f}%\")\n",
    "        print(f\"Profit Factor: {profit_factor:.2f}\")\n",
    "        print(f\"Sharpe Ratio: {sharpe:.2f}\")\n",
    "        print(f\"Trades: {(positions != 0).sum()}\")\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'buy_hold': buy_hold_return,\n",
    "            'alpha': total_return - buy_hold_return,\n",
    "            'win_rate': win_rate,\n",
    "            'sharpe': sharpe\n",
    "        }\n",
    "\n",
    "# Create ensemble with all components\n",
    "print(\"\\nðŸŽ¯ BUILDING ULTIMATE ENSEMBLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ensemble = UltimateEnsemble(\n",
    "    visual_finder=finder,\n",
    "    rare_detector=detector,\n",
    "    best_formula=best_formula,\n",
    "    evolver=evolver,\n",
    "    rl_agent=agent,\n",
    "    env=env\n",
    ")\n",
    "\n",
    "print(\"âœ“ Ensemble integrates:\")\n",
    "print(\"  1. Visual Pattern Recognition (GASF + KMeans)\")\n",
    "print(\"  2. Genetic Formula Discovery (25 features)\")\n",
    "print(\"  3. Rare State Detection (Multi-feature anomaly)\")\n",
    "print(\"  4. RL Agent (Q-Learning policy)\")\n",
    "\n",
    "# Run backtest\n",
    "backtest_results = ensemble.backtest_ensemble(df, aligned_gasf_images, aligned_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9207a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Stage 7: ULTIMATE Summary & Export\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸŽ¯ ALPHA DISCOVERY COMPLETE - ULTIMATE EDITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ“Š WHAT WAS DISCOVERED:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 1. Visual Patterns\n",
    "print(f\"\\n1ï¸âƒ£  VISUAL PATTERNS: {len(finder.patterns)} discovered\")\n",
    "best_pattern = max(finder.patterns.items(), key=lambda x: x[1]['win_rate'])\n",
    "print(f\"    Best: Pattern {best_pattern[0]} â†’ {best_pattern[1]['win_rate']*100:.1f}% win rate\")\n",
    "print(f\"    Sharpe: {best_pattern[1]['sharpe']:.2f}\")\n",
    "\n",
    "# 2. Rare States\n",
    "print(f\"\\n2ï¸âƒ£  RARE STATES:\")\n",
    "for contam, profile in detector.rare_profiles.items():\n",
    "    print(f\"    Top {contam*100:.0f}%: {profile['win_rate']*100:.1f}% WR, {profile['avg_return']*100:.2f}% avg return\")\n",
    "\n",
    "# 3. Evolved Formulas\n",
    "print(f\"\\n3ï¸âƒ£  GENETIC FORMULAS: {len(best_formulas)} evolved\")\n",
    "if best_formula:\n",
    "    print(f\"    Best fitness: {best_formula.fitness.values[0]:.4f}\")\n",
    "    formula_str = str(best_formula)\n",
    "    if len(formula_str) > 80:\n",
    "        formula_str = formula_str[:80] + \"...\"\n",
    "    print(f\"    Formula: {formula_str}\")\n",
    "\n",
    "# 4. RL Agent\n",
    "print(f\"\\n4ï¸âƒ£  RL AGENT: Trained (500 episodes)\")\n",
    "print(f\"    State features: {len(env.state_features)}\")\n",
    "print(f\"    Final epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "# 5. Ensemble Performance\n",
    "print(f\"\\n5ï¸âƒ£  ENSEMBLE BACKTEST:\")\n",
    "print(f\"    Total Return: {backtest_results['total_return']*100:.2f}%\")\n",
    "print(f\"    Buy & Hold: {backtest_results['buy_hold']*100:.2f}%\")\n",
    "print(f\"    Alpha: {backtest_results['alpha']*100:.2f}%\")\n",
    "print(f\"    Win Rate: {backtest_results['win_rate']*100:.1f}%\")\n",
    "print(f\"    Sharpe: {backtest_results['sharpe']:.2f}\")\n",
    "\n",
    "# === SAVE DISCOVERIES ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ’¾ SAVING DISCOVERIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Create discoveries dict\n",
    "discoveries = {\n",
    "    'ticker': ticker,\n",
    "    'date_range': '2014-2024',\n",
    "    'visual_patterns': {\n",
    "        k: {\n",
    "            'avg_return': float(v['avg_return']),\n",
    "            'win_rate': float(v['win_rate']),\n",
    "            'frequency': float(v['frequency']),\n",
    "            'sharpe': float(v['sharpe'])\n",
    "        } for k, v in finder.patterns.items()\n",
    "    },\n",
    "    'rare_states': {\n",
    "        str(k): {\n",
    "            'count': int(v['count']),\n",
    "            'avg_return': float(v['avg_return']),\n",
    "            'win_rate': float(v['win_rate'])\n",
    "        } for k, v in detector.rare_profiles.items()\n",
    "    },\n",
    "    'best_formula': str(best_formula) if best_formula else None,\n",
    "    'best_formula_fitness': float(best_formula.fitness.values[0]) if best_formula else None,\n",
    "    'ensemble_results': {\n",
    "        'total_return': float(backtest_results['total_return']),\n",
    "        'buy_hold': float(backtest_results['buy_hold']),\n",
    "        'alpha': float(backtest_results['alpha']),\n",
    "        'win_rate': float(backtest_results['win_rate']),\n",
    "        'sharpe': float(backtest_results['sharpe'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON summary\n",
    "with open('alpha_discoveries.json', 'w') as f:\n",
    "    json.dump(discoveries, f, indent=2)\n",
    "print(\"âœ“ Saved: alpha_discoveries.json\")\n",
    "\n",
    "# Save models (for production use)\n",
    "models = {\n",
    "    'kmeans': finder.kmeans,\n",
    "    'q_table': agent.q_table,\n",
    "    'state_bins': agent.state_bins,\n",
    "    'scaler': env.scaler,\n",
    "    'state_features': env.state_features\n",
    "}\n",
    "with open('alpha_models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "print(\"âœ“ Saved: alpha_models.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸš€ NEXT STEPS TO FIND BIGGER GAINS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. INCREASE TRAINING INTENSITY:\n",
    "   - Change pop_size=500, generations=100 in Formula Evolution\n",
    "   - Train RL agent for 2000+ episodes\n",
    "   \n",
    "2. TRY DIFFERENT ASSETS:\n",
    "   - QQQ (tech-heavy), IWM (small caps), GLD (gold)\n",
    "   - Individual stocks with high volume\n",
    "\n",
    "3. USE SHORTER PREDICTION HORIZONS:\n",
    "   - Change Future_5d_Return to Future_1d_Return or Future_3d_Return\n",
    "   - This matches your 4.73% daily gain observation\n",
    "\n",
    "4. ADD MORE FEATURES:\n",
    "   - VIX (fear index)\n",
    "   - Sector rotation indicators\n",
    "   - Market breadth (advance/decline)\n",
    "   - Order flow / dark pool data\n",
    "\n",
    "5. MULTI-TIMEFRAME:\n",
    "   - Run discovery on 1-hour, 4-hour, and daily simultaneously\n",
    "   - Combine signals for confirmation\n",
    "\n",
    "6. PRODUCTION DEPLOYMENT:\n",
    "   - Load alpha_models.pkl in your trading system\n",
    "   - Generate signals in real-time\n",
    "   - Use ensemble.get_ensemble_signal(current_row)\n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
