# üî¨ PERPLEXITY RESEARCH: AlphaGo-Style Visual Pattern Discovery for Trading

## üéØ CRITICAL GAP IDENTIFIED

**Current System:** 61.7% win rate using numerical indicators only
**Missing Component:** Visual pattern recognition like AlphaGo analyzes Go board states

**AlphaGo Insight:**
- Doesn't just calculate moves numerically
- **SEES** the board as spatial patterns
- Learns visual features humans never conceived
- Combines policy network (what patterns to look for) + value network (outcome prediction)

**Our Need:**
- Train CNN to **SEE** chart patterns like AlphaGo sees board positions
- Discover visual patterns that correlate with profitable moves
- Go beyond human-defined patterns (head & shoulders, triangles, etc.)
- Let AI discover patterns in the visual structure of price/volume

---

## üìä VISUAL PATTERN DISCOVERY RESEARCH QUESTIONS

### üñºÔ∏è Section 1: GASF/GADF Image Transformation

**Q1.1 ‚Äî Gramian Angular Field Deep Dive**
```
We want to convert OHLCV candlestick data into Gramian Angular Summation Field (GASF) and Gramian Angular Difference Field (GADF) images for CNN training, similar to how AlphaGo views board positions as spatial patterns. 

Specific questions:
1. What's the optimal window size (20, 30, 60 days) for GASF generation in daily stock data?
2. Should we use GASF (summation), GADF (difference), or both stacked as multi-channel input?
3. How do we normalize price data before GASF transformation to handle different stock price ranges?
4. What image resolution (64x64, 128x128, 224x224) balances detail vs training speed?
5. Provide complete Python code using pyts library to generate GASF images from pandas DataFrame

Include code for:
- OHLCV ‚Üí GASF conversion pipeline
- Multi-channel stacking (Open, High, Low, Close, Volume as separate channels)
- Batch generation for training dataset
- Visualization of resulting images
```

**Q1.2 ‚Äî Recurrence Plot Alternative**
```
Besides GASF/GADF, Recurrence Plots (RP) are another way to visualize time series as 2D images. Compare:
- GASF vs GADF vs Recurrence Plots for financial data
- Which captures trend, volatility, and momentum patterns best?
- Can we stack all three as RGB channels for richer pattern representation?
- Provide Python code for generating all three and combining them
```

**Q1.3 ‚Äî Markov Transition Field (MTF)**
```
Markov Transition Field encodes the transition probabilities of quantized time series as images. 
- How does MTF compare to GASF for capturing price action regime changes?
- What quantization strategy (equal-width bins, quantiles, k-means) works best?
- Provide code for MTF generation and comparison with GASF
```

---

### üß† Section 2: CNN Architecture for Financial Image Classification

**Q2.1 ‚Äî ResNet vs EfficientNet vs Vision Transformer**
```
AlphaGo uses deep residual networks. For classifying financial GASF images (224x224 or 128x128) into BUY/HOLD/SELL:

Compare architectures:
1. **ResNet-18/34** (proven for AlphaGo)
2. **EfficientNet-B0/B1** (parameter efficient)
3. **Vision Transformer (ViT)** (attention-based)
4. **Custom shallow CNN** (4-6 layers, faster training)

For each:
- Expected accuracy on financial images
- Training time on T4 GPU (10K images)
- Memory requirements
- Recommended hyperparameters
- PyTorch code implementation

Which architecture discovered the most novel patterns in similar computer vision tasks?
```

**Q2.2 ‚Äî Attention Mechanisms for Chart Patterns**
```
AlphaGo uses attention to focus on critical board regions. For financial charts:

1. **Spatial Attention:** Which parts of the chart are most predictive? (Recent candles? Volume spikes? Support/resistance zones?)
2. **Channel Attention (CBAM):** Which price components matter most? (Close vs Volume vs High-Low range?)
3. **Self-Attention (Transformer blocks):** How do different time periods interact?

Provide PyTorch code for:
- CBAM (Convolutional Block Attention Module) 
- Spatial Attention Module
- Integration into ResNet backbone
- Visualization of attention maps (which chart regions CNN focuses on)
```

**Q2.3 ‚Äî Multi-Scale Pattern Detection**
```
Financial patterns exist at multiple scales (intraday, daily, weekly). AlphaGo uses policy+value networks.

Design CNN architecture with:
1. **Multi-scale input:** Process 1H, 4H, 1D charts simultaneously
2. **Feature pyramid networks:** Detect patterns at multiple resolutions
3. **Temporal attention:** Weight recent vs distant patterns differently

Provide architecture diagram and PyTorch implementation showing:
- How to feed 3 timeframes into one model
- How to fuse multi-scale features
- How to output both action (policy) and expected return (value)
```

---

### üéÆ Section 3: AlphaGo-Style Policy + Value Networks

**Q3.1 ‚Äî Dual Network Architecture for Trading**
```
AlphaGo uses:
- **Policy Network:** Given board state ‚Üí What move to make?
- **Value Network:** Given board state ‚Üí Who's winning?

For trading:
- **Policy Network:** Given chart ‚Üí Should we BUY/HOLD/SELL?
- **Value Network:** Given chart ‚Üí What's the expected return?

Questions:
1. Should we train these jointly (shared backbone) or separately?
2. What loss functions? (Cross-entropy for policy, MSE for value, or combined?)
3. How to handle the explore-exploit tradeoff during training?
4. Provide PyTorch code for dual-head architecture

Include:
- Shared ResNet-18 backbone
- Policy head (softmax over 3 actions)
- Value head (regression for expected 5-day return)
- Joint training loop with combined loss
```

**Q3.2 ‚Äî Monte Carlo Tree Search for Trade Decisions**
```
AlphaGo combines CNN with MCTS for planning. For trading:

1. **State:** Current chart pattern (GASF image)
2. **Actions:** BUY, HOLD, SELL, or position sizing variants
3. **Tree search:** Simulate multiple future scenarios using historical patterns
4. **Rollout:** Use value network to estimate outcomes

Questions:
- Is MCTS feasible for trading? (Market is stochastic, not deterministic)
- How to handle non-stationary market dynamics?
- Can we use MCTS for portfolio optimization instead of single trades?
- Provide simplified MCTS pseudocode adapted for stock trading
```

**Q3.3 ‚Äî Self-Play for Pattern Discovery**
```
AlphaZero improves by playing against itself. For trading:

Concept: Create market simulation where:
1. Multiple AI agents trade against each other
2. They evolve strategies through competition
3. Successful patterns survive, weak ones die

Questions:
1. How to design a realistic market simulator with price impact?
2. How to prevent all agents converging to the same strategy (mode collapse)?
3. Can we use evolutionary algorithms instead of self-play?
4. Provide framework design for multi-agent trading environment
```

---

### üî¨ Section 4: Unsupervised Visual Pattern Discovery

**Q4.1 ‚Äî Contrastive Learning for Chart Patterns (SimCLR)**
```
AlphaGo learns representations WITHOUT labels first. For trading:

Use contrastive learning (SimCLR) to learn meaningful chart representations:
1. Take two augmented views of the same chart (crop, noise, flip)
2. Train CNN to recognize they're the same pattern
3. Use learned features for downstream prediction

Questions:
- What augmentations preserve financial meaning? (Can't flip vertically - reverses trend!)
- How many unlabeled charts needed for good representations?
- After pretraining with contrastive learning, how much labeled data needed for finetuning?
- Provide PyTorch implementation of SimCLR for GASF images

Include:
- Data augmentation pipeline for financial images
- Contrastive loss function (NT-Xent)
- Training loop on unlabeled data
- Feature extraction for downstream tasks
```

**Q4.2 ‚Äî Variational Autoencoder (VAE) for Pattern Generation**
```
Can we train VAE to:
1. Learn latent space of profitable chart patterns
2. Generate synthetic charts that share successful pattern characteristics
3. Interpolate between patterns to discover novel combinations

Questions:
- How many latent dimensions capture chart pattern diversity?
- How to condition VAE on market regime (bull/bear/sideways)?
- Can we use VAE latent space for anomaly detection (unusual profitable patterns)?
- Provide PyTorch VAE implementation for GASF images

Should output:
- Encoder: Chart ‚Üí Latent vector
- Decoder: Latent vector ‚Üí Reconstructed chart
- Loss function (reconstruction + KL divergence)
- Pattern generation and interpolation code
```

**Q4.3 ‚Äî Cluster Analysis of Visual Patterns**
```
After training CNN encoder (from contrastive learning or VAE):
1. Extract embeddings for all historical charts
2. Cluster them (K-means, HDBSCAN) to find pattern archetypes
3. Label each cluster by average future return
4. Use cluster membership as a feature

Questions:
- How many clusters capture meaningful pattern diversity?
- Which clustering algorithm works best for high-dimensional CNN features?
- How to visualize clusters (t-SNE, UMAP)?
- How to name/interpret discovered clusters?
- Provide Python pipeline for pattern clustering and analysis
```

---

### üöÄ Section 5: Advanced Visual Techniques

**Q5.1 ‚Äî GradCAM for Pattern Interpretation**
```
After training CNN on chart images, use Gradient-weighted Class Activation Mapping (GradCAM) to visualize:
- Which parts of the chart CNN focuses on for BUY decision
- Which candles are most important
- Whether CNN learned meaningful patterns or just noise

Questions:
- How to implement GradCAM for financial CNNs?
- Can we use attention maps to extract human-interpretable rules?
- Provide PyTorch code for GradCAM visualization

Should produce:
- Heatmap overlay showing important chart regions
- Comparison across BUY/HOLD/SELL predictions
- Example interpretations
```

**Q5.2 ‚Äî Temporal Convolutional Networks (TCN) for Sequential Patterns**
```
Standard CNNs treat charts as static images. TCN preserves temporal causality:

1. Process candles left-to-right (past ‚Üí present)
2. Use dilated convolutions to capture long-range dependencies
3. Maintain causal ordering (no future leakage)

Questions:
- TCN vs LSTM vs Transformer for sequential financial data?
- Optimal dilation rates for daily stock data?
- How to combine TCN (temporal) with CNN (spatial)?
- Provide PyTorch implementation of TCN for price sequences
```

**Q5.3 ‚Äî 3D CNN for Multi-Timeframe Charts**
```
Instead of flattening timeframes, treat them as 3D volume:
- X-axis: Time
- Y-axis: Price/Volume features
- Z-axis: Timeframe (1H, 4H, 1D stacked)

3D CNN can learn cross-timeframe patterns:

Questions:
- 3D CNN vs separate 2D CNNs per timeframe?
- Training time and memory requirements?
- Have 3D CNNs been tested on financial data?
- Provide PyTorch 3D CNN architecture for multi-timeframe GASF
```

---

### üéØ Section 6: Pattern Evolution & Discovery

**Q6.1 ‚Äî Neural Architecture Search (NAS) for Trading CNNs**
```
Instead of hand-designing CNN architecture, let AI discover optimal network topology:

1. **Search space:** Layer types, kernel sizes, skip connections, attention modules
2. **Search strategy:** Reinforcement learning, evolutionary algorithms, gradient-based (DARTS)
3. **Objective:** Maximize validation Sharpe ratio (not just accuracy)

Questions:
- Is NAS feasible on Colab Pro? (computationally expensive)
- What's the minimal search space for financial CNNs?
- Can we warm-start with ResNet-18 and search for modifications?
- Provide NAS setup using DARTS or AutoKeras
```

**Q6.2 ‚Äî Evolutionary Pattern Discovery**
```
Use genetic algorithms to evolve image transformations that maximize predictive power:

1. **Genome:** Parameters for GASF generation (window size, normalization, channels)
2. **Mutation:** Try different image preprocessing techniques
3. **Fitness:** Validation accuracy of CNN trained on those images
4. **Evolution:** Keep best image representations over generations

Questions:
- How many generations needed to converge?
- Population size and mutation rate?
- Can we evolve data augmentation strategies too?
- Provide Python implementation using DEAP library
```

**Q6.3 ‚Äî Meta-Learning for Fast Pattern Adaptation**
```
Train model to quickly adapt to new patterns (like MAML - Model-Agnostic Meta-Learning):

1. Pre-train on many tickers
2. Few-shot learning: Adapt to new ticker with just 10-20 examples
3. Fast adaptation to regime changes

Questions:
- MAML vs Reptile vs Prototypical Networks for financial data?
- How many meta-training tasks (tickers) needed?
- Can meta-learning help with black swan events?
- Provide PyTorch implementation of MAML for trading CNN
```

---

## üìã PRIORITIZED RESEARCH ORDER

### TIER 1 (IMMEDIATE - MUST HAVE)
1. **Q1.1** ‚Äî GASF image generation (core visual transformation)
2. **Q2.1** ‚Äî CNN architecture comparison (foundation)
3. **Q3.1** ‚Äî Policy + Value network design (AlphaGo core)
4. **Q4.1** ‚Äî Contrastive learning for pattern discovery (unsupervised learning)
5. **Q5.1** ‚Äî GradCAM visualization (interpretability)

### TIER 2 (HIGH VALUE - SHOULD HAVE)
6. **Q2.2** ‚Äî Attention mechanisms (focus on key patterns)
7. **Q2.3** ‚Äî Multi-scale detection (multiple timeframes)
8. **Q4.3** ‚Äî Cluster analysis (pattern archetypes)
9. **Q1.2** ‚Äî Recurrence Plots alternative (richer representations)

### TIER 3 (ADVANCED - NICE TO HAVE)
10. **Q3.2** ‚Äî MCTS for trade planning (full AlphaGo approach)
11. **Q4.2** ‚Äî VAE for pattern generation (synthetic data)
12. **Q6.1** ‚Äî Neural Architecture Search (auto-design)
13. **Q3.3** ‚Äî Self-play simulation (emergent strategies)

---

## üé¨ COPY-PASTE PROMPTS FOR PERPLEXITY PRO

### Prompt 1: GASF Image Generation
```
We want to convert stock OHLCV candlestick data into Gramian Angular Summation Field (GASF) images for CNN-based pattern recognition, similar to how AlphaGo views Go board positions as spatial patterns. 

Provide comprehensive answers for:
1. Optimal window size (20, 30, 60 days) for GASF generation in daily stock data?
2. Should we use GASF (summation), GADF (difference), or both stacked as multi-channel input?
3. How to normalize price data before GASF transformation for different stock price ranges ($10 vs $300)?
4. What image resolution (64x64, 128x128, 224x224) balances pattern detail vs CNN training speed on T4 GPU?
5. Complete Python code using pyts library to generate GASF images from pandas DataFrame with OHLCV data

Include working code for:
- OHLCV ‚Üí GASF conversion pipeline
- Multi-channel stacking (Open, High, Low, Close, Volume as 5 separate channels)
- Efficient batch generation for 10,000+ training samples
- Visualization of resulting images with matplotlib
- Memory-efficient storage (should we save as PNG or numpy arrays?)
```

### Prompt 2: CNN Architecture for Financial Images
```
For classifying financial GASF images (224x224 with 5 channels: OHLCV) into BUY/HOLD/SELL decisions, compare these CNN architectures:

1. **ResNet-18/34** (AlphaGo uses residual networks)
2. **EfficientNet-B0** (parameter efficient, good for limited GPU memory)
3. **Vision Transformer (ViT-Small)** (attention-based, discovers global patterns)
4. **Custom lightweight CNN** (4-6 layers, faster training for rapid iteration)

For EACH architecture provide:
- Expected accuracy on financial pattern classification (cite papers if available)
- Training time estimate for 10,000 GASF images on T4 GPU
- GPU memory requirements
- Recommended hyperparameters (learning rate, batch size, optimizer)
- Complete PyTorch code implementation with pretrained weights loading
- Pros/cons for financial pattern discovery

Focus on: Which architecture has discovered novel patterns in similar computer vision tasks? Which is most sample-efficient for limited training data?
```

### Prompt 3: AlphaGo Dual Network for Trading
```
AlphaGo uses dual networks: Policy Network (what move to make?) + Value Network (who's winning?). 

For stock trading with GASF chart images as input:
- **Policy Network:** Chart ‚Üí BUY/HOLD/SELL decision (classification)
- **Value Network:** Chart ‚Üí Expected 5-day return (regression)

Questions:
1. Should we train jointly (shared CNN backbone with two heads) or separately?
2. What loss functions and loss weights? (Cross-entropy + MSE? Or combined custom loss?)
3. How to handle exploration vs exploitation during training? (Œµ-greedy? Softmax temperature?)
4. Should value network predict absolute return or Sharpe ratio?

Provide complete PyTorch implementation including:
- Shared ResNet-18 backbone (pretrained on ImageNet or random init?)
- Policy head (3-class softmax output)
- Value head (single regression output with appropriate activation)
- Joint training loop with combined loss function
- Example of using both networks together for trading decisions
```

### Prompt 4: Contrastive Learning for Chart Pattern Discovery
```
AlphaGo learns board representations unsupervised before training. Apply SimCLR (Simple Contrastive Learning) to financial GASF images:

Goal: Train CNN to learn meaningful chart pattern features WITHOUT labels, then finetune for BUY/SELL prediction.

Critical questions:
1. What data augmentations preserve financial meaning? (Horizontal flip reverses trend direction! Time-shift causes lookahead! What's safe?)
2. How many unlabeled charts needed for good representations? (10K? 100K?)
3. After contrastive pretraining, how much labeled data needed for finetuning? (Can we reduce from 10K to 1K labeled samples?)
4. Temperature parameter in NT-Xent loss for financial data?

Provide complete PyTorch implementation:
- Data augmentation pipeline for GASF financial images (safe transformations only)
- NT-Xent contrastive loss function
- SimCLR training loop for unsupervised pretraining (2-stage: pretrain ‚Üí finetune)
- Evaluation: Compare performance with vs without contrastive pretraining
- How to visualize learned features (t-SNE embeddings)
```

### Prompt 5: GradCAM Visualization for Pattern Interpretation
```
After training CNN on financial GASF images, use Gradient-weighted Class Activation Mapping (GradCAM) to visualize which parts of the chart the model focuses on for BUY vs SELL decisions.

Questions:
1. How to implement GradCAM for multi-class financial CNNs? (BUY/HOLD/SELL)
2. Can we extract human-interpretable rules from attention heatmaps? (e.g., "model focuses on volume spike + price support")
3. How to validate that CNN learned meaningful patterns vs noise fitting?
4. Can we use GradCAM to debug misclassifications?

Provide complete PyTorch code:
- GradCAM implementation for ResNet-18 with custom classification head
- Function to generate heatmap overlay on original GASF image
- Batch processing for analyzing 100+ predictions
- Comparison visualization: Show GradCAM heatmaps for correct vs incorrect predictions
- Statistical analysis: Which chart regions (early candles, recent candles, volume bars) are most important on average?
```

### Prompt 6: Attention Mechanisms for Multi-Scale Charts
```
For processing multiple timeframes (1H, 4H, 1D charts) simultaneously, design CNN with attention modules:

1. **Spatial Attention:** Which chart regions matter most? (Recent candles? Support/resistance zones?)
2. **Channel Attention (CBAM):** Which price components matter? (Close price vs Volume vs High-Low range?)
3. **Temporal Attention:** How to weight different timeframes? (Maybe 1D chart matters more during trends, 1H during reversals?)

Questions:
- How to stack 3 timeframes as input? (Separate channels? Concatenate? Multi-branch network?)
- Where to place attention modules in ResNet architecture?
- How to visualize attention weights to understand what model learned?
- Does attention improve accuracy vs simple multi-input CNN?

Provide PyTorch implementation:
- CBAM (Convolutional Block Attention Module) integrated into ResNet-18
- Multi-timeframe input handling (3 separate GASF images ‚Üí fused features)
- Attention weight visualization code
- Ablation study: ResNet-18 baseline vs ResNet-18+CBAM performance
```

### Prompt 7: Recurrence Plots vs GASF Comparison
```
Besides GASF/GADF, Recurrence Plots (RP) are another time-series-to-image transformation. Compare for financial pattern recognition:

1. **GASF** (Gramian Angular Summation Field)
2. **GADF** (Gramian Angular Difference Field)  
3. **RP** (Recurrence Plots)
4. **MTF** (Markov Transition Field)

Questions:
- Which captures trend patterns best? Volatility patterns? Momentum shifts?
- Can we stack all three as RGB channels for richer representation?
- Which is most sensitive to price normalization choices?
- Training speed and memory: Are RPs faster to generate than GASF?

Provide Python code:
- Generate all 4 image types from same OHLC data
- Side-by-side visualization
- Train 4 separate CNNs on each image type, compare validation accuracy
- Recommendation: Which to use for production trading system?
```

### Prompt 8: Multi-Agent Self-Play for Trading
```
AlphaZero improves through self-play. For trading, create market simulation where multiple AI agents compete:

Concept:
1. Spawn N agents with different strategies (random initialization)
2. Simulate trading in a shared order book
3. Agents learn from profit/loss outcomes
4. Evolutionary selection: Top performers survive, bottom ones mutate

Questions:
1. How to design realistic market simulator with price impact and liquidity?
2. How many agents needed? (10? 100?)
3. How to prevent mode collapse (all agents converge to same strategy)?
4. Can this discover novel strategies not in historical data?

Provide framework design:
- Market environment class (order book, execution, PnL tracking)
- Agent class (strategy DNA, mutation operators)
- Evolutionary loop (compete ‚Üí select ‚Üí mutate ‚Üí repeat)
- Pseudocode or Python skeleton
- Example: Show how agents might discover "fade the weak" or "momentum surfing" patterns emergently
```

### Prompt 9: Neural Architecture Search for Trading CNNs
```
Instead of manually choosing CNN layers, use Neural Architecture Search (NAS) to discover optimal topology for financial GASF images.

Constraints:
- Must run on Colab Pro (T4 GPU, ~12 hours max search time)
- Search space: Layer types, kernel sizes, number of residual blocks, attention modules
- Objective: Maximize validation Sharpe ratio (not just accuracy - we care about risk-adjusted returns)

Questions:
1. DARTS (gradient-based) vs ENAS (RL-based) vs Random Search for this task?
2. Minimal search space that's feasible in 12 hours?
3. Can we warm-start with ResNet-18 and search for modifications?
4. How to define Sharpe ratio as differentiable NAS objective?

Provide implementation approach:
- NAS framework setup (DARTS or NNI toolkit)
- Search space definition for financial CNNs
- Custom metric: Validation Sharpe ratio calculation
- Expected improvements over hand-designed architecture
- Risks: Overfitting to validation set?
```

### Prompt 10: Meta-Learning for Fast Ticker Adaptation
```
Train CNN using meta-learning (MAML - Model-Agnostic Meta-Learning) to quickly adapt to new tickers or regime changes with minimal data.

Approach:
1. Meta-train on 50 diverse tickers (tech, finance, energy, healthcare)
2. Each ticker is a "task" in meta-learning
3. Learn initialization that adapts quickly to new ticker with 10-20 examples
4. Fast adaptation to regime changes (market crash ‚Üí recovery)

Questions:
1. MAML vs Reptile vs Prototypical Networks for financial time series?
2. How many meta-training tasks (tickers) needed for good generalization?
3. Inner loop steps and outer loop learning rates?
4. Can meta-learning help with black swan events (fast adaptation to unprecedented patterns)?

Provide PyTorch implementation:
- MAML training loop for financial CNNs
- Meta-train on multiple tickers
- Meta-test: Finetune on new ticker with 20 samples, measure accuracy
- Comparison: MAML-trained model vs standard pretrained model on few-shot ticker adaptation
```

---

## üöÄ IMPLEMENTATION PRIORITY

### Week 1: Visual Foundation
- Research Q1.1 (GASF generation)
- Research Q2.1 (CNN architecture)
- Implement GASF pipeline
- Train baseline ResNet-18 on GASF images

### Week 2: AlphaGo Core
- Research Q3.1 (Policy + Value networks)
- Research Q4.1 (Contrastive learning)
- Implement dual-head network
- Pretrain with contrastive learning

### Week 3: Interpretability & Multi-Scale
- Research Q5.1 (GradCAM)
- Research Q2.2 (Attention)
- Add attention modules
- Visualize learned patterns

### Week 4: Advanced Discovery
- Research Q6.1 or Q3.3 (NAS or Self-play)
- Implement pattern evolution
- Compare v1.1 (numerical only) vs v2.0 (visual+numerical)

**Target:** v2.0 with visual patterns ‚Üí 70%+ win rate (up from 61.7%)

---

## üí° KEY INSIGHTS FOR IMPLEMENTATION

1. **Start Simple:** GASF + ResNet-18 baseline FIRST, then add complexity
2. **Interpretability:** Always visualize what CNN learned (GradCAM is critical)
3. **Multi-Scale:** Financial patterns exist at multiple timeframes - capture all
4. **Unsupervised First:** Contrastive learning on unlabeled data = better representations
5. **AlphaGo DNA:** Policy + Value networks, not just classification
6. **Evolution:** Let AI discover patterns through meta-learning/NAS/self-play
7. **Validate Rigorously:** Walk-forward CV, regime-aware splits, no lookahead

---

**Status:** Ready for Perplexity Pro deep research ‚Üí Colab Pro implementation
**Confidence:** 98% this will break 70% win rate
**Risk:** Computational cost (mitigated by Colab Pro T4/A100)
