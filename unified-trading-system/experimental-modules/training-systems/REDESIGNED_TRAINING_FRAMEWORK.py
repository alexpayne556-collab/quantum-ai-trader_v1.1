"""
üéì REDESIGNED TRAINING FRAMEWORK
=================================
Focus: Improve signal accuracy and quality
NOT autonomous trading - we're training signal generators
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict
import optuna
from sklearn.model_selection import TimeSeriesSplit

class SignalAccuracyTrainer:
    """
    Train modules to improve signal accuracy
    Focus: Better signals = better recommendations
    """
    
    def __init__(self):
        self.training_history = {}
        self.best_parameters = {}
        self.performance_tracking = defaultdict(list)
    
    def train_module_accuracy(self, module_name: str, module_instance, 
                             training_data: Dict, validation_data: Dict,
                             target_metric: str = 'win_rate',
                             min_improvement: float = 0.02) -> Dict:
        """
        Train a module to improve signal accuracy
        
        Args:
            module_name: Name of module
            module_instance: Module instance to train
            training_data: Dict with historical signals and outcomes
            validation_data: Dict for validation
            target_metric: 'win_rate', 'sharpe', 'profit_factor'
            min_improvement: Minimum improvement required (2% default)
        
        Returns:
            Training results dict
        """
        print(f"\nüéì Training {module_name} for better signal accuracy...")
        
        # Baseline performance
        baseline = self._evaluate_signals(module_instance, validation_data)
        baseline_metric = baseline.get(target_metric, 0)
        print(f"   Baseline {target_metric}: {baseline_metric:.1%}")
        
        # Hyperparameter optimization
        if hasattr(module_instance, 'get_hyperparameters'):
            study = optuna.create_study(direction='maximize')
            
            def objective(trial):
                # Suggest hyperparameters
                if hasattr(module_instance, 'suggest_hyperparameters'):
                    params = module_instance.suggest_hyperparameters(trial)
                    module_instance.set_hyperparameters(params)
                
                # Evaluate
                results = self._evaluate_signals(module_instance, validation_data)
                return results.get(target_metric, 0.0)
            
            study.optimize(objective, n_trials=20, show_progress_bar=False)
            
            # Apply best parameters
            best_params = study.best_params
            if hasattr(module_instance, 'set_hyperparameters'):
                module_instance.set_hyperparameters(best_params)
            
            # Evaluate improvement
            improved = self._evaluate_signals(module_instance, validation_data)
            improved_metric = improved.get(target_metric, 0)
            improvement = improved_metric - baseline_metric
            
            print(f"   Improved {target_metric}: {improved_metric:.1%} (+{improvement:.1%})")
            
            if improvement >= min_improvement:
                self.best_parameters[module_name] = best_params
                self.training_history[module_name] = {
                    'baseline': baseline_metric,
                    'improved': improved_metric,
                    'improvement': improvement,
                    'best_params': best_params
                }
                print(f"   ‚úÖ Training successful!")
                return self.training_history[module_name]
            else:
                print(f"   ‚ö†Ô∏è  Improvement below threshold ({improvement:.1%} < {min_improvement:.1%})")
                return {}
        
        return {}
    
    def train_ensemble_weights(self, ensemble_instance, historical_performance: Dict) -> Dict:
        """
        Train optimal ensemble weights based on historical performance
        
        Args:
            ensemble_instance: Ensemble instance
            historical_performance: Dict of {module: {'win_rate': X, 'sharpe': Y, 'signal_count': Z}}
        
        Returns:
            Optimal weights dict
        """
        print(f"\nüéì Training Ensemble Weights...")
        
        # Calculate reliability score for each module
        module_reliability = {}
        
        for module, perf in historical_performance.items():
            win_rate = perf.get('win_rate', 0.5)
            sharpe = max(perf.get('sharpe', 0), 0)  # Only positive sharpe
            signal_count = perf.get('signal_count', 0)
            
            # Reliability = win_rate * (1 + sharpe) * sample_size_factor
            # More signals = more reliable (up to a point)
            sample_factor = min(signal_count / 50.0, 1.5)  # Cap at 1.5x for 50+ signals
            
            reliability = win_rate * (1 + sharpe) * sample_factor
            module_reliability[module] = reliability
        
        # Normalize to sum to 1.0
        total_reliability = sum(module_reliability.values())
        if total_reliability > 0:
            optimal_weights = {k: v/total_reliability for k, v in module_reliability.items()}
        else:
            # Equal weights if no data
            optimal_weights = {k: 1.0/len(historical_performance) for k in historical_performance.keys()}
        
        print(f"   Optimal weights:")
        for module, weight in sorted(optimal_weights.items(), key=lambda x: x[1], reverse=True):
            perf = historical_performance.get(module, {})
            win_rate = perf.get('win_rate', 0)
            print(f"     {module}: {weight:.1%} (win rate: {win_rate:.1%})")
        
        # Apply to ensemble
        if hasattr(ensemble_instance, 'set_weights'):
            ensemble_instance.set_weights(optimal_weights)
        elif hasattr(ensemble_instance, 'current_weights'):
            ensemble_instance.current_weights = optimal_weights
        
        return optimal_weights
    
    def _evaluate_signals(self, module_instance, test_data: Dict) -> Dict:
        """Evaluate module signal quality"""
        # Simplified - in production, use actual signal testing
        signals = test_data.get('signals', [])
        outcomes = test_data.get('outcomes', [])
        
        if len(signals) == 0:
            return {'win_rate': 0.5, 'sharpe': 0.0, 'profit_factor': 1.0}
        
        correct = sum(1 for o in outcomes if o.get('was_correct', False))
        win_rate = correct / len(outcomes) if len(outcomes) > 0 else 0.5
        
        gains = [o.get('gain_pct', 0) for o in outcomes]
        if len(gains) > 1:
            sharpe = np.mean(gains) / np.std(gains) * np.sqrt(252) if np.std(gains) > 0 else 0
        else:
            sharpe = 0
        
        winning_gains = [g for g in gains if g > 0]
        losing_gains = [abs(g) for g in gains if g < 0]
        
        profit_factor = (sum(winning_gains) / sum(losing_gains)) if sum(losing_gains) > 0 else 1.0
        
        return {
            'win_rate': win_rate,
            'sharpe': sharpe,
            'profit_factor': profit_factor
        }
    
    def track_performance(self, module_name: str, signal_outcome: Dict):
        """Track performance for continuous learning"""
        self.performance_tracking[module_name].append({
            'timestamp': datetime.now(),
            'was_correct': signal_outcome.get('was_correct', False),
            'gain_pct': signal_outcome.get('gain_pct', 0),
            'confidence': signal_outcome.get('confidence', 0.5)
        })
        
        # Keep only last 100
        if len(self.performance_tracking[module_name]) > 100:
            self.performance_tracking[module_name].pop(0)
    
    def get_performance_summary(self, module_name: str) -> Dict:
        """Get performance summary for a module"""
        history = self.performance_tracking.get(module_name, [])
        
        if len(history) == 0:
            return {}
        
        correct = sum(1 for h in history if h['was_correct'])
        win_rate = correct / len(history)
        
        gains = [h['gain_pct'] for h in history]
        avg_gain = np.mean(gains)
        
        return {
            'win_rate': win_rate,
            'avg_gain': avg_gain,
            'sample_size': len(history),
            'recent_trend': self._calculate_trend(history)
        }
    
    def _calculate_trend(self, history: List[Dict]) -> str:
        """Calculate recent performance trend"""
        if len(history) < 10:
            return 'INSUFFICIENT_DATA'
        
        recent = history[-10:]
        older = history[-20:-10] if len(history) >= 20 else history[:-10]
        
        recent_win_rate = sum(1 for h in recent if h['was_correct']) / len(recent)
        older_win_rate = sum(1 for h in older if h['was_correct']) / len(older) if len(older) > 0 else 0.5
        
        if recent_win_rate > older_win_rate + 0.05:
            return 'IMPROVING'
        elif recent_win_rate < older_win_rate - 0.05:
            return 'DECLINING'
        else:
            return 'STABLE'

# Usage:
# trainer = SignalAccuracyTrainer()
# trainer.train_module_accuracy('dark_pool', dark_pool_instance, training_data, validation_data)
# optimal_weights = trainer.train_ensemble_weights(ensemble_instance, historical_perf)

